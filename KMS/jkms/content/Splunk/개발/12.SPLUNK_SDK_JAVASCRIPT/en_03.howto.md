# How to use the Splunk SDK for JavaScript

This section shows how to do the basics in Splunk® Enterprise. We're assuming you already followed the instructions in the Getting Started section and were able to run the examples. We're also assuming you know your way around Splunk Enterprise and got your feet wet—you've added some data and saved a search or two. If so, you're ready to start using the SDK to develop Splunk Enterprise applications.

If you try the examples that are included with the SDK, you'll notice that many are written from a command-line perspective—that is, they start by parsing the parameters that are provided at the command line and the parameters that are defined in the .splunkrc file (the optional file that stores your login credentials for convenience when running the SDK examples, described in Utilities).

For simplicity, the code examples in this section avoid error handling, command-line processing, and complex logic, staying focused simply on showing you how to use the SDK APIs. If you want more advanced real-world examples, see the JavaScript code examples on GitHub.
How to...
 Connect to Splunk Enterprise

Introduces the basic process of connecting to splunkd and logging in.
 Work with saved searches

Shows how to list, create, and run saved searches.
 Work with modular inputs

Shows how to create and use modular inputs.
 Run searches and display results

Shows how to list search jobs, create new jobs by running different types of searches, and display the results from searches in different formats.
 Get data into Splunk Enterprise

Shows how to add, view, and modify your indexes.
 Work with users and roles

Shows how to add, view, and modify the users of the Splunk Enterprise system to control access to Splunk.
 Work with alerts

Shows how to add, view, and modify Splunk Enterprise alert groups.

## How to connect to Splunk Enterprise using the Splunk SDK for JavaScript



To start a Splunk session, the first thing your program must do is connect to Splunk by sending login credentials to the splunkd server. Splunk returns an authentication token, which is then automatically included in subsequent calls for the rest of your session. By default, the token is valid for one hour, but is refreshed every time you make a call to splunkd.

We'll go over how to do this for server and client.
Server-side JavaScript

The basic steps to connect to Splunk are as follows:

    The Splunk SDK for JavaScript requires Node.js for server-side scripting, so use the require('splunk-sdk') function to add the Splunk SDK for JavaScript to your own Node.js programs.
    Create an instance of the Service class, which is the primary entry point to the Splunk client library and provides access to most of Splunk's resources.

Here's the example code to start a Splunk session. For simplicity, the credentials for logging in to the splunkd server are hard-coded, so replace them with your own. This example also prints the locally-installed Splunk apps to the console to verify you connected successfully.

Before you run this example, be sure to start the Splunk server if it's not running already.

var splunkjs = require('splunk-sdk');

// Create a Service instance and log in
var service = new splunkjs.Service({
                    username: "admin",
                    password: "password",
                    scheme:"https",
                    host:"localhost",
                    port:"8089"
                  });

// Print installed apps to the console to verify login
service.login(function(err, success) {
    if (err) {
        throw err;
    }

    console.log("Login was successful: " + success);
    service.apps().fetch(function(err, apps) {
       if (err) {
         console.log("Error retrieving apps: ", err);
         return;
       }

  console.log("Applications:");

   var appsList = apps.list();
   for(var i = 0; i < appsList.length; i++) {
     var app = appsList[i];
     console.log("  App " + i + ": " + app.name);
   }
 });
});

Client-side JavaScript

The basic steps to connect to Splunk in client-side script are as follows:

    Add <script> tags that include the required libraries: jQuery.min.js, and either splunk.js or splunk.min.js.
    Create an instance of the Service class, which is the primary entry point to the Splunk client library and provides access to most of Splunk's resources.

Here's a sample HTML page that displays a button that when clicked, connects to Splunk and lists the locally-installed Splunk apps to verify that you connected successfully. For simplicity, the credentials for logging in to the splunkd server are hard-coded, so replace them with your own.

Before you run this example, update the paths to the required libraries and start the Splunk server if it's not running already.

<html>
  <head>
    <meta charset="utf-8">
    <title>How to start a Splunk session</title>
    <script type="text/javascript" src="YOUR_PATH/jquery.min.js"></script>
    <script type="text/javascript" src="YOUR_PATH/client/splunk.js"></script>

    <script type="text/javascript" charset="utf-8">
    function displayApps() {
        var http = new splunkjs.ProxyHttp("/proxy");

        // Create a Service instance and log in
        var service = new splunkjs.Service(http, {
            username: "admin",
            password: "yourpassword",
            scheme: "https",
            host: "localhost",
            port:"8089",
            version:"5.0"
        });


        // Verify we logged in

        // Print installed apps to the console to verify login
        service.apps().fetch(function(err, apps) {
            if (err) {
                alert("Error listing apps");
                return;
            }

            var myapps = "";
            var appsList = apps.list();
            for(var i = 0; i < appsList.length; i++) {
                myapps += "App " + i + ": " + appsList[i].name + "<br/>"
            }

            document.getElementById("applist").innerHTML=myapps;
        });

    }
    </script>
</head>
<body>
    <button type="button" onclick="displayApps()">Log in and list apps</button>
    <p id="applist"></p>
</body>
</html>

You can use the sample web server to test this code. First, start the web server—open a command prompt in the /splunk-sdk-javascript directory and enter:

node sdkdo runserver

Then, save this code as an HTML page and open it in a web browser. For example, navigate to http://localhost:6969/examples/browser/filename.html.


## How to work with saved searches using the Splunk SDK for JavaScript

The most fundamental feature in Splunk® is searching your data. But before diving into the details of how to use the SDK to search, let's clarify the terms:

    A search query is a set of commands and functions you use to retrieve events from an index or a real-time stream, for example: "search * | head 10".
    A saved search is a search query that has been saved to be used again and can be set up to run on a regular schedule. The results from the search are not saved with the query.
    A search job is an instance of a completed or still-running search operation, along with the results. A search ID is returned when you create a job, allowing you to access the results of the search when they become available. Search results are returned in JSON, JSON_ROWS, JSON_COLS, XML, or CSV format. The default format for JavaScript is JSON_ROWS.

This topic focuses on working with saved searches. For more about working with search jobs, see How to run searches and display results.
The search APIs

The classes for working with saved searches are:

    The splunkjs.Service.SavedSearches class for a collection of saved searches.
    The splunkjs.Service.SavedSearch class for an individual saved search.

Access these classes through an instance of the splunkjs.Service class. Retrieve a collection, and from there you can access individual items in the collection and create new ones.
Code examples

This section provides examples of how to use the search APIs, assuming you first connect to a Splunk instance:

    To list saved searches
    To view the history of a saved search
    To create a saved search
    To view and modify the properties of a saved search
    To run a saved search and display search results
    To delete a saved search

The following parameters are available for saved searches:

    Collection parameters
    Saved search parameters

To list saved searches

This example shows how to retrieve and list the saved searches in the saved search collection. If you don't explicitly specify a namespace, the default one is used (to continue the How to connect to Splunk example, the current user is "admin" and the current app is the default "search").

// List all saved searches for the current username
var mySavedSearches = service.savedSearches();
mySavedSearches.fetch(function(err, mySavedSearches) {

    console.log("There are " + mySavedSearches.list().length + " saved searches");

    var savedSearchColl = mySavedSearches.list();

    for(var i = 0; i < savedSearchColl.length; i++) {
        var search = savedSearchColl[i];
        console.log(i + ": " + search.name);
        console.log("    Query: " + search.properties().search + "\n");
    }
});

To retrieve a collection for a specific namespace—for example, to list the saved searches available to a specific username—provide the namespace as arguments to the splunkjs.Service.SavedSearches method. This example lists the saved searches for the user "username":

// Get saved searches for "username"
var moreSavedSearches = service.savedSearches({owner: "username", app: "search"});

moreSavedSearches.fetch(function (err, moreSavedSearches) {
  console.log("There are " + moreSavedSearches.list().length + " saved searches for 'username'");
});

To view the history of a saved search

The history of a saved search contains the past and current instances (jobs) of the search. This example shows the history for all the saved searches in the current collection:

// Retrieve the collection of saved searches
var mySavedSearches = service.savedSearches();

mySavedSearches.fetch(function(err, mySavedSearches) {
  var savedSearchColl = mySavedSearches.list();

  // Loop through the collection and get info about each search
  for(var i = 0; i < savedSearchColl.length; i++) {
    var search = savedSearchColl[i];

    search.history(function(err, jobs, search) {
      console.log("Jobs for " + search.name + ": ");

      for(var j = 0; j < jobs.length; j++) {
        console.log("    " + jobs[j].sid);
      }

    });
  }
});

To create a saved search

When you create a saved search, at a minimum you need to provide a search query and a name for the search. You can also specify additional properties for the saved search at this time by providing a dictionary of key-value pairs for the properties (the possible properties are summarized in Saved search parameters). Or, modify properties after you have created the saved search.

This example shows how to create a basic saved search:

// Retrieve the collection of saved searches
var mySavedSearches = service.savedSearches();

// Specify a name and search query
var searchName = "Test Search";
// Note: Do not include the 'search' keyword for a saved search
var searchQuery = "* &#124; head 10";

// Create the saved search
mySavedSearches.create({
  name: searchName,
  search: searchQuery
}, function(err, newSearch) {
  console.log("A new saved search was created");
});

To view and modify the properties of a saved search

To access properties of a saved search, use the properties method of the saved search object along with the property's name (see Saved search parameters for a list of all the possible properties for a saved search).

To set properties, pass property key-value pairs to the entity's update method to make the changes on the server. Next, call the entity's fetch method to update your local, cached copy of the object with these changes.

This example shows how to view properties for the saved search that was created earlier, then shows how to set the description and schedule the saved search in cron format:

// The saved search created earlier
var searchName = "Test Search";

// Retrieve the saved search collection
var mySavedSearches = service.savedSearches();

mySavedSearches.fetch(function(err, mySavedSearches) {

  // Retrieve a specific saved search
  var mySavedSearch = mySavedSearches.item(searchName);

  // Display some properties
  console.log("Name:                " + mySavedSearch.name);
  console.log("Query:               " + mySavedSearch.properties().search);
  console.log("Description:         " + mySavedSearch.properties().description);
  console.log("Scheduled:           " + mySavedSearch.properties().is_scheduled);
  console.log("Next scheduled time: " + mySavedSearch.properties().next_scheduled_time);

  // Modify the description and schedule the saved search
  mySavedSearch.update({
    description: "This is a test search",
    is_scheduled: true,         // Schedule the search
    cron_schedule: "15 4 * * 6" // Runs the search on Saturdays at 4:15am
  }, function() {
    console.log("\n...properties were modified...")
  });

  // Create a small delay to allow time for the update between server and client
  splunkjs.Async.sleep(2000, function() {

    // Update the local copy of the object with changes
    mySavedSearch.fetch(function(err, mySavedSearch) {

      // Display the updated properties to verify
      console.log("\nUpdated properties:");
      console.log("Description:         " + mySavedSearch.properties().description);
      console.log("Scheduled:           " + mySavedSearch.properties().is_scheduled);
      console.log("Next scheduled time: " + mySavedSearch.properties().next_scheduled_time);
    });
  });
});

To run a saved search and display search results

Running a saved search creates a search job that is scheduled to run right away. Use the splunkjs.Service.SavedSearch.dispatch method to run a saved search, which returns a splunkjs.Service.Job object that corresponds to the search job. This Job object gives you access to information about the search job, such as the search ID, the status of the search, and the search results once the search job has finished.

The dispatch method takes these optional parameters:

    dispatch.now: A time string that is used to dispatch the search as though the specified time were the current time.
    dispatch.*: Overwrites the value of the search field specified in *.
    trigger_actions: A Boolean that indicates whether to trigger alert actions.
    force_dispatch: A Boolean that indicates whether to start a new search if another instance of this search is already running.

Once the search has finished, retrieve the search results from the Job object. The default output format is JSON_ROWS.

This example runs the search that was created above, shows how to poll the status using the splunkjs.Job.track function to determine when the search has completed, and displays the raw results:

// The saved search created earlier
var searchName = "Test Search";

// Retrieve the saved search collection
var mySavedSearches = service.savedSearches();

mySavedSearches.fetch(function(err, mySavedSearches) {

  // Retrieve the saved search that was created earlier
  var mySavedSearch = mySavedSearches.item(searchName);

  // Run the saved search and poll for completion
  mySavedSearch.dispatch(function(err, job) {

    // Display the job's search ID
    console.log("Job SID: ", job.sid);

    // Poll the status of the search job
    job.track({
      period: 200
    }, {
      done: function(job) {
        console.log("Done!");

        // Print out the statics
        console.log("Job statistics:");
        console.log("  Event count:  " + job.properties().eventCount);
        console.log("  Result count: " + job.properties().resultCount);
        console.log("  Disk usage:   " + job.properties().diskUsage + " bytes");
        console.log("  Priority:     " + job.properties().priority);

        // Get 10 results and print them
        job.results({
          count: 10
        }, function(err, results, job) {
          console.log(JSON.stringify(results));
        });
      },
      failed: function(job) {
        console.log("Job failed")
      },
      error: function(err) {
        done(err);
      }
    });
  });
});

To delete a saved search

You can delete a saved search using the splunkjs.Service.Entity.remove method. Any jobs for the saved search are not deleted.

This example shows how to delete a saved search:

// The saved search created earlier
var searchName = "Test Search";

// Retrieve the saved search collection
var mySavedSearches = service.savedSearches();

mySavedSearches.fetch(function(err, mySavedSearches) {

  // Retrieve the saved search that was created earlier
  var mySavedSearch = mySavedSearches.item(searchName);

  // Determine whether the saved search exists before deleting it
  if(!mySavedSearch) {
    console.log("There is no saved search named '" + searchName + "'");
  } else {
    mySavedSearch.remove();
    console.log("Deleted the saved search '" + searchName + "'");
  }

});

Collection parameters

The following parameters are available when retrieving a collection of saved searches.

Parameter
	
Description
count	A number that indicates the maximum number of entries to return. A value of 0 means all entries are returned.
earliest_time	A string that contains all the scheduled times starting from this time (not just the next run time).
latest_time	A string that contains all the scheduled times until this time.
offset	A number that specifies the index of the first item to return.
For oneshot inputs, this value refers to the current position in the source file, indicating how much of the file has been read.
search	A string that specifies a search expression to filter the response with, matching field values against the search expression. For example, "search=foo" matches any object that has "foo" as a substring in a field, and "search=field_name%3Dfield_value" restricts the match to a single field.
sort_dir	An enum value that specifies how to sort entries. Valid values are "asc" (ascending order) and "desc" (descending order).
sort_key	A string that specifies the field to sort by.
sort_mode	An enum value that specifies how to sort entries. Valid values are "auto", "alpha" (alphabetically), "alpha_case" (alphabetically, case sensitive), or "num" (numerically).
Saved search parameters

The properties that are available for saved searches correspond to the parameters for the saved/searches endpoint in the REST API.

This table summarizes the properties you can set for a saved search.
Parameter
	
Description
name	Required. A string that contains the name of the saved search.
search	Required. A string that contains the search query.
action.*	A string with wildcard arguments to specify specific action arguments.
action.email	A Boolean that indicates the state of the email alert action. Read only.
action.email.auth_password	A string that specifies the password to use when authenticating with the SMTP server. Normally this value is set while editing the email settings, but you can set a clear text password here that is encrypted when Splunk is restarted.
action.email.auth_username	A string that specifies the username to use when authenticating with the SMTP server. If this is empty string, authentication is not attempted.
action.email.bcc	A string that specifies the BCC email address to use if "action.email" is enabled.
action.email.cc	A string that specifies the CC email address to use if "action.email" is enabled.
action.email.command	A string that contains the search command (or pipeline) for running the action.
action.email.format	An enum value that indicates the format of text and attachments in the email ("plain", "html", "raw", or "csv"). Use "plain" for plain text.
action.email.from	A string that specifies the email sender's address.
action.email.hostname	A string that specifies the hostname used in the web link (URL) that is sent in email alerts. Valid forms are "hostname" and "protocol://hostname:port".
action.email.inline	A Boolean that indicates whether the search results are contained in the body of the email.
action.email.mailserver	A string that specifies the address of the MTA server to be used to send the emails.
action.email.maxresults	The maximum number of search results to send when "action.email" is enabled.
action.email.maxtime	A number indicating the maximum amount of time an email action takes before the action is canceled. The valid format is number followed by a time unit ("s", "m", "h", or "d"), for example "5d".
action.email.pdfview	A string that specifies the name of the view to deliver if "action.email.sendpdf" is enabled.
action.email.preprocess_results	A string that specifies how to pre-process results before emailing them.
action.email.reportCIDFontList	Members of an enumeration in a space-separated list specifying the set (and load order) of CID fonts for handling Simplified Chinese(gb), Traditional Chinese(cns), Japanese(jp), and Korean(kor) in Integrated PDF Rendering.
action.email.reportIncludeSplunkLogo	A Boolean that indicates whether to include the Splunk logo with the report.
action.email.reportPaperOrientation	An enum value that indicates the paper orientation ("portrait" or "landscape").
action.email.reportPaperSize	An enum value that indicates the paper size for PDFs ("letter", "legal", "ledger", "a2", "a3", "a4", or "a5").
action.email.reportServerEnabled	A Boolean that indicates whether the PDF server is enabled.
action.email.reportServerURL	A string that contains the URL of the PDF report server, if one is set up and available on the network.
action.email.sendpdf	A Boolean that indicates whether to create and send the results as a PDF.
action.email.sendresults	A Boolean that indicates whether to attach search results to the email.
action.email.subject	A string that specifies the subject line of the email.
action.email.to	A string that contains a comma- or semicolon-delimited list of recipient email addresses. Required if this search is scheduled and "action.email" is enabled.
action.email.track_alert	A Boolean that indicates whether running this email action results in a trackable alert.
action.email.ttl	The number of seconds indicating the minimum time-to-live (ttl) of search artifacts if this email action is triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
action.email.use_ssl	A Boolean that indicates whether to use secure socket layer (SSL) when communicating with the SMTP server.
action.email.use_tls	A Boolean that indicates whether to use transport layer security (TLS) when communicating with the SMTP server.
action.email.width_sort_columns	A Boolean that indicates whether columns should be sorted from least wide to most wide, left to right. This value is only used when "action.email.format"="plain", indicating plain text.
action.populate_lookup	A Boolean that indicates the state of the populate-lookup alert action. Read only.
action.populate_lookup.command	A string that specifies the search command (or pipeline) to run the populate-lookup alert action.
action.populate_lookup.dest	A string that specifies the name of the lookup table or lookup path to populate.
action.populate_lookup.hostname	A string that specifies the host name used in the web link (URL) that is sent in populate-lookup alerts. Valid forms are "hostname" and "protocol://hostname:port".
action.populate_lookup.maxresults	The maximum number of search results to send in populate-lookup alerts.
action.populate_lookup.maxtime	The number indicating the maximum amount of time an alert action takes before the action is canceled. The valid format is number followed by a time unit ("s", "m", "h", or "d").
action.populate_lookup.track_alert	A Boolean that indicates whether running this populate-lookup action results in a trackable alert.
action.populate_lookup.ttl	The number of seconds indicating the minimum time-to-live (ttl) of search artifacts if this populate-lookup action is triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
action.rss	A Boolean that indicates the state of the RSS alert action. Read only.
action.rss.command	A string that contains the search command (or pipeline) that runs the RSS alert action.
action.rss.hostname	A string that contains the host name used in the web link (URL) that is sent in RSS alerts. Valid forms are "hostname" and "protocol://hostname:port".
action.rss.maxresults	The maximum number of search results to send in RSS alerts.
action.rss.maxtime	The maximum amount of time an RSS alert action takes before the action is canceled. The valid format is number followed by a time unit ("s", "m", "h", or "d").
action.rss.track_alert	A Boolean that indicates whether running this RSS action results in a trackable alert.
action.rss.ttl	The number of seconds indicating the minimum time-to-live (ttl) of search artifacts if this RSS action is triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
action.script	A Boolean that indicates the state of the script alert action. Read only.
action.script.command	A string that contains the search command (or pipeline) that runs the script action.
action.script.filename	A string that specifies the file name of the script to call, which is required if "action.script" is enabled.
action.script.hostname	A string that specifies the hostname used in the web link (URL) that is sent in script alerts. Valid forms are "hostname" and "protocol://hostname:port".
action.script.maxresults	The maximum number of search results to send in script alerts.
action.script.maxtime	The maximum amount of time a script action takes before the action is canceled. The valid format is number followed by a time unit ("s", "m", "h", or "d").
action.script.track_alert	A Boolean that indicates whether running this script action results in a trackable alert.
action.script.ttl	The number of seconds indicating the minimum time-to-live (ttl) of search artifacts if this script action is triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
action.summary_index	A Boolean that indicates the state of the summary index alert action. Read only.
action.summary_index._name	A string that specifies the name of the summary index where the results of the scheduled search are saved.
action.summary_index.command	A string that contains the search command (or pipeline) that runs the summary-index action.
action.summary_index.hostname	A string that specifies the hostname used in the web link (URL) that is sent in summary-index alerts. Valid forms are "hostname" and "protocol://hostname:port".
action.summary_index.inline	A Boolean that indicates whether to run the summary indexing action as part of the scheduled search.
action.summary_index.maxresults	The maximum number of search results to send in summary-index alerts.
action.summary_index.maxtime	A number indicating the maximum amount of time a summary-index action takes before the action is canceled. The valid format is number followed by a time unit ("s", "m", "h", or "d"), for example "5d".
action.summary_index.track_alert	A Boolean that indicates whether running this summary-index action results in a trackable alert.
action.summary_index.ttl	The number of seconds indicating the minimum time-to-live (ttl) of search artifacts if this summary-index action is triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
actions	A string that contains a comma-delimited list of actions to enable, for example "rss,email".
alert.digest_mode	A Boolean that indicates whether Splunk applies the alert actions to the entire result set or digest ("true"), or to each individual search result ("false").
alert.expires	The amount of time to show the alert in the dashboard. The valid format is number followed by a time unit ("s", "m", "h", or "d").
alert.severity	A number that indicates the alert severity level (1=DEBUG, 2=INFO, 3=WARN, 4=ERROR, 5=SEVERE, 6=FATAL).
alert.suppress	A Boolean that indicates whether alert suppression is enabled for this scheduled search.
alert.suppress.fields	A string that contains a comma-delimited list of fields to use for alert suppression.
alert.suppress.period	A value that indicates the alert suppression period, which is only valid when "Alert.Suppress" is enabled. The valid format is number followed by a time unit ("s", "m", "h", or "d").
alert.track	An enum value that indicates how to track the actions triggered by this saved search. Valid values are: "true" (enabled), "false" (disabled), and "auto" (tracking is based on the setting of each action).
alert_comparator	A string that contains the alert comparator. Valid values are: "greater than", "less than", "equal to", "rises by", "drops by", "rises by perc", and "drops by perc".
alert_condition	A string that contains a conditional search that is evaluated against the results of the saved search.
alert_threshold	A value to compare to before triggering the alert action. Valid values are: integer or integer%. If this value is expressed as a percentage, it indicates the value to use when "alert_comparator" is set to "rises by perc" or "drops by perc".
alert_type	A string that indicates what to base the alert on. Valid values are: "always", "custom", "number of events", "number of hosts", and "number of sources". This value is overridden by "alert_condition" if specified.
args.*	A string containing wildcard arguments for any saved search template argument, such as "args.username"="foobar" when the search is search $username$.
auto_summarize	A Boolean that indicates whether the scheduler ensures that the data for this search is automatically summarized.
auto_summarize.command	A string that contains a search template that constructs the auto summarization for this search.
auto_summarize.cron_schedule	A string that contains the cron schedule for probing and generating the summaries for this saved search.
auto_summarize.dispatch.earliest_time	A string that specifies the earliest time for summarizing this saved search. The time can be relative or absolute; if absolute, use the "dispatch.time_format" parameter to format the value.
auto_summarize.dispatch.latest_time	A string that contains the latest time for summarizing this saved search. The time can be relative or absolute; if absolute, use the "dispatch.time_format" parameter to format the value.
auto_summarize.dispatch.ttl	The number of seconds indicating the time to live (in seconds) for the artifacts of the summarization of the scheduled search. If the value is a number followed by "p", it is the number of scheduled search periods.
auto_summarize.max_disabled_buckets	A number that specifies the maximum number of buckets with the suspended summarization before the summarization search is completely stopped, and the summarization of the search is suspended for the "auto_summarize.suspend_period" parameter.
auto_summarize.max_summary_ratio	A number that specifies the maximum ratio of summary size to bucket size, which specifies when to stop summarization and deem it unhelpful for a bucket. The test is only performed if the summary size is larger than the value of "auto_summarize.max_summary_size".
auto_summarize.max_summary_size	A number that specifies the minimum summary size, in bytes, before testing whether the summarization is helpful.
auto_summarize.max_time	A number that specifies the maximum time (in seconds) that the summary search is allowed to run. Note that this is an approximate time because the summary search stops at clean bucket boundaries.
auto_summarize.suspend_period	A string that contains the time indicating when to suspend summarization of this search if the summarization is deemed unhelpful.
auto_summarize.timespan	A string that contains a comma-delimited list of time ranges that each summarized chunk should span. This comprises the list of available granularity levels for which summaries would be available.
cron_schedule	A string that contains the cron-style schedule for running this saved search.
description	A string that contains a description of this saved search.
disabled	A Boolean that indicates whether the saved search is enabled.
dispatch.*	A string that specifies wildcard arguments for any dispatch-related argument.
dispatch.buckets	The maximum number of timeline buckets.
dispatch.earliest_time	A time string that specifies the earliest time for this search. Can be a relative or absolute time. If this value is an absolute time, use "dispatch.time_format" to format the value.
dispatch.latest_time	A time string that specifies the latest time for this saved search. Can be a relative or absolute time. If this value is an absolute time, use "dispatch.time_format" to format the value.
dispatch.lookups	A Boolean that indicates whether lookups for this search are enabled.
dispatch.max_count	The maximum number of results before finalizing the search.
dispatch.max_time	The maximum amount of time (in seconds) before finalizing the search.
dispatch.reduce_freq	The number of seconds indicating how frequently Splunk runs the MapReduce reduce phase on accumulated map values.
dispatch.rt_backfill	A Boolean that indicates whether to back fill the real-time window for this search. This value is only used for a real-time search.
dispatch.spawn_process	A Boolean that indicates whether Splunk spawns a new search process when running this saved search.
dispatch.time_format	A string that defines the time format that Splunk uses to specify the earliest and latest time.
dispatch.ttl	The number indicating the time to live (ttl) for artifacts of the scheduled search (the time before the search job expires and artifacts are still available), if no alerts are triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
displayview	A string that contains the default UI view name (not label) in which to load the results.
is_scheduled	A Boolean that indicates whether this saved search runs on a schedule.
is_visible	A Boolean that indicates whether this saved search is visible in the saved search list.
max_concurrent	The maximum number of concurrent instances of this search the scheduler is allowed to run.
next_scheduled_time	A string that indicates the next scheduled time for this saved search. Read only.
qualifiedSearch	A string that is computed during run time. Read only.
realtime_schedule	A Boolean that specifies how the scheduler computes the next time a scheduled search is run:

    When "true": The schedule is based on the current time. The scheduler might skip some scheduled periods to make sure that searches over the most recent time range are run.
    When "false": The schedule is based on the last search run time (referred to as "continuous scheduling") and the scheduler never skips scheduled periods. However, the scheduler might fall behind depending on its load. Use continuous scheduling whenever you enable the summary index option ("action.summary_index").

The scheduler tries to run searches that have real-time schedules enabled before running searches that have continuous scheduling enabled.
request.ui_dispatch_app	A string that contains the name of the app in which Splunk Web dispatches this search.
request.ui_dispatch_view	A string that contains the name of the view in which Splunk Web dispatches this search.
restart_on_searchpeer_add	A Boolean that indicates whether a real-time search managed by the scheduler is restarted when a search peer becomes available for this saved search. The peer can be one that is newly added or one that has become available after being down.
run_on_startup	A Boolean that indicates whether this search is run when Splunk starts. If the search is not run on startup, it runs at the next scheduled time. It is recommended that you set this value to "true" for scheduled searches that populate lookup tables.
vsid	A string that contains the view state ID that is associated with the view specified in the "displayview" attribute.

## How to work with modular inputs in the Splunk SDK for JavaScript

Support for modular inputs in Splunk® Enterprise 5.0 and later enables you to add new types of custom inputs to Splunk Enterprise that are treated as native Splunk Enterprise inputs. Your users interactively create and update your custom inputs using Splunk Web, just as they do for native inputs. If you're not already familiar with modular inputs, see the following topics to get started:

    Modular inputs overview
    Modular inputs basic example

Some examples of modular inputs include:

    Real-time Windows performance monitoring: A perfmon input that runs a separate process for every input defined.
    Splunk Add-on for Microsoft PowerShell: A modular input that runs PowerShell 3.0 scripts to collect data.
    Twitter example: A modular input that streams JSON data from a Twitter source to Splunk Enterprise.
    Amazon S3 example: A modular input that streams data from the Amazon S3 data storage service to Splunk Enterprise.

The Splunk SDK for JavaScript (starting in version 1.4) includes built-in support for creating and using modular inputs with JavaScript. In addition, Node.js offers rich support for handling I/O-bound workloads and streaming data asynchronously. It can scale out across multiple cores and machines, and has a very rich ecosystem of third party modules via npm.

This topic shows you how to create modular inputs using the Splunk SDK for JavaScript, and how to integrate them into your app that incorporates Splunk Enterprise.

This topic contains the following sections:

    Why use modular inputs?
    Prerequisites
    What you can do with modular input support in the SDK
    Modular input SDK examples
    To create modular inputs programmatically

Why use modular inputs?

Modular inputs are ideal for packaging and sharing technology-specific data sources. One of the primary reasons to use modular inputs is to enable users to interact with key information using the familiar Splunk Web interface, without needing to edit config files. Modular inputs also provide runtime controls and allow the user to specify per-event index-time settings for the input.

Splunk Enterprise treats your custom modular input definition as if it was a native input, allowing users to interactively create and update the input in Splunk Settings just as they would for native inputs. Enabling simple user-level customization in this way differentiates modular inputs from scripted inputs, as does built-in support for validation, multi-platform support, custom REST endpoint access, and more. More information about the differences between modular inputs and traditional scripted inputs is available here: Modular inputs vs. scripted inputs.
What you can do with modular input support in the SDK

With the Splunk SDK for JavaScript you can:

    Create a modular input.
    Define the scheme for your modular input.
    Implement a handler to create and stream back events to Splunk Enterprise.
    Wire up handlers to handle setup and teardown of the process.
    Wire up handlers to handle stream start and end.
    Implement your modular input in an asynchronous, non-blocking manner.
    Create both single and multiple-instance modular inputs.
    Log easily back to Splunk from within your modular input.
    Easily access Splunk capabilities via the SDK from within your modular input.
    Package your modular input within a Splunk Processing Language (SPL) query to deploy it to other Splunk instances.

Prerequisites

Before you can get started creating modular inputs using the Splunk SDK for JavaScript, ensure you have the following installed on your computer:

    Node.js v0.8 or later: Splunk Enterprise includes Node.js v0.8 in its standard install, but you should download and install Node.js from http://nodejs.org/download/ if you do not have Splunk Enterprise installed on your JavaScript development computer or if you want the latest version.
    Splunk SDK for JavaScript v1.4 or later: Install the latest version of the Splunk SDK for JavaScript. The functionality described in this topic is only available in version 1.4 or later of the SDK.

Modular input SDK examples

The Splunk SDK for JavaScript includes two modular input examples in the /splunk-sdk-javascript/examples/modularinputs directory: random numbers and Github commits. To run these examples, you'll need to install them. (They are not installed with the SDK installation process.)

    Set the SPLUNK_HOME environment variable to the root directory of your Splunk Enterprise instance.

    Copy one or both of the sample directories (github_commits or random_numbers) to the following directory:

        $SPLUNK_HOME/etc/apps

    Open a command prompt or terminal window and go to the following directory, where sample_name is either github_commits or random_numbers:

        $SPLUNK_HOME/etc/apps/sample_name/bin/app

    Type the following, and then press Enter or Return:

        npm install

    If the install is not successful, first ensure that Node.js is installed. For more information, see Prerequisites. If Node.js is installed, try the following:

        Download and extract a ZIP file of the SDK from Github: https://github.com/splunk/splunk-sdk-javascript/archive/master.zip

        Copy the extracted splunk-sdk-javascript-master folder to the following path, where sample_name is either github_commits or random_numbers:

            $SPLUNK_HOME/etc/apps/sample_name/bin/app/node_modules

        Rename the copied folder to splunk-sdk.

    Restart Splunk Enterprise: From Splunk Home, click the Settings menu. Under System, click Server Controls. Click Restart Splunk.

After you have installed the example modular inputs, they appear alongside other data inputs. From Splunk Home, click the Settings menu. Under Data, click Data inputs, and find the names of the modular inputs you just added. Click Add new and fill in the settings that you specified when you created the modular input script:

    The Random Numbers example has fields for the name of the input and the minimum and maximum numbers to be produced.
    The Github Commits example has fields for the name of the input, and the owner and name of the repository. There is also an optional access token field to avoid Github's API limits, though it is required if the repository you entered is private. To generate an access token, visit the application settings page, click Generate new token, make sure the repo and public_repo scope checkboxes are enabled, and then click Generate token.

To get a better understanding of how the examples work, take a look at the source code, which is located in the bin/app/ directory within the random_numbers or github_commits example's directory. Within app is a .JS file (named using the name of the app) that does all the work for the modular input.
To create modular inputs programmatically

With the Splunk SDK for JavaScript, you can create modular inputs programmatically using JavaScript. Adding a modular input to Splunk Enterprise is a two-step process: First, write a modular input script, and then package the script with several accompanying files and install it.
To write a modular input script in JavaScript

A modular input script does the following:

    Return the introspection scheme to Splunk Enterprise. The introspection scheme defines the behavior and endpoints of the script. When Splunk Enterprise starts, it runs the script to determine the modular input's behavior and configuration.

    Validate the script's configuration (optional). Whenever a user creates or edits an input, Splunk Enterprise can call the script to validate the configuration.

    Stream data. The script streams event data that can be indexed by Splunk Enterprise. Splunk Enterprise invokes the script and waits for it to stream events.

To create a modular input programmatically using JavaScript, first require "splunk-sdk". In our examples, we've also assigned the classes we'll be using to variables, for convenience. At the very least, we recommend defining a ModularInputs variable as shown here:

    var splunkjs        = require("splunk-sdk");
    var ModularInputs   = splunkjs.ModularInputs;

The preceding three steps are accomplished as follows using the Splunk SDK for JavaScript:

    Return the introspection scheme: Define the getScheme method on the exports object.

    Validate the script's configuration (optional): Define the validateInput method. This is required if you set the scheme returned by getScheme to use external validation (that is, set Scheme.useExternalValidation to true).

    Stream data: Define a function for the streamEvents method.

In addition, you must run the script by calling the ModularInputs.execute method, passing in the exports object you just configured:

ModularInputs.execute(exports, module);

Here's the skeleton of a new modular input script created in JavaScript:

(function() {
    var splunkjs        = require("splunk-sdk");
    var ModularInputs   = splunkjs.ModularInputs;
    var Logger          = ModularInputs.Logger;
    var Event           = ModularInputs.Event;
    var Scheme          = ModularInputs.Scheme;
    var Argument        = ModularInputs.Argument;
    // other global variables here

    // getScheme method returns introspection scheme
    exports.getScheme = function() {
        var scheme = new Scheme("My Modular Input");

        // scheme properties
        scheme.description = "A modular input.";
        scheme.useExternalValidation = true;  // if true, must define validateInput method
        scheme.useSingleInstance = true;      // if true, all instances of mod input passed to
                                              //   a single script instance; if false, user 
                                              //   can set the interval parameter under "more settings"

        // add arguments
        scheme.args = [
            new Argument({
                name: "arg",
                dataType: Argument.dataTypeNumber,
                description: "An argument.",
                requiredOnCreate: true,
                requiredOnEdit: false
            }),
            new Argument({
                name: "count",
                dataType: Argument.dataTypeNumber,
                description: "A counter.",
                requiredOnCreate: true,
                requiredOnEdit: false
            })
            // other arguments here
        ];

        return scheme;
    };

    // validateInput method validates the script's configuration (optional)
    exports.validateInput = function(definition, done) {
        // local variables here
        var arg = parseFloat(definition.parameters.arg);
        var count = parseInt(definition.parameters.count, 10);

        // error checking goes here
	if (count < 0) {
            done(new Error("The count was a negative number."));
        }
        else {
            done();
        }
    };

    // streamEvents streams the events to Splunk Enterprise
    exports.streamEvents = function(name, singleInput, eventWriter, done) {
        // modular input logic goes here
        var getMyArgument = function (arg) {
            return arg;
        };

        // local variables here
        var arg = parseFloat(singleInput.arg);
        var count = parseInt(singleInput.count, 10);
        var errorFound = false;

        // stream as many events as specified by the count parameter
        for (var i = 0; i < count && !errorFound; i++) {            
            var curEvent = new Event({
                stanza: name,
                data: "argument=" + getArgument(arg)
            });

            try {
                eventWriter.writeEvent(curEvent);
            }
            catch (e) {
                errorFound = true; // Make sure we stop streaming if there's an error at any point
                Logger.error(name, e.message);
                done(e);

                // we had an error; die
                return;
            }
        }

        // streaming is done
        done();
    };

    ModularInputs.execute(exports, module);
})();

Using this skeleton as a starting point, we'll now guide you through the creation of the components of a modular input script in JavaScript. This is the same script that is located at /splunk-sdk-javascript/ examples/modularinputs/random_numbers/. It produces a series of random numbers to demonstrate event generation and streaming.
The getScheme method

When Splunk Enterprise starts, it looks for all the modular inputs defined by its configuration, and tries to run them with the argument --scheme. Splunkd expects each modular input to print a description of itself in XML to stdout. The SDK's modular input framework takes care of all the details of formatting the XML and printing it. You only need to implement a getScheme method to return a new Scheme object.

First, create a new Scheme object, providing both a name and description for it:

exports.getScheme = function() {
    var scheme = new Scheme("Random Numbers");
    scheme.description = "Streams events containing a random number.";

In this case, Splunk Enterprise will display "Random Numbers" to users for this input, with the given description.

Next, specify whether you want to use external validation using the useExternalValidation property. External validation is taken care of by implementing the validateInput method. If you set external validation without implementing the validateInput method, the script will accept anything as valid.

scheme.useExternalValidation = true;

If you set useSingleInstance to true, the scheme will pass all the instances of the modular input to a single instance of the script. You're then responsible for handling all of the instances of the modular input.

scheme.useSingleInstance = true;

Generally you only need external validation if there are relationships you must maintain among the parameters, such as requiring one variable to be less than another, or checking whether some resource is reachable or valid. For example, in our random numbers example, we want to ensure the minimum value is always smaller than the maximum value. (See The validateInput method for more information.) If you don't choose external validation, Splunk Enterprise lets you specify a validation string for each argument and runs validation internally using that string.

In the random_numbers modular input example, there are three parameters—min, max, and count—that represent the minimum and maximum values for the generated random numbers, plus a count used to determine how many events to stream per input execution. We'll add them to the scheme's args property using the Argument class and its properties:

    scheme.args = [
        new Argument({
            name: "min",
            dataType: Argument.dataTypeNumber,
            description: "Minimum random number to be produced by this input.",
            requiredOnCreate: true,
            requiredOnEdit: false
        }),
        new Argument({
            name: "max",
            dataType: Argument.dataTypeNumber,
            description: "Maximum random number to be produced by this input.",
            requiredOnCreate: true,
            requiredOnEdit: false
        }),
        new Argument({
            name: "count",
            dataType: Argument.dataTypeNumber,
            description: "Number of events to generate.",
            requiredOnCreate: true,
            requiredOnEdit: false
        })
    ];

After adding arguments to the scheme, return the scheme:

return scheme;
};

The validateInput method

The validateInput method is where the configuration of an input is validated, and is only needed if you've set your modular input to use external validation. If validateInput does not throw an error, the input is assumed to be valid. Otherwise it throws an error when it tells splunkd that the configuration is not valid.

When you use external validation, after splunkd calls the modular input with the --scheme argument to get the scheme, it calls it again with the --validate-arguments argument for each instance of the modular inputs in its configuration files, feeding XML on stdin to the modular input to validate all enabled inputs. Splunkd calls the modular input the same way again whenever the modular input's configuration is changed.

In our random_numbers example, we're using external validation, since we want the max variable to always be greater than the min value, and that count is a non-negative value. Our validateInput method contains basic logic that retrieves the two variables and then compares them to each other:

exports.validateInput = function(definition, done) {
    var min = parseFloat(definition.parameters.min);
    var max = parseFloat(definition.parameters.max);
    var count = parseInt(definition.parameters.count, 10);

    if (min >= max) {
        done(new Error("min must be less than max; found min=" + min + ", max=" + max));
        }
    else if (count < 0) {
        done(new Error("count must be a positive value; found count=" + count));
    }
    else {
        done();
    }
};

The streamEvents method

The streamEvents method is where the event streaming happens. Events are streamed into stdout using an InputDefinition object as input that determines what events are streamed. In the case of the Random Numbers example, for each input, the values are first retrieved and parsed as floats. Then, an Event object is created, its data fields are set, and it's written using the EventWriter.

exports.streamEvents = function(name, singleInput, eventWriter, done) {
    var getRandomFloat = function (min, max) {
        return Math.random() * (max - min) + min;
    };

    var min = parseFloat(singleInput.min);
    var max = parseFloat(singleInput.max);
    var count = parseInt(singleInput.count, 10);

    var errorFound = false;

    for (var i = 0; i < count && !errorFound; i++) {            
        var curEvent = new Event({
            stanza: name,
            data: "number=" + getRandomFloat(min, max)
        });

        try {
            eventWriter.writeEvent(curEvent);
        }
        catch (e) {
            errorFound = true;
            Logger.error(name, e.message);
            done(e);

            return;
        }
    }

    done();
};

Optional: Set up logging

It's best practice for your modular input script to log diagnostic data to splunkd.log. Use a Logger method to write log messages, which include both a standard splunkd.log severity level (such as "DEBUG", "WARN", "ERROR" and so on) and a descriptive message. For instance, the following code is from the Github Commits example, and logs a message if the Github commit has already been indexed:

if (alreadyIndexed > 0) {
    Logger.info(name, "Skipped " + alreadyIndexed.toString() + " already indexed Github commits from " + owner + "/" + repository);
}

To add the modular input to Splunk Enterprise

With your modular input script completed, you're ready to integrate it into Splunk Enterprise. First, package the script, and then install the modular input.
Package the script

To add a modular input that you've created in JavaScript to Splunk Enterprise, you'll need to add the script as a Splunk Enterprise app. First, create the files you'll need to package the script as an app.
Files

Create the following files with the content indicated. Wherever you see modinput_name—whether in the file name or its contents—replace it with the name of your modular input JavaScript file. For example, if your script's name is random_numbers.js, give the file indicated as modinput_name.cmd the name random_numbers.cmd.

modinput_name.cmd

@"%SPLUNK_HOME%"\bin\splunk cmd node "%~dp0\app\modinput_name.js" %*

modinput_name.sh

#!/bin/bash  


current_dir=$(dirname "$0")
"$SPLUNK_HOME/bin/splunk" cmd node "$current_dir/app/modinput_name.js" $@

package.json

When creating this file, replace the values given with the corresponding values for your modular input. All values (except the splunk-sdk dependency, which should stay at ">=1.4.0") can be changed.

{
    "name": "modinput_name",
    "version": "0.0.1",
    "description": "My great modular input",
    "main": "modinput_name.js",
    "dependencies": {
        "splunk-sdk": ">=1.4.0"
    },
    "author": "Me"
}

app.conf

When creating this file, replace the values given with the corresponding values for your modular input:

    The is_configured value determines whether the modular input is preconfigured on install, or whether the user should configure it.
    The is_visible value determines whether the modular input is visible to the user in Splunk Web.

[install]
is_configured = 0

[ui]
is_visible = 0
label = My modular input

[launcher]
author=Me
description=My great modular input
version = 1.0

inputs.conf.spec

When creating this file, in addition to replacing modinput_name with the name of your modular input's JavaScript file, do the following:

    After the asterisk (*), type a description for your modular input.
    Add any arguments to your modular input as shown. You must list every argument that you define in the getScheme method of your script.

[modinput_name://<name>]
*My great modular input.

arg1 = <value>
arg2 = <value>
count = <value>

Directories

Next, create a directory that corresponds to the name of your modular input script—for instance, "modinput_name"—in a location such as your Documents directory. (It can be anywhere; you'll copy the directory over to your Splunk Enterprise directory at the end of this process.)

    Within this directory, create the following directory structure:

    modinput_name/
      bin/
        app/
      default/
      README/

    Copy your modular input script (modinput_name.js) and the files you created in the previous section so that your directory structure looks like this:

    modinput_name/
      bin/
        modinput_name.cmd
        modinput_name.sh
        app/
          package.json
          modinput_name.js
      default/
        app.conf
      README/
        inputs.conf.spec

Install the modular input

Before using your modular input as a data input for your Splunk Enterprise instance, you must first install it.

    Set the $SPLUNK_HOME environment variable to the root directory of your Splunk Enterprise instance.

    Copy the directory you created in Package the script to the following directory:

    $SPLUNK_HOME/etc/apps

    Open a command prompt or terminal window and go to the following directory, where modinput_name is the name of your modular input script:

    $SPLUNK_HOME/etc/apps/modinput_name/bin/app

    Type the following, and then press Enter or Return:

    npm install

    If the install is not successful, first ensure that Node.js is installed. For more information, see Prerequisites. If Node.js is installed, try the following:

        Download and extract a ZIP file of the SDK from Github: https://github.com/splunk/splunk-sdk-javascript/archive/master.zip

        Copy the extracted splunk-sdk-javascript-master folder to the following path, where modinput_name is the name of your modular input script:

        $SPLUNK_HOME/etc/apps/modinput_name/bin/app/node_modules

        Rename the copied folder to splunk-sdk.
    Restart Splunk Enterprise: From Splunk Home, click the Settings menu. Under System, click Server Controls. Click Restart Splunk.

After you have installed your modular input, it appears alongside other data inputs. From Splunk Home, click the Settings menu. Under Data, click Data inputs, and find the name of your modular input. Click Add new and fill in the settings that you specified when you created the modular input script. Click Save.

You've now configured an instance of your modular input as a Splunk Enterprise input.


## How to work with data models and pivots in the Splunk SDK for JavaScript

Starting in Splunk® Enterprise 6, you can use data models to create specialized searches of your datasets. Data models allow you to produce pivot tables, charts, and visualizations on the fly, based on column and row configurations that you select, and without necessarily having to know the Splunk Enterprise search language.

The Splunk SDK for JavaScript includes support for data models and pivots. Using the SDK, you can enable your JavaScript application to read and use data models that have been created on your Splunk Enterprise instance. You can also use the SDK to create data models, but the easiest way to create new data models continues to be logging directly into Splunk Enterprise with a browser.

This topic contains the following sections:

    About data models
    Create a data model
    Retrieve a data model
    See the contents of a data model
    Retrieve a data model object
    Work with data model objects
    Work with pivots
    Accelerate data models and pivots
    Data model example

About data models

Data models map semantic knowledge about one or more datasets. The data model encodes the domain knowledge that is necessary to generate specialized searches of those datasets. Data models are what enable you to use pivots to produce useful reports and dashboards without having to write the searches that generate them. Data models contain data model objects, which are essentially specifications for a dataset. Each data model object represents a different dataset within the larger set of data that Splunk Enterprise indexes.

Data model objects inherit from other data model objects. Each object inherits either from one of three base objects built into Splunk Enterprise (BaseEvent, BaseTransaction, and BaseSearch) or from another object in the same data model. Allowing inheritance only from a fixed set of base classes or in the data model prevents complications involving access control lists (ACLs).

To learn more about data models, see the following topics:

    About data models
    Manage data models
    Design data models and objects

Create a data model

To create an individual data model, you first get the collection of all data models that are accessible to a user with your credentials: After you've connected to Splunk Enterprise, use the dataModels() method of your Service object to retrieve a DataModels collection (which extends the Collection class) that contains the set of data models. Then, use the create() method of the DataModels collection to create a data model (DataModel object) with the following three parameters:

    name - The name of the data model to create
    params - A dictionary of parameters that define the data model
    callback - A callback function to call on error or once the data model has been created on the Splunk Enterprise server

Following is an example that creates a data model called "MyDataModel:"

var dataModelsCollection = service.dataModels(); 
dataModelsCollection.create("MyDataModel", {some: "parameters"}, function (err, dataModel) {  
    ...
});

Retrieve a data model

To retrieve an individual data model, you first get the collection of all data models that are accessible to a user with your credentials. After you've connected to Splunk Enterprise, the dataModels() method of your Service object to retrieve a DataModels collection containing the set of data models. Then, use the fetch() method to reload the object from the server. Next, use the item("data_model") method of the DataModels collection to retrieve the data model with the name specified to the item() method.

Following is an example that retrieves the data model called "internal_audit_logs," which is part of every standard Splunk Enterprise install.

service.dataModels().fetch(function(err, dataModels) { 
    var dataModel = service.dataModels().item("internal_audit_logs");
    ...
}

See the contents of a data model

A data model consists of several metadata fields (its internal name, its human-readable name, and a description), a value that indicates whether acceleration has been enabled (For more information about accelerating data models, see "Accelerate data models and pivots."), and a collection of data model objects.

Retrieve the data model's internal name using the DataModel class' name property. Retrieve its display name using the displayName property. Retrieve its description using the description property. For example:

console.log("Data model named " + dataModel.displayName + " (internal name: " + dataModel.name + ")");
console.log("Description: ");
console.log(dataModel.description);

Retrieve a data model object

The objects in the data model are structured views on a Splunk Enterprise index, and are represented by the DataModelObject class. You can check for and retrieve individual data model objects by name from a data model by using the hasObject() and objectByName() methods of the DataModel object.

Following is an example of the objectByName() method in use:

var searches = dataModel.objectByName("searches");
console.log("Object: " + searches.displayName + " (internal name: " + searches.name + ")");

Work with data model objects

Data model objects are hierarchical; they are arranged in parent-child relationships. The top-level object in any object hierarchy is referred to as a root object. Any object that descends from a root object is a child object.

Child objects inherit calculations and fields from their parent objects. Calculations narrow down the set of data represented by the object, while fields are name/value pairs associated with the object dataset. Each child object can add calculations to the ones it inherits. Fields are used by Pivot designers to define pivot tables and charts. Child objects can optionally have new fields in addition to the fields they inherit from their parent object.

A data model object is uniquely identified by the full list of ancestors from which it inherits. You can get the lineage of an object with the DataModelObject class' lineage property, as demonstrated here, using the join() method to join together ancestors with a "->" symbol:

console.log("\t Lineage:", searches.lineage.join(" -> "));

Fields are represented in the Splunk SDK for JavaScript by instances of the DataModelField class. You can retrieve a field by name with the fieldByName() method of the DataModelObject class, or retrieve an array of all the fields defined on the object with the fieldNames() method. Each field contains a type (DataModelField.type) with one of the following values:

    string
    number
    timestamp
    IPv4 address
    Boolean value
    one of two internal types: object count and child count

You can check whether a field is of any of the preceding types by using the following helper functions, respectively:

    DataModelField.isString()
    DataModelField.isNumber()
    DataModelField.isTimestamp()
    DataModelField.isIPv4()
    DataModelField.isBoolean()
    DataModelField.isObjectcount()
    DataModelField.isChildcount()

You can also retrieve certain attributes of a data model field using the following properties of your DataModelField object:

    fieldName (string): The internal name of this field. 
    displayName (string): A human readable name for this field. 
    type (string): The type of this field.
    multivalue (Boolean): Whether this field is multivalued. 
    required (Boolean): Whether this field is required on events in the object 
    hidden (Boolean): Whether this field should be displayed in a data model UI. 
    editable (Boolean): Whether this field can be edited. 
    comment (string): A comment for this field, or null if there is none. 
    fieldSearch (string): A search query fragment for this field. 
    lineage (string): The lineage of the data model object on which this field is defined. Items are delimited by a dot. This is converted into an array of strings upon construction.

A group of people who are working with sets of similar Splunk Enterprise queries can define a data model object that encapsulates the shared prefix of their various queries, and then use the data model's search command to execute it followed by the rest of their particular query. You can think of a data model object as a kind of stored procedure.

This is a common enough scenario that the data model object provides the startSearch() method (on the DataModelObject class) for it:

searches.startSearch({}, "| stats count by user | sort -count | head 5", done);

    Note: Be aware that, any time you see done as a parameter in examples, the done parameter represents a callback function. 

Work with pivots

The Splunk SDK for JavaScript provides a pivot table interface to the events in data model objects. You create pivots from data model objects. The SDK gives you the same control over pivots that the pivot tool does within Splunk Enterprise. A pivot is created in several stages.

    Note: For a full working example of pivots using the Splunk SDK for JavaScript, see the pivot_async.js example in the examples/node/helloworld/ directory of the SDK. 

First, you define a PivotSpecification instance, setting the fields to use to split rows, split columns, or calculate aggregates for each cell in the table. Then call the DataModelObject object's createPivotSpecification() method to create a new PivotSpecification object.

Then, call the pivot() method on the new PivotSpecification object. That sends a request to the Splunk Enterprise server to get a set of SPL queries that represent that pivot, which you can then use however you want. For example, using the searches data model object we defined in "Retrieve a data model object.":

// create a specification of a pivot on the searches 
// data model object we retrieved previously
var pivotSpecification = searches.createPivotSpecification();

// configure pivotSpecification and generate the pivot
pivotSpecification
    .addRowSplit("user", "Executing user")
    .addRangeColumnSplit("exec_time", {limit: 4})
    .addCellValue("search", "Search Query", "values")
    .pivot(function(pivotErr, pivot) {
        console.log("Pivot search is:", pivot.search);
    }
    .run(done);

Configuring the pivot consists of adding four kinds of entities to it:

    Filters restrict the events to be calculated on in the pivot.
    Cell values describe an aggregate calculation to be done.
    Row splits describe how to split the data along one axis before aggregating it in the cell values.
    Column splits describe how to split the data along the other axis.

The entities are added to a PivotSpecification by calling any of the following methods:

    addBooleanColumnSplit()
    addBooleanRowSplit()
    addCellValue()
    addColumnSplit()
    addFilter()
    addLimitFilter()
    addRangeColumnSplit()
    addRangeRowSplit()
    addRowSplit()
    addTimestampColumnSplit()
    addTimestampRowSplit()

The arguments to each depend on the type of field to be added. Each of these methods is examined in detail in the following sections.
Filters

Filters restrict the events that will be processed by the pivot. They are added by invoking either the addFilter() or addLimitFilter() method.
The addFilter() method adds a Boolean filter on a data model field. The filter is in the form of a constraint. The addFilter() method takes four parameters, in the following order:

    fieldName: The name of the data model field on which to filter.
    comparisonType: The type of the field to compare ("string", "number", "timestamp", "objectCount", "childCount", "ipv4", or "boolean").
    comparisonOp: The comparison operator, such as "=". The comparison operator you can use depends on the comparisonType. See Comparison Types  for lists of available operators.
    compareTo: The value to which to compare the field.

For instance, pivotSpecification.addFilter("field1", "ipv4", "is", false) would add a filter consisting of the constraint ipv4 is false for the field field1.

The addLimitFilter() method adds a limit on the events shown in a pivot by sorting them according to a specified field, and then taking the specified number of events from the beginning or end of the list. The addLimitFilter() method takes five parameters, in the following order:

    fieldName: The name of the data model field on which to filter. (Only a field with a type of "string", "number", or "objectCount" can have a limit filter added to it.)
    sortAttribute: The name of the field to use for sorting.
    sortDirection: The direction to sort events ("ASCENDING", "DESCENDING", or "DEFAULT").
    limit: The number of values from the sorted list to allow through this filter.
    statsFunction: The stats function to use for aggregation before sorting ("count", "dc" (for DISTINCT_COUNT), "average" or "sum"; not all stats functions work with all field types).

For example, the following filter limits the number of distinct values of host to allow, sorted by aggregating the number of users from each host. The filter counts the distinct users that have produced searches from each host, sorts the hosts from largest number of distinct users to smallest, and only admits events with the top 50 hosts into the pivot.

pivotSpecification.addFilter("host", "user", "DESCENDING", 50, "dc");

Cell values

The cells of a pivot table consist of aggregate calculations done on the events that pass through the filters, and which are assigned to that cell by row and column splits. There is only one method for adding cell values (addCellValue()), but not all the aggregating functions defined in the SDK work with all cell values. The functions that can be used with each type are:

    string: "list", "values" (DISTINCT_VALUES), "first", "last", "count", "dc" (DISTINCT_COUNT)
    IPv4: (same as string)
    number: "sum", "count", "average", "max", "min", "stdev", "list", "values" (DISTINCT_VALUES)
    timestamp: "duration", "earliest", "latest", "list", "values" (DISTINCT_VALUES)
    childcount or objectcount: "count"
    Boolean: none. You cannot use Boolean valued fields as cell values.

The addCellValue() method takes the following three parameters:

    fieldName: The name of the field to aggregate.
    label: A human-readable name for this aggregate.
    statsFunction: The function to use for aggregation. See Stats functions for a list of valid functions.

Here are two examples of adding cell values to the PivotSpecification object pivotSpecification, which we retrieved in "Work with pivots.

pivotSpecification
    .addCellValue("host", "Relevant hosts", "values")
    .addCellValue("exec_time", "Longest running job", "max")
    ....;

Row splits

Row splits divide the data in a pivot table into rows before aggregates are calculated for each cell. A PivotSpecification object with more than one row split will result in separate rows for each combination of distinct splits for each row split. So if we have one row split that produces two rows, abcd=0 and abcd=1, and another that produces two, wxyz=a and wxyz=b, then there would be four rows in the split, (abcd=0, wxyz=a), (abcd=1, wxyz=a), (abcd=0, wxyz=b), and (abcd=1, wxyz=b).

Row splits are added to PivotSpecification objects with the following methods:

    addRowSplit(): Adds a row split on a numeric or string valued field, splitting on each distinct value of the field.
    addBooleanRowSplit(): Adds a row split on a Boolean valued field.
    addRangeRowSplit(): Adds a row split on a numeric field, splitting into numeric ranges.
    addTimestampRowSplit(): Adds a row split on a timestamp valued field, binned by the specified bucket size.

To split the row for each distinct value of a field, for fields of type string or number, use the addRowSplit(fieldName, label) method, where fieldName is the name of the field to split and label is a human-readable label to display in a visual representation.

To add row splits on Boolean-valued fields, use the addBooleanRowSplit(fieldName, label, trueDisplayValue, falseDisplayValue) method, which takes the same fieldName and label arguments, but which also requires two arguments (trueDisplayValue and falseDisplayValue) for the labels to display in each row if the field value is true or false.

To add row splits on number-valued fields, use the addRangeRowSplit(fieldName, label, options) method, which splits number-valued fields into ranges similar to the way in which timestamp-valued fields are split. In addition to the fieldName and label arguments, this method also takes an optional dictionary of collection filtering and pagination options (all integers):

    start: The value of the start of the first range, or null to take the lowest value in the events.
    end: The value for the end of the last range, or null to take the highest value in the events.
    step: The width of each range, or null to indicate that Splunk Enterprise will calculate it.
    limit: The maximum number of ranges to split into, or null for no limit.

For example, if you want to only bin values between 0 and 100 and have no more than ten bins, you would call:

.addRangeRowSplit("exec_time", "myFieldLabel", {start:0, end:12, step:5, limit:4});

If you want bins that are 15 wide and started at 12, you would call:

.addRangeRowSplit("exec_time", "myFieldLabel", {start:12, end:null, step:15, limit:null});

Finally, to add row splits on timestamp-valued fields, use the addTimestampRowSplit(fieldName, label, binning) method, which splits the field's (with the name fieldName and the label label) values into ranges of a precision specified by binning. Valid values for the binning parameter are "auto", "year", "month", "day", "hour", "minute", and "second".
Column splits

Column splits are the complement to row splits. They divide events that pass through the filters into sets before aggregates are calculated for each cell. The methods for column splits are identical to those for row splits, but they lack a label argument:

    addColumnSplit(fieldName)
    addBooleanColumnSplit(fieldName, trueDisplayValue, falseDisplayValue)
    addRangeColumnSplit(fieldName, options)
    addTimestampColumnSplit(fieldName, binning)

Accelerate data models and pivots

Data models take advantage of the built-in support for accelerated searches and aggregations in Splunk Enterprise 6. Within Splunk Enterprise, acceleration entails running a search job on a regular schedule and caching its results for use in the data model and any pivots on the data model. Acceleration is enabled with the Splunk SDK for JavaScript by setting the acceleration property to true when creating a DataModel object. Be aware that only public data models can be accelerated.

To check whether a data model has acceleration enabled, use the DataModel object's isAccelerated() method.

To update a data model's acceleration settings, use the DataModel object's update() method. The update() method takes the props parameter, which is a dictionary that specifies acceleration properties. The props object can contain any of the following keys (any keys not set will remain unchanged on the data model):

    enabled: Whether acceleration should be enabled after update.
    earliestTime: A time modifier (e.g., "-2mon") indicating the earliest time relative to now that the acceleration cache should be maintained (for example, the last week, the last two months, or the last three hours.
    cronSchedule: The cron schedule on which the acceleration job should be run. For example, "0 0 * * *".

Enabling acceleration on a data model will accelerate all objects that inherit from BaseEvent in the data model. Objects that inherit from BaseTransaction or BaseSearch cannot be accelerated, and will be unaffected by enabling acceleration.

You can also do ad hoc acceleration, running and managing the caching job yourself. Call the createLocalAccelerationJob() method on a particular data model object to return an acceleration job, as demonstrated here. Be aware that you must cancel the ad hoc job when you are finished querying the results.

var service = ...; // Get a logged in Splunk service with the Splunk SDK for JavaScript

service.dataModels().fetch(function(err, dataModels) {
    var searches = dataModels.item("internal_audit_logs").objectByName("searches");
    var pivotSpec = searches.createPivotSpecification();

    searches.createLocalAccelerationJob("-1d", function(err, accelerationJob) {
        pivotSpec.addRowSplit("user", "Executing user")
            .setAccelerationJob(acclerationJob) // Or, can pass in the SID by passing in accelerationJob.sid
            .addRangeColumnSplit("exec_time", {start: 0, end: 12, step:5, limit:4})
            .addCellValue("search", "Search Query", "values")
            .pivot(function(pivotErr, pivot) {
                console.log("Pivot search is:", pivot.search);
                pivot.run({}, function(err, job) {
                    console.log("We have a job with SID:", job.sid);
                    // Cancel the local acceleration job when done with it
                });
            });
    });
});

Data model example

The following code demonstrates how to perform a few basic actions with one of a data model's objects. The example:

    Connects to Splunk Enterprise. (The full details of this step are not shown, but they are provided in "How to connect to Splunk Enterprise.")
    Retrieves a data model (in this case, "internal_audit_logs," which is included with each standard Splunk Enterprise install).
    Retrieves the "searches" data model object from the query.
    Runs a query on the data model object, appending "| head 5" to the query to return just the first five events.
    Tracks the job for completion.
    Reads the results of the query, and then prints the results to the console.
    Creates a pivot specification by using the data model object as input.
    Splits the pivot's events into groups with a distinct user and no more than four execution time ranges, specifying a list of distinct search queries for each cell.
    Prints the human-readable Splunk Processing Language (SPL) query that implements the pivot.
    Runs the pivot query, and retrieves the pivot's search queries.
    Reads the results of the pivot query, and then prints the results to the console.

    Note: For a full working example of using data models and pivots using the Splunk SDK for JavaScript, see the pivot_async.js example in the examples/node/helloworld/ directory of the SDK.

var service = new splunkjs.Service({
    username: username,
    password: password,
    scheme: scheme,
    host: host,
    port: port,
    version: version
});
 
var searches; // We'll use this later
 
Async.chain([
        // First, we log in.
        function(done) {
            service.login(done);
        },
        
        function(success, done) {
            if (!success) {
                done("Error logging in");
            }
 
            // Now that we're logged in, let's get the data models collection
            service.dataModels().fetch(done);
        },
        function(dataModels, done) {
            // ...and the specific data model we're concerned with
            var dm = dataModels.item("internal_audit_logs");
            // Get the "searches" object out of the "internal_audit_logs" data model
            searches = dm.objectByName("searches");
 
            console.log("Working with object", searches.displayName,
                "in model", dm.displayName);
 
            console.log("\t Lineage:", searches.lineage.join(" -> "));
            console.log("\t Internal name: " + searches.name);
 
            // Run a data model search query, getting the first 5 results
            searches.startSearch({}, "| head 5", done);
        },
        function(job, done) {
            job.track({}, function(job) {
                job.results({}, done);
            });
        },
        function(results, job, done) {
            // Print out the results
            console.log("Results:");
            for (var i = 0; i < results.rows.length; i++) {
                var rowString = " result " + i + ":  ";
                var row = results.rows[i];
                for (var j = 0; j < results.fields.length; j++) {
                    if (row[j] !== null && row[j] !== undefined) {
                        rowString += results.fields[j] + "=" + row[j] + ", ";
                    }
                }
                console.log(rowString);
                console.log("------------------------------");
            }
            
            var pivotSpecification = searches.createPivotSpecification();
            // Each function call here returns a pivotSpecification so we can chain them
            pivotSpecification
                .addRowSplit("user", "Executing user")
                .addRangeColumnSplit("exec_time", {limit: 4})
                .addCellValue("search", "Search Query", "values")
                .run(done);
        },
        function(job, pivot, done) {
            console.log("Query for binning search queries by execution time and executing user:");
            console.log("\t", pivot.prettyQuery);
            job.track({}, function(job) {
                job.results({}, done);
            });
        },
        function(results, job, done) {
            // Print out the results
            console.log("Results:");
            for (var i = 0; i < results.rows.length; i++) {
                var rowString = " result " + i + ":  ";
                var row = results.rows[i];
                for (var j = 0; j < results.fields.length; j++) {
                    if (row[j] !== null && row[j] !== undefined) {
                        rowString += results.fields[j] + "=" + row[j] + ", ";
                    }
                }
                console.log(rowString);
                console.log("------------------------------");
            }
            job.cancel(done);
        }
    ],
    function(err) {
        if (err) {
            console.log("ERROR", err);
            callback(err);
        }
        callback(err);
    }
);

Reference
Comparison operators

This section details the comparison operators that are available when you are adding filters to pivots. For more information, see "Filters" in "Work with pivots".

Following are valid comparison operators for Boolean types:

    "="
    "is"
    "isNull"
    "isNotNull"

Following are valid comparison operators for string types:

    "="
    "is"
    "isNull"
    "isNotNull"
    "contains"
    "doesNotContain"
    "startsWith"
    "endsWith"
    "regex"

Following are valid comparison operators for number types:

    "="
    "!="
    "<"
    ">"
    "<="
    ">="
    "is"
    "isNull"
    "isNotNull"

Following are valid comparison operators for IPv4 types:

    "is"
    "isNull"
    "isNotNull"
    "contains"
    "doesNotContain"
    "startsWith"

Stats functions

This section details the comparison operators that are available when you are configuring pivots. For more information, see "Work with pivots".

    "list"
    "values"
    "first"
    "last"
    "count"
    "dc"
    "sum"
    "average"
    "max"
    "min"
    "stdev"
    "duration"
    "earliest"
    "latest"


## How to run searches and display results using the Splunk SDK for JavaScript

Searches run in different modes, determining when and how you can retrieve results:

    Normal: A normal search runs asynchronously. It returns a search job immediately. Poll the job to determine its status. You can retrieve the results when the search has finished. You can also preview the results if "preview" is enabled. Normal mode works with real-time searches.
    Blocking: A blocking search runs synchronously. It does not return a search job until the search has finished, so there is no need to poll for status. Blocking mode doesn't work with real-time searches.
    Oneshot: A oneshot search is a blocking search that is scheduled to run immediately. Instead of returning a search job, this mode returns the results of the search once completed. Because this is a blocking search, the results are not available until the search has finished.
    Export: An export search is another type of search operation that runs immediately, does not create a job for the search, and starts streaming results immediately.
    At this time, the Splunk® SDK for JavaScript does not support this type of search.

For those searches that produce search jobs (normal and blocking), the search results are saved for a period of time on the server and can be retrieved on request. For those searches that stream the results (oneshot and export), the search results are not retained on the server. If the stream is interrupted for any reason, the results are not recoverable without running the search again.
The job APIs

The classes for working with jobs are:

    The splunkjs.Service.Jobs class for a collection of search jobs.
    The splunkjs.Service.Job class for an individual search job.

Access these classes through an instance of the splunkjs.Service class. Retrieve a collection, and from there you can access individual items in the collection and create new ones.
Code examples

This section provides examples of how to use the job APIs, assuming you first connect to a Splunk instance:

    To list search jobs for the current user
    To create a blocking search and display properties and results of the job
    To paginate through a large set of results
    To create a normal search, poll for completion, and display results
    To create a basic oneshot search and display results

The following parameters are available for search jobs:

    Collection parameters
    Search job parameters (properties to set)
    Search job parameters (properties to retrieve)

To list search jobs for the current user

This example gets the collection of jobs available to the current user and lists the search ID for each job:

// Retrieve the collection of jobs
var myJobs = service.jobs();

myJobs.fetch(function(err, jobs) {
  
  // Determine how many jobs are in the collection
  var jobsList = myJobs.list() || [];
  console.log("There are " + jobsList.length + " jobs available to the current user");

  // Loop through the collection and display each job's SID
  for(var i = 0; i < jobsList.length; i++) {
    console.log((i + 1) + ": " + jobsList[i].sid);
  };
});

To create a blocking search and display properties and results of the job

Running a blocking search creates a search job and runs the search synchronously in "blocking" mode. The job is returned after the search has finished and all the results are in.

When you create a search job, you need to set the parameters of the job as a dictionary of key-value pairs. For a list of all the possible parameters, see Search job parameters.

This example runs a blocking search, waits for the job to finish, displays some statistics for the completed job, and then displays the results:

// Search everything and return the first 100 results
var searchQuery = "search * &#124; head 100";

// Set the search parameters
var searchParams = {
  exec_mode: "blocking",
  earliest_time: "2012-06-20T16:27:43.000-07:00"
};

// A blocking search returns the job's SID when the search is done
console.log("Wait for the search to finish...");

// Run a blocking search and get back a job
service.search(
  searchQuery,
  searchParams,
  function(err, job) {
    console.log("...done!\n");

    // Get the job from the server to display more info
    job.fetch(function(err){
      // Display properties of the job
      console.log("Search job properties\n---------------------");
      console.log("Search job ID:         " + job.sid);
      console.log("The number of events:  " + job.properties().eventCount); 
      console.log("The number of results: " + job.properties().resultCount);
      console.log("Search duration:       " + job.properties().runDuration + " seconds");
      console.log("This job expires in:   " + job.properties().ttl + " seconds");

      // Get the results and display them
      job.results({}, function(err, results) {
        var fields = results.fields;
        var rows = results.rows;
        for(var i = 0; i < rows.length; i++) {
          var values = rows[i];
          console.log("Row " + i + ": ");
          for(var j = 0; j < values.length; j++) {
            var field = fields[j];
            var value = values[j];
            console.log("  " + field + ": " + value);
          }
        }
      })

    });
    
  }
);

To paginate through a large set of results

The maximum number of results you can retrieve at a time from your search results is determined by the maxresultrows field, which is specified in a Splunk configuration file. We don't recommend changing the default value of 50,000. If your job has more results than this limit, just retrieve your results in sets (0-49999, then 50000-99999, and so on), using the "count" and "offset" parameters to define how many results to retrieve at a time. Set "count" (the number of results in a set) to maxresultrows (or a smaller value), and increment "offset" by this same value to page through each set.

The following example shows how to retrieve search results in sets, using a count of 10 for this example:

// Search everything and return the first 100 results
var searchQuery = "search * | head 100";

// Set the search parameters
var searchParams = {
  exec_mode: "blocking",
  earliest_time: "2012-06-20T16:27:43.000-07:00"
};

// Create a blocking search, which returns the job's SID when the search is done
console.log("Wait for the search to finish...");

// Create the search, wait for it to finish, and get back a job
service.search(
  searchQuery,
  searchParams,
  function(err, job) {
    console.log("...done!\n");

    // Get the job from the server to display more info
    job.fetch(function(err){
      // Display properties of the job
      console.log("Search job properties\n---------------------");
      console.log("Search job ID:         " + job.sid);
      console.log("The number of results: " + job.properties().resultCount);
      console.log("Search duration:       " + job.properties().runDuration + " seconds");
      console.log("This job expires in:   " + job.properties().ttl + " seconds");

      // Page through results by looping through sets of 10 at a time
      var resultCount = job.properties().resultCount; // Number of results this job returned
      var myOffset = 0;         // Start at result 0
      var myCount = 10;         // Get sets of 10 results at a time

      // Run an asynchronous while loop using the Async.whilst helper function to
      // loop through each set of results 
      splunkjs.Async.whilst(

        // Condition--loop while there are still results to display
        function() {
          return (myOffset < resultCount);
        },

        // Body--display each set of results
        function(done) {
          // Get one set of results
          job.results({count: myCount,offset:myOffset}, function(err, results) {

            // Display results
            var fields = results.fields;
            var rows = results.rows;
            for(var i = 0; i < rows.length; i++) {
              var values = rows[i];
              console.log("*** Row " + i + ": ");
              for(var j = 0; j < values.length; j++) {
                var field = fields[j];
                var value = values[j];
                console.log(" " + field + ": " + value);
              }
            }
            
            // Increase the offset to get the next set of results
            // once we are done processing the current set.
            myOffset = myOffset + myCount;
            done();
          })
        },

        // Done
        function(err){
          if (err) console.log("Error: " + err);
        }
      );
    });
  }
);

To create a normal search, poll for completion, and display results

Running a normal search creates a search job and immediately returns the search ID, so you need to poll the job to find out when the search has finished.

When you create a search job, set the parameters of the job as a dictionary of key-value pairs. For a list of all the possible parameters, see Search job parameters.

This example runs a normal search, waits for the job to finish, and then displays the results along with some final statistics:

// Search everything and return the first 100 results
var searchQuery = "search * | head 100";

// Set the search parameters
var searchParams = {
  exec_mode: "normal",
  earliest_time: "2012-06-20T16:27:43.000-07:00"
};

// Run a normal search that immediately returns the job's SID
service.search(
  searchQuery,
  searchParams,
  function(err, job) {

    // Display the job's search ID
    console.log("Job SID: ", job.sid);

    // Poll the status of the search job
    job.track({period: 200}, {
      done: function(job) {
        console.log("Done!");

        // Print out the statics
        console.log("Job statistics:");
        console.log("  Event count:  " + job.properties().eventCount); 
        console.log("  Result count: " + job.properties().resultCount);
        console.log("  Disk usage:   " + job.properties().diskUsage + " bytes");
        console.log("  Priority:     " + job.properties().priority);

        // Get the results and print them
        job.results({}, function(err, results, job) {
          var fields = results.fields;
          var rows = results.rows;
          for(var i = 0; i < rows.length; i++) {
            var values = rows[i];
            console.log("Row " + i + ": ");
            for(var j = 0; j < values.length; j++) {
              var field = fields[j];
              var value = values[j];
              console.log("  " + field + ": " + value);
            }
          }
        });
        
      },
      failed: function(job) {
        console.log("Job failed")
      },
      error: function(err) {
        done(err);
      }
    });

  }
);

To create a basic oneshot search and display results

Unlike other searches, the oneshot search does not create a search job, so you can't access it using the splunkjs.Service.Jobs class. Instead, use the splunkjs.Service.oneshotSearch method. To set properties for the search (for example, to specify a time range to search), you'll need to create a dictionary of key-value pairs. Some common parameters are:

    output_mode: Specifies the output format of the results (XML, JSON, JSON_COLS, JSON_ROWS, CSV, ATOM, or RAW).
    earliest_time: Specifies the earliest time in the time range to search. The time string can be a UTC time (with fractional seconds), a relative time specifier (to now), or a formatted time string.
    latest_time: Specifies the latest time in the time range to search. The time string can be a UTC time (with fractional seconds), a relative time specifier (to now), or a formatted time string.
    rf: Specifies one or more fields to add to the search.

For a full list of possible properties, see the list of Search job parameters, although most of these parameters don't apply to a oneshot search.

This example runs a oneshot search within a specfied time range and displays the results.

    Note: If you don't see any search results, that means there aren't any in the specified time range. Just modify the date and time as needed for your data set.


// Search everything and return the first 10 results
var searchQuery = "search * | head 10";

// Set the search parameters--specify a time range
var searchParams = {
  earliest_time: "2011-06-19T12:00:00.000-07:00",
  latest_time: "2012-12-02T12:00:00.000-07:00"
};

// Run a oneshot search that returns the job's results
service.oneshotSearch(
  searchQuery,
  searchParams,
  function(err, results) {
    // Display the results
    var fields = results.fields;
    var rows = results.rows;
    
    for(var i = 0; i < rows.length; i++) {
      var values = rows[i];
      console.log("Row " + i + ": ");
      
      for(var j = 0; j < values.length; j++) {
        var field = fields[j];
        var value = values[j];
        console.log("  " + field + ": " + value);
      }
    }
  }
);

Collection parameters

By default, all entries are returned when you retrieve a collection. But by using the parameters below, you can also specify the number of entities to return and how to sort them. These parameters are available whenever you retrieve a collection.
Parameter	Description
count	A number that indicates the maximum number of entities to return.
offset	A number that specifies the index of the first entity to return.
search	A string that specifies a search expression to filter the response with, matching field values against the search expression. For example, "search=foo" matches any object that has "foo" as a substring in a field, and "search=field_name%3Dfield_value" restricts the match to a single field.
sort_dir	An enum value that specifies how to sort entities. Valid values are "asc" (ascending order) and "desc" (descending order).
sort_key	A string that specifies the field to sort by.
sort_mode	An enum value that specifies how to sort entities. Valid values are "auto", "alpha" (alphabetically), "alpha_case" (alphabetically, case sensitive), or "num" (numerically).
Search job parameters
Properties to set

The parameters you can use for search jobs correspond to the parameters for the search/jobs endpoint in the REST API.

This list summarizes the properties you can set for a search job. For examples of setting these properties, see To create a blocking search and display properties and results of the job and To create a normal search, poll for completion, and display results.
Parameter	Description
search	Required. A string that contains the search query.
auto_cancel	The number of seconds of inactivity after which to automatically cancel a job. 0 means never auto-cancel.
auto_finalize_ec	The number of events to process after which to auto-finalize the search. 0 means no limit.
auto_pause	The number of seconds of inactivity after which to automatically pause a job. 0 means never auto-pause.
earliest_time	A time string that specifies the earliest time in the time range to search. The time string can be a UTC time (with fractional seconds), a relative time specifier (to now), or a formatted time string. For a real-time search, specify "rt".
enable_lookups	A Boolean that indicates whether to apply lookups to events.
exec_mode	An enum value that indicates the search mode ("blocking", "oneshot", or "normal").
force_bundle_replication	A Boolean that indicates whether this search should cause (and wait depending on the value of "sync_bundle_replication") bundle synchronization with all search peers.
id	A string that contains a search ID. If unspecified, a random ID is generated.
index_earliest	A string that specifies the time for the earliest (inclusive) time bounds for the search, based on the index time bounds. The time string can be a UTC time (with fractional seconds), a relative time specifier (to now), or a formatted time string.
index_latest	A string that specifies the time for the latest (inclusive) time bounds for the search, based on the index time bounds. The time string can be a UTC time (with fractional seconds), a relative time specifier (to now), or a formatted time string.
latest_time	A time string that specifies the latest time in the time range to search. The time string can be a UTC time (with fractional seconds), a relative time specifier (to now), or a formatted time string. For a real-time search, specify "rt".
max_count	The number of events that can be accessible in any given status bucket.
max_time	The number of seconds to run this search before finalizing. Specify 0 to never finalize.
namespace	A string that contains the application namespace in which to restrict searches.
now	A time string that sets the absolute time used for any relative time specifier in the search.
reduce_freq	The number of seconds (frequency) to run the MapReduce reduce phase on accumulated map values.
reload_macros	A Boolean that indicates whether to reload macro definitions from the macros.conf configuration file.
remote_server_list	A string that contains a comma-separated list of (possibly wildcarded) servers from which to pull raw events. This same server list is used in subsearches.
rf	A string that adds one or more required fields to the search.
rt_blocking	A Boolean that indicates whether the indexer blocks if the queue for this search is full. For real-time searches.
rt_indexfilter	A Boolean that indicates whether the indexer pre-filters events. For real-time searches.
rt_maxblocksecs	The number of seconds indicating the maximum time to block. 0 means no limit. For real-time searches with "rt_blocking" set to "true".
rt_queue_size	The number indicating the queue size (in events) that the indexer should use for this search. For real-time searches.
search_listener	A string that registers a search state listener with the search. Use the format: search_state;results_condition;http_method;uri;
search_mode	An enum value that indicates the search mode ("normal" or "realtime"). If set to "realtime", searches live data. A real-time search is also specified by setting "earliest_time" and "latest_time" parameters to "rt", even if the search_mode is normal or is not set.
spawn_process	A Boolean that indicates whether to run the search in a separate spawned process. Searches against indexes must run in a separate process.
status_buckets	The maximum number of status buckets to generate. 0 means to not generate timeline information.
sync_bundle_replication	A Boolean that indicates whether this search should wait for bundle replication to complete.
time_format	A string that specifies the format to use to convert a formatted time string from {start,end}_time into UTC seconds.
timeout	The number of seconds to keep this search after processing has stopped.
Properties to retrieve

This list summarizes the properties that are available for an existing search job:
Property	Description
cursorTime	The earliest time from which no events are later scanned.
delegate	For saved searches, specifies jobs that were started by the user.
diskUsage	The total amount of disk space used, in bytes.
dispatchState	The state of the search. Can be any of QUEUED, PARSING, RUNNING, PAUSED, FINALIZING, FAILED, DONE.
doneProgress	A number between 0 and 1.0 that indicates the approximate progress of the search.
dropCount	For real-time searches, the number of possible events that were dropped due to the "rt_queue_size".
eai:acl	The access control list for this job.
eventAvailableCount	The number of events that are available for export.
eventCount	The number of events returned by the search.
eventFieldCount	The number of fields found in the search results.
eventIsStreaming	A Boolean that indicates whether the events of this search are being streamed.
eventIsTruncated	A Boolean that indicates whether events of the search have not been stored.
eventSearch	Subset of the entire search before any transforming commands.
eventSorting	A Boolean that indicates whether the events of this search are sorted, and in which order ("asc" for ascending, "desc" for descending, and "none" for not sorted).
isDone	A Boolean that indicates whether the search has finished.
isFailed	A Boolean that indicates whether there was a fatal error executing the search (for example, if the search string syntax was invalid).
isFinalized	A Boolean that indicates whether the search was finalized (stopped before completion).
isPaused	A Boolean that indicates whether the search has been paused.
isPreviewEnabled	A Boolean that indicates whether previews are enabled.
isRealTimeSearch	A Boolean that indicates whether the search is a real time search.
isRemoteTimeline	A Boolean that indicates whether the remote timeline feature is enabled.
isSaved	A Boolean that indicates whether the search is saved indefinitely.
isSavedSearch	A Boolean that indicates whether this is a saved search run using the scheduler.
isZombie	A Boolean that indicates whether the process running the search is dead, but with the search not finished.
keywords	All positive keywords used by this search. A positive keyword is a keyword that is not in a NOT clause.
label	A custom name created for this search.
messages	Errors and debug messages.
numPreviews	Number of previews that have been generated so far for this search job.
performance	A representation of the execution costs.
priority	An integer between 0-10 that indicates the search's priority.
remoteSearch	The search string that is sent to every search peer.
reportSearch	If reporting commands are used, the reporting search.
request	GET arguments that the search sends to splunkd.
resultCount	The total number of results returned by the search, after any transforming commands have been applied (such as stats or top).
resultIsStreaming	A Boolean that indicates whether the final results of the search are available using streaming (for example, no transforming operations).
resultPreviewCount	The number of result rows in the latest preview results.
runDuration	A number specifying the time, in seconds, that the search took to complete.
scanCount	The number of events that are scanned or read off disk.
searchEarliestTime	The earliest time for a search, as specified in the search command rather than the "earliestTime" parameter. It does not snap to the indexed data time bounds for all-time searches (as "earliestTime" and "latestTime" do).
searchLatestTime	The latest time for a search, as specified in the search command rather than the "latestTime" parameter. It does not snap to the indexed data time bounds for all-time searches (as "earliestTime" and "latestTime" do).
searchProviders	A list of all the search peers that were contacted.
sid	The search ID number.
ttl	The time to live, or time before the search job expires after it has finished.


## How to get data into Splunk Enterprise using the Splunk SDK for JavaScript

Getting data into Splunk Enterprise involves taking data from inputs, and then indexing that data by transforming it into individual events that contain searchable fields. Here's a brief overview of how it all works.
Data inputs

A data input is a source of incoming event data. Splunk can index data from the following types of inputs:

    Files and directories—the contents of files and directories of files. You can upload a file for one-time indexing (a oneshot input), monitor for new data, or monitor for file system changes (events are generated when the directory undergoes a change). Files and directories can be included using whitelists, and excluded using blacklists.
    Network events—data that is received over network Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) ports, such as data that is sent from a Splunk forwarder from a remote computer. TCP inputs are separated into raw (unprocessed) and cooked (processed) inputs, with SSL as an option for either type.
    Windows data—data from Windows computers, which includes:
        Windows event log data
        Windows Registry data
        Windows Management Instrumentation (WMI) data
        Active Directory data
        Performance monitoring (perfmon) data
    Other data sources—data from custom apps, FIFO queues, scripts that get data from APIs, and other remote data interfaces and message queues.

Your data inputs and their configurations are saved in the inputs.conf configuration file.

    Note: At this time, you can't use the Splunk SDK for JavaScript to create or modify data inputs. However, you can send events directly to an index.


Indexes

The index stores compressed, raw event data. When receiving data from your inputs, Splunk parses the data into events and then indexes them, as follows:

    During parsing, Splunk extracts default fields, configures character-set encoding, identifies line termination, identifies timestamps (creating them if they aren't there), masks sensitive or private data, and can apply custom metadata. Parsing can be done by heavy forwarders. Universal forwarders do minimal parsing.
    During indexing, Splunk breaks events into segments, builds the index data structures, and writes the raw data and index files to disk.

Splunk can usually determine the data type and handle the data accordingly. But when setting up new inputs, you might consider sending data to a test index first to make sure everything is configured the way you want. You can delete the indexed data (clean the index) and start over as needed. Event processing rules are set in the props.conf configuration file, which you'll need to modify directly if you want to reconfigure how events are processed.

Each index is stored as a collection of database directories (also known as buckets) in the file system, located in $SPLUNK_HOME/var/lib/splunk. Buckets are organized by age:

    Hot buckets are searchable, actively being written to, one per index. Hot buckets roll to warm at a certain size or when splunkd is restarted, then a new hot bucket is created.
    Warm buckets are searchable. Oldest warm buckets roll to cold when the number of warm buckets reaches a number limit.
    Cold buckets are searchable. After a set period of time, cold buckets roll to frozen.
    Frozen buckets are not searchable. These buckets are archived or deleted.

You can configure aspects such as the path configuration for your buckets. For example, keep the hot and warm buckets on a local computer for quick access, and put the cold and frozen buckets on separate disks for long-term storage. You can also set the storage size.

By default, data is stored in the main index, but you can add more indexes for different data inputs. You might want multiple indexes to:

    Control user access. Users can search only in indexes they are allowed to by their assigned role.
    Accommodate varying retention policies. Set a different archive or retention policy by index.
    Speed searches in certain situations. Create dedicated indexes for each data source, search just in the index you want.

The index APIs

The classes for working with indexes are:

    The splunkjs.Service.Indexes class for the collection of indexes.
    The splunkjs.Service.Index class for an individual index.

Access these classes through an instance of the splunkjs.Service class. Retrieve a collection, and from there you can access individual items in the collection and create new ones.
Code examples

This section provides examples of how to use the index APIs, assuming you first connect to a Splunk instance:

    To list indexes
    To create a new index
    To add data directly to an index
    To view and modify the properties of an index

Here are the parameters for inputs and indexes:

    Collection parameters
    Index parameters

To list indexes

This example shows how to retrieve and list the indexes that have been configured for Splunk, along with the number of events contained in each. For a list of available parameters to use when retrieving a collection, see Collection parameters.

// Get the collection of indexes
var myindexes = service.indexes();

// Iterate through the indexes and list them
myindexes.fetch(function(err, myindexes) {

  console.log("There are " + myindexes.list().length + " indexes");

  var indexcoll = myindexes.list();

  for(var i = 0; i < indexcoll.length; i++) {
    console.log(i + ": " + indexcoll[i].name);
  }
});

To create a new index

When you create an index, all you need to specify is a name. You can also specify additional properties for the index at the same time by providing a dictionary of key-value pairs (the possible properties are summarized in Index parameters). Or, modify properties after you have created the index.

    Note: To be able to create and modify an index, the user's role must include those capabilites. For a list of available capabilities, see Capabilities.

This example shows how to create a new index.

    Note: If you are using a version of Splunk earlier than 5.0, you can't delete indexes using the SDK or the REST API—something to be aware of before creating lots of test indexes.


// Get the collection of indexes
var myindexes = service.indexes();

// Create the index
myindexes.create("test_index", {}, function(err, newIndex) {
  console.log("The index was created");
});

To add data directly to an index

You can send events directly to an index without configuring a data input. First, retrieve an index using the splunkjs.Service.Index class, and then use either of the following methods (they accomplish the same thing) to send an event over HTTP:

    splunkjs.Service.Index.submitEvent method
    splunkjs.Service.log method

You'll need to provide the event as a string, and you can also specify values to apply to the event (host, source, and sourcetype).

Here is an example of submitting an event over HTTP using Index.submitEvent:

// Get the collection of indexes
var myindexes = service.indexes();

// Get an index to send events to
myindexes.fetch(function(err, myindexes) {
  var myindex = myindexes.item("test_index");

  // Submit an event to the index
  myindex.submitEvent("A new event", {
    sourcetype: "mysourcetype"
  }, function(err, result, myindex) {
    console.log("Submitted event: ", result);
  });
});

To view and modify the properties of an index

This example shows how to view the properties of the index created in the previous example and modify its properties.

To access properties of an index, use the properties method of the index object along with the property's name (see Index parameters for a list of all the possible properties for an index).

To set properties, pass property key-value pairs to the entity's update method to make the changes on the server. Next, call the entity's fetch method to update your local, cached copy of the object with these changes.

    Note: To be able to create and modify an index, the user's role must include those capabilites. For a list of available capabilities, see Capabilities.


// Get the collection of indexes
var myindexes = service.indexes();

// Get the index that was just created
myindexes.fetch(function(err, myindexes) {
  var myindex = myindexes.item("test_index");

  // Display some properties
  console.log("Name:                " + myindex.name);
  console.log("Current DB size:     " + myindex.properties().currentDBSizeMB + "MB");
  console.log("Max hot buckets:     " + myindex.properties().maxHotBuckets);
  console.log("# of hot buckets:    " + myindex.properties().numHotBuckets);
  console.log("# of warm buckets:   " + myindex.properties().numWarmBuckets);
  console.log("Max data size:       " + myindex.properties().maxDataSize);
  console.log("Max total data size: " + myindex.properties().maxTotalDataSizeMB + "MB");

  // Modify some properties
  myindex.update({
    maxTotalDataSizeMB: 1000
  }, function() {
    console.log("\n...properties were modified...")
  });

  // Create a small delay to allow time for the update between server and client
  splunkjs.Async.sleep(2000, function() {
    // Update the local copy of the object with changes
    myindex.fetch(function(err, myindex) {
      console.log("\nUpdated properties:");
      console.log("Max total data size: " + myindex.properties().maxTotalDataSizeMB + "MB");
    });
  });
});

Collection parameters

By default, all entities are returned when you retrieve a collection. But by using the parameters below, you can also specify the number of entities to return and how to sort them. These parameters are available whenever you retrieve a collection:

Parameter	Description
count	A number that indicates the maximum number of entities to return.
offset	A number that specifies the index of the first entity to return.
search	A string that specifies a search expression to filter the response with, matching field values against the search expression. For example, "search=foo" matches any object that has "foo" as a substring in a field, and "search=field_name%3Dfield_value" restricts the match to a single field.
sort_dir	An enum value that specifies how to sort entities. Valid values are "asc" (ascending order) and "desc" (descending order).
sort_key	A string that specifies the field to sort by.
sort_mode	An enum value that specifies how to sort entities. Valid values are "auto", "alpha" (alphabetically), "alpha_case" (alphabetically, case sensitive), or "num" (numerically).
Index parameters

The parameters you can use for working with indexes correspond to the parameters for the data/indexes endpoint in the REST API.

The following parameters are available for indexes:

Parameter	Description
assureUTF8	A Boolean that indicates whether all data retrieved from the index is in proper UTF8 encoding. When true, indexing performance is reduced. This setting is global, not per index.
blockSignatureDatabase	A string that specifies the name of the index that stores block signatures of events. This setting is global, not per index.
blockSignSize	A number that indicates how many events make up a block for block signatures. A value of 0 means block signing has been disabled for this index.
bloomfilterTotalSizeKB	A number that indicates the total size of all bloom filter files, in KB.
bucketRebuildMemoryHint	A string that contains a suggestion for the Splunk bucket rebuild process for the size of the time-series (tsidx) file to make.
coldPath	A string that contains the file path to the cold databases for the index.
coldPath_expanded	A string that contains an absolute path to the cold databases for the index.
coldToFrozenDir	A string that contains the destination path for the frozen archive. Use as an alternative to the "coldToFrozenScript" parameter. The "coldToFrozenDir" parameter takes precedence over "coldToFrozenScript" if both are specified.
coldToFrozenScript	A string that contains the destination path to the archiving script. If your script requires a program to run it (for example, python), specify the program followed by the path. The script must be in $SPLUNK_HOME/bin or one of its subdirectories.
compressRawdata	This parameter is ignored.
currentDBSizeMB	A number that indicates the total size of data stored in the index, in MB. This total includes data in the home, cold, and thawed paths.
defaultDatabase	A string that contains the index destination, which is used when index destination information is not available in the input data.
disabled	A Boolean that indicates whether the index has been disabled.
eai:acl	A string that contains the access control list for this input.
eai:attributes	A string that contains the metadata for this input.
enableOnlineBucketRepair	A Boolean that indicates whether to enable asynchronous online fsck bucket repair, which runs in a concurrent process with Splunk. When enabled, you do not have to wait until buckets are repaired to start Splunk. However, you might observe a slight performance degradation.
enableRealtimeSearch	A Boolean that indicates whether real-time search is enabled. This setting is global, not per index.
frozenTimePeriodInSecs	A number that indicates how many seconds after which indexed data rolls to frozen.
homePath	A string that contains a file path to the hot and warm buckets for the index.
homePath_expanded	A string that contains an absolute file path to the hot and warm buckets for the index.
indexThreads	A number that indicates how many threads are used for indexing. This setting is global, not per index.
isInternal	A Boolean that indicates whether the index in internal.
lastInitTime	A string that contains the last time the index processor was successfully initialized. This setting is global, not per index.
maxBloomBackfillBucketAge	A string that indicates the age of the bucket. If a warm or cold bucket is older than this time, Splunk does not create (or re-create) its bloom filter. The valid format is number followed by a time unit ("s", "m", "h", or "d"), for example "5d".
maxConcurrentOptimizes	A number that indicates how many concurrent optimize processes can run against a hot bucket.
maxDataSize	A string that indicates the maximum size for a hot bucket to reach before a roll to warm is triggered. The valid format is a number in MB, "auto" (Splunk auto-tunes this value, setting the size to 750 MB), or "auto_high_volume" (for high-volume indexes such as the main index, setting the size to 10 GB on 64-bit, and 1 GB on 32-bit systems).
maxHotBuckets	A number that indicates the maximum number of hot buckets that can exist per index. When this value is exceeded, Splunk rolls the least recently used (LRU) hot bucket to warm. Both normal hot buckets and quarantined hot buckets count towards this total. This setting operates independently of "maxHotIdleSecs", which can also cause hot buckets to roll.
maxHotIdleSecs	A number that indicates the maximum life, in seconds, of a hot bucket. When this value is exceeded, Splunk rolls the hot bucket to warm. This setting operates independently of "maxHotBuckets", which can also cause hot buckets to roll. A value of 0 turns off the idle check.
maxHotSpanSecs	A number that indicates the upper bound, in seconds, of the target maximum timespan of hot and warm buckets. If this value is set too small, you can get an explosion of hot and warm buckets in the file system.
maxMemMB	A number that indicates the amount of memory, in MB, that is allocated for indexing.
maxMetaEntries	A number that indicates the maximum number of unique lines in .data files in a bucket, which may help to reduce memory consumption. When set to 0, this parameter is ignored. When this value is exceeded, a hot bucket is rolled to prevent further increase.
maxRunningProcessGroups	A number that indicates the maximum number of processes that the indexer creates at a time. This setting is global, not per index.
maxTime	A string that contains the UNIX timestamp of the newest event time in the index.
maxTimeUnreplicatedNoAcks	A number that specifies the upper limit, in seconds, on how long an event can remain in a raw slice. This value applies only when replication is enabled for this index.
maxTimeUnreplicatedWithAcks	A number that specifies the upper limit, in seconds, on how long events can remain unacknowledged in a raw slice. This value applies only when acks are enabled on forwarders and replication is enabled (with clustering).
maxTotalDataSizeMB	A number that indicates the maximum size of an index, in MB. If an index grows larger than the maximum size, the oldest data is frozen.
maxWarmDBCount	A number that indicates the maximum number of warm buckets. If this number is exceeded, the warm buckets with the lowest value for their latest times are moved to cold.
memPoolMB	A number that indicates how much memory is given to the indexer memory pool. This setting is global, not per index.
minRawFileSyncSecs	A string that indicates how frequently splunkd forces a file system sync while compressing journal slices. This value can be either an integer or "disable". If set to 0, splunkd forces a file system sync after every slice has finished compressing. If set to "disable", syncing is disabled and uncompressed slices are removed as soon as compression is complete. Some file systems are very inefficient at performing sync operations, so only enable this if you are sure it is needed. During this interval, uncompressed slices are left on disk even after they are compressed, then splunkd forces a file system sync of the compressed journal and removes the accumulated uncompressed files.
minTime	A string that contains the UNIX timestamp of the oldest event time in the index.
name	A string that contains the name of the index.
numBloomfilters	A number that indicates how many bloom filters are created for this index.
numHotBuckets	A number that indicates how many hot buckets are created for this index.
numWarmBuckets	A number that indicates how many warm buckets are created for this index.
partialServiceMetaPeriod	A number that indicates how often to sync metadata, in seconds, but only for records where the sync can be done efficiently in place, without requiring a full re-write of the metadata file. Records that require a full re-write are synced at the frequency specified by "serviceMetaPeriod". When set to 0 or a value greater than "serviceMetaPeriod", metadata is not partially synced, but is synced at the frequency specified by "serviceMetaPeriod".
quarantineFutureSecs	A number that indicates a time, in seconds. Events with a timestamp of this value newer than "now" are dropped into a quarantine bucket. This is a mechanism to prevent main hot buckets from being polluted with fringe events.
quarantinePastSecs	A number that indicates a time, in seconds. Events with timestamp of this value older than "now" are dropped into a quarantine bucket. This is a mechanism to prevent the main hot buckets from being polluted with fringe events.
rawChunkSizeBytes	A number that indicates the target uncompressed size, in bytes, for individual raw slice in the raw data journal of the index. If set to 0, "rawChunkSizeBytes" is set to the default value. Note that this value specifies a target chunk size. The actual chunk size may be slightly larger by an amount proportional to an individual event size.
repFactor	A string that contains the replication factor, which is a non-negative number or "auto". This value only applies to Splunk clustering slaves.
rotatePeriodInSecs	A number that indicates how frequently, in seconds, to check whether a new hot bucket needs to be created, and how frequently to check if there are any warm or cold buckets that should be rolled or frozen.
serviceMetaPeriod	A number that indicates how frequently metadata is synced to disk, in seconds.
summarize	A Boolean that indicates whether to omit certain index details to provide a faster response. This parameter is only used when retrieving the index collection.
suppressBannerList	A string that contains a list of indexes to suppress "index missing" warning banner messages for. This setting is global, not per index.
sync	A number that indicates how many events can trigger the indexer to sync events. This setting is global, not per index.
syncMeta	A Boolean that indicates whether to call a sync operation before the file descriptor is closed on metadata file updates.
thawedPath	A string that contains the file path to the thawed (resurrected) databases for the index.
thawedPath_expanded	A string that contains the absolute file path to the thawed (resurrected) databases for the index.
throttleCheckPeriod	A number that indicates how frequently Splunk checks for index throttling condition, in seconds.
totalEventCount	A number that indicates the total number of events in the index.

## How to work with users, roles, and storage passwords using the Splunk SDK for JavaScript

With the Splunk SDKs, you can manage who can access your Splunk Enterprise system and control what they can do by setting up users and assigning them roles. You can also manage secure credentials, or storage passwords.

This topic contains the following sections:

    Users, roles, and storage passwords
    The user and storage password APIs
    Code examples
    Parameter tables

Users, roles, and storage passwords

This section provides a brief overview of users, roles, and storage passwords in Splunk Enterprise:

    Users
    Roles
    Storage passwords

Users

Splunk Enterprise has a single default user ("admin"), and you can add more users (Splunk Free doesn't support user authentication). For each new user you add to your Splunk Enterprise system, you can specify:

    A username and password
    A full name
    An email address
    A default time zone
    A default app
    One or more roles to control what the user can do

Roles

Roles specify what the user is allowed to do in Splunk. Splunk includes these predefined roles:

    admin: This role has the most capabilities.
    power: This role can edit all shared objects and alerts, tag events, and other similar tasks.
    user: This role can create and edit its own saved searches, run searches, edit preferences, create and edit event types, and other similar tasks.
    can_delete: This role has the single capability of deleting by keyword, which is required for using the delete search operator.
    splunk-system-role: This role is based on admin, but has more restrictions on searches and jobs.

    Note: At this time, you can't create or modify roles using the Splunk SDK for JavaScript.

Each role is defined by a combination of these permissions and restrictions:

    Capabilities, which specify the system settings and resources the user is allowed to view or modify. For example, you could allow users to list data inputs but not edit them. For a full list of capabilities, see Capabilities, below.
    Restrictions on searches and search jobs. For example, you can set a limit on the number of concurrent search jobs the user can run, or restrict the data that the user can search by setting a search filter.
    Allowed indexes, to explicitly specify which indexes the user is allowed to search.
    Indexes to search by default.
    Other roles to inherit properties from.

When you inherit other roles, their capabilities, restrictions, and properties are not merged with those of the current role, but rather they are maintained separately. For example, if you list capabilities of a role, its inherited capabilities are not listed—you must explicitly request a list of inherited capabilities. When a role is modified, the changes are made automatically where ever the role is inherited.

You can also assign one or more roles to each user. When multiple roles are assigned, the broadest permissions from these roles are given. Specifically, the user's permissions are the union of all capabilities and the intersection of the restrictions.
Storage passwords

Storage passwords in Splunk Enterprise allow for management of secure credentials. The password is encrypted with a secret key that resides on the same machine. The clear text passwords can be accessed by users who have access to this service. Only users in the admin role can access storage passwords.
The user and storage password APIs

To work with users, use these classes:

    splunkjs.Service.User class for an individual user.
    splunkjs.Service.Users class for a collection of users.

To work with storage passwords, use these classes:

    splunkjs.Service.StoragePassword class for an individual storage password.
    splunkjs.Service.StoragePasswords class for a collection of passwords.

Access these classes through an instance of the splunkjs.Service class. Retrieve a collection, and from there you can access individual items in the collection and create new ones.
Code examples

This section provides examples of how to use the user APIs, assuming you first connect to a Splunk instance:

    To list users and display properties
    To add a new user and modify properties
    To get the current user
    To create a new storage password
    To list storage passwords

To list users and display properties

This example shows how to use the splunkjs.Service.Users class to retrieve the collection of users that have been added to your Splunk system and list them, sorted by the realname field. This example also retrieves a few properties for each user, including their roles. For a list of available parameters to use when retrieving a collection, see Collection parameters.

// Get the collection of users, sorted by realname
var myusers = service.users();

// Print the users' real names, usernames, and roles
myusers.fetch({
  sort_key: "realname",
  sort_dir: "asc"
}, function(err, myusers) {

  console.log("There are " + myusers.list().length + " users:");

  var usercoll = myusers.list();

  for(var i = 0; i < usercoll.length; i++) {
    console.log(usercoll[i].properties().realname + " (" + usercoll[i].name + ")");
    var roles = usercoll[i].properties().roles;
    console.log("  - roles: " + roles);
  }
});

To add a new user and modify properties

This example shows how to create a new user. At a minimum, provide a username and password, and specify one or more roles. You can also specify additional properties for the user at the same time by providing a dictionary of key-value pairs for the properties (the possible properties are summarized in User authentication parameters). Or, modify properties after you have created the user.

Note that when you set properties, the new values overwrite any existing ones. For example, if you set a role, it replaces any existing roles already assigned to the user.

// Get the collection of users
var myusers = service.users();

// Create a new user
newuser = myusers.create({
    name: "testuser",
    password: "yourpassword",
    roles: ["user", "power"]
}, function(err, newuser) {
    console.log("A new user was created");

    // Print the user's properties
    console.log("\nProperties:");
    console.log("Username:    " + newuser.name);
    console.log("Full name:   " + newuser.properties().realname);
    console.log("Default app: " + newuser.properties().defaultApp);
    console.log("Time zone:   " + newuser.properties().tz);
    console.log("Role:        " + newuser.properties().roles);

    // Modify some properties
    newuser.update({
        realname: "Test User",
        defaultApp: "gettingstarted",
        tz: "Europe/Paris"
    }, function() {
        //  Print updated info
        console.log("\n...properties were modified...\n");
        console.log("Updated properties:");
        console.log("Full name:   " + newuser.properties().realname);
        console.log("Default app: " + newuser.properties().defaultApp);
        console.log("Time zone:   " + newuser.properties().tz);
    });
});

To get the current user

This example shows how to find out which user is currently logged in by displaying the current user's real name and username.

// Get the current user
service.currentUser(function(err, user) {
    console.log("Current user: ", user.properties().realname, " (" + user.name + ")");
});</pre>

<a id="newpassword"></a>
<h3>To create a new storage password</h3>
<p>This example shows how to create a new storage password.</p>

<pre>// Get the collection of storage passwords
var storagePasswords = service.storagePasswords();
 
// Create a new storage password
storagePasswords.create({
    name: "Splunker", 
    realm: "SDK", 
    password: "yourpassword"}, 
    function(err, storagePassword) {
        if (err) 
            { /* handle error */ }
        else {
        // Storage password was created successfully
        console.log(storagePassword.properties());
        }
    });

To list storage passwords

This example shows how to list the storage passwords visible to the current user.

// List storage passwords
storagePasswords.fetch(
    function(err, storagePasswords) {
        if (err) 
            { /* handle error */ }
        else {
        // Storage password was created successfully
        console.log("Found " + storagePasswords.list().length + " storage passwords");
        }
    });

Parameter tables

Here are the available parameters for working with users, roles, and passwords:

    Capabilities
    Collection parameters
    User authentication parameters

Capabilities

Here are the capabilities you can allow in a role. Check authorize.conf for the most up-to-date version of this list. The admin role has all the capabilities in this list except for the "delete_by_keyword" capability.
Capability name 	What it lets you do
accelerate_datamodel 	Enable or disable acceleration for data models.
accelerate_search 	Enable or disable acceleration for reports. For a role to use this it must also have the schedule_search capability.
admin_all_objects 	Access and modify any object in the system (user objects, search jobs, etc.). (Overrides any limits set in the objects.)
change_authentication 	Change authentication settings and reload authentication.
change_own_password 	User can change their own password.
delete_by_keyword 	Use the "delete" operator in searches.
edit_deployment_client 	Change deployment client settings.
edit_deployment_server 	Change deployment server settings.
edit_dist_peer 	Add and edit peers for distributed search.
edit_forwarders 	Change forwarder settings.
edit_httpauths 	Edit and end user sessions.
edit_input_defaults 	Change default hostnames for input data.
edit_monitor 	Add inputs and edit settings for monitoring files.
edit_roles 	Edit roles and change user/role mappings.
edit_scripted 	Create and edit scripted inputs.
edit_search_server 	Edit general distributed search settings like timeouts, heartbeats, and blacklists.
edit_server 	Edit general server settings like server name, log levels, etc.
edit_splunktcp 	Change settings for receiving TCP inputs from another Splunk instance.
edit_splunktcp_ssl 	Can list or edit any SSL-specific settings for Splunk TCP input.
edit_tcp 	Change settings for receiving general TCP inputs.
edit_udp 	Change settings for UDP inputs.
edit_user 	Create, edit, or remove users.
edit_view_html 	Create, edit, or modify HTML-based views.
edit_web_settings 	Change settings for web.conf.
embed_reports 	Embed reports and disable embedding for embedded reports.
get_diag 	Use the /streams/diag endpoint to get a remote diag from a Splunk instance.
get_metadata 	Use the "metadata" search processor.
get_typeahead 	Use typeahead.
indexes_edit 	Change index settings like file size and memory limits.
input_file 	Add a file as an input.
license_tab 	Access and change the license.
license_edit 	Edit the license.
list_deployment_client 	View deployment client settings.
list_deployment_server 	View deployment server settings.
list_forwarders 	View forwarder settings.
list_httpauths 	View user sessions.
list_inputs 	View list of various inputs, including input from files, TCP, UDP, scripts, etc.
output_file 	Add a file as an output.
pattern_detect 	Controls ability to see and use the Patterns tab in the Search view.
request_remote_tok 	Get a remote authentication token.
rest_apps_management 	Edit settings in the python remote apps handler.
rest_apps_view 	List properties in the python remote apps handler.
rest_properties_get 	Can get information from the services/properties endpoint.
rest_properties_set 	Edit the services/properties endpoint.
restart_splunkd 	Restart Splunk through the server control handler.
rtsearch 	Run real-time searches.
run_debug_commands 	Run debug commands.
schedule_search 	Schedule saved searches, create and update alerts, and review triggered alert information.
schedule_rtsearch 	Schedule real-time saved searches. In order for a user to use this capability their role must also have the schedule_search capability.
search 	Run searches.
use_file_operator 	Use the "file" search operator.
Collection parameters

By default, all entries are returned when you retrieve a collection. But by using the parameters below, you can also specify the number of entities to return and how to sort them. These parameters are available whenever you retrieve a collection.
Name	Datatype	Default	Description
count 	Number 	30 	Maximum number of entries to return. Set value to zero to get all available entries.
offset 	Number 	0 	Index of first item to return.
search 	String 		Response filter, where the response field values are matched against this search expression.

Example:

search=foo matches on any field with the string foo in the name.
search=field_name%3Dfield_value restricts the match to a single field. (Requires URI-encoding.)
sort_dir 	Enum 	asc 	Response sort order:

asc = ascending
desc = descending
sort_key 	String 	name 	Field name to use for sorting.
sort_mode 	Enum 	auto 	Collated ordering:

auto = If all field values are numeric, collate numerically. Otherwise, collate alphabetically.
alpha = Collate alphabetically, not case-sensitive.
alpha_case = Collate alphabetically, case-sensitive.
num = Collate numerically.
summarize 	Bool 	false 	Response type:

true = Summarized response, omitting some index details, providing a faster response.
false = full response.
User authentication parameters

The parameters you can use for user authentication correspond to the parameters for the authentication/* endpoints in the REST API.

The following parameters are available for user authentication:
Name	Description
authString 	Unique identifier for this session.
capabilities 	List of capabilities assigned to role.
defaultApp 	Default app for the user, which is invoked at login.
defaultAppIsUserOverride 	Default app override indicates:
true = Default app overrides the user role default app.
false = Default app does not override the user role default app.
defaultAppSourceRole 	The role that determines the default app for the user, if the user has multiple roles.
email 	User email address.
password 	User password.
realname 	User full name.
restart_background_jobs 	Restart background search job that has not completed when Splunk Enterprise restarts indication:
true = Restart job.
false = Do not restart job.
roles 	Roles assigned to the user.
searchId 	Search ID associated with the session, if it was created for a search job. If it is a login-type session, the value is empty.
timeAccessed 	Last time the session was touched.
type 	User authentication system type:

    LDAP
    Scripted
    Splunk
    System (reserved for system user) 

tz 	User timezone.
username 	Authenticated session owner name. 


## How to work with alerts using the Splunk SDK for JavaScript

You can configure Splunk Enterprise to send alert messages to you and others when real-time or historical search results have met a set of circumstances that you define. These circumstances can include a wide range of threshold- and trend-based scenarios. For more information about alerts in Splunk Enterprise, see "About alerts."

The Splunk SDK for JavaScript gives you programmatic access to your Splunk Enterprise instance's fired alerts.

This topic contains the following sections:

    The fired alert classes
    Code examples
    Alert APIs
    Alert parameters

The fired alert classes

The classes for working with alerts are:

    The splunkjs.Service.FiredAlert class for a fired alert.
    The splunkjs.Service.FiredAlertGroup class for a group of alerts for a given report (saved search in Splunk Enterprise 5).
    The splunkjs.Service.FiredAlertGroupCollection class for a collection of fired alert groups.

Access these classes through an instance of the splunkjs.Service class. Retrieve the collection of all fired alert groups, and from there you can access individual alert groups in the collection and create new ones.

These new APIs are described in detail in "Alert APIs," and in the Splunk SDK for JavaScript API Reference section.
Code examples

This section provides examples of how to use the alert APIs, assuming you first connect to a Splunk Enterprise instance:

    To list fired alerts
    To create an alert group
    To delete an alert group

To list fired alerts

This example shows how to retrieve and list the fired alerts in all the fired alert groups.

function done(err) {
    //error handling logic here
}

// Get the collection of all fired alert groups for the current user
service.firedAlertGroups().fetch(function(err, firedAlertGroups) {

    // Get the list of all fired alert groups, including the all group (represented by "-")
    var firedAlertGroups = firedAlertGroups.list();
    console.log("Fired alert groups:");

    // For each fired alert group...
    for(var a in firedAlertGroups) {
        if (firedAlertGroups.hasOwnProperty(a)) {
            var firedAlertGroup = firedAlertGroups[a];
            firedAlertGroup.list(function(err, firedAlerts) {
                // How many times were this group's alerts fired?
                console.log(firedAlertGroup.name, "(Count:", firedAlertGroup.count(), ")");
                // Print the properties for each fired alert (default of 30 per alert group)
                for(var i = 0; i < firedAlerts.length; i++) {
                    var firedAlert = firedAlerts[i];
                    for(var key in firedAlert.properties()) {
                        if (firedAlert.properties().hasOwnProperty(key)) {
                           console.log("\t", key, ":", firedAlert.properties()[key]);
                        }
                    }
                    console.log();
                }
                console.log("======================================");
            });
        }
    }

    done();
});

For a complete code example that you can run in your browser, see the firedalerts.js example within the SDK's examples directory.

For a version of this example that uses the splunkjs.Async module to run asynchronously, see the firedalerts_async.js example within the SDK's examples directory.
To create an alert group

This example shows how to create an alert group. First, the example specifies the alert group's properties, and then it uses the service.savedSearches().create() function to create an alert group. Using alert-specific properties causes the report (saved search in Splunk Enterprise 5) to be created as an alert. To see all the alert-specific properties, see "Alert parameters." Alert-specific properties begin with "alert".

function done(err) {
    //error handling logic here
}

// Specify properties for the alert.
var alertOptions = {
    name: "My Awesome Alert",
    search: "index=_internal error sourcetype=splunkd* | head 10",
    "alert_type": "always",
    "alert.severity": "2",
    "alert.suppress": "0",
    "alert.track": "1",
    "dispatch.earliest_time": "-1h",
    "dispatch.latest_time": "now",
    "is_scheduled": "1",
    "cron_schedule": "* * * * *"
};

// Create a saved search/report as an alert.
service.savedSearches().create(alertOptions, function(err, alert) {
    // Error checking.
    if (err && err.status === 409) {
        console.error("ERROR: A saved search/report with the name '" + alertOptions.name + "' already exists");
        done();
        return;
    }
    else if (err) {
        console.error("There was an error creating the saved search/report:", err);
        done(err);
        return;
    }
  
    // Confirmation message.
    console.log("Created saved search/report as alert: " + alert.name);            
    done();
});

For a complete code example that you can run in your browser, see the firedalerts_create.js example within the SDK's examples directory.
To delete an alert group

This example shows how to delete an alert group. In particular, this example deletes the alert group that was created in the previous section. However, it does not delete the individual fired alerts within the alert group.

function done(err) {
    //error handling logic here
}

// The alert created earlier.
var name = "My Awesome Alert";

// Retrieve the alert group.
service.savedSearches().fetch(function(err, firedAlertGroups) {
    // Error checking.
    if (err) {
        console.log("There was an error in fetching the alerts");
        done(err);
        return;
    }

    // Get the alert group to delete using its name.
    var alertToDelete = firedAlertGroups.item(name);

    // Does the alert exist?
    if (!alertToDelete) {
        console.log("Can't delete '" + name + "' because it doesn't exist!");
        done();
    }
    else {
        // Delete the alert.
        alertToDelete.remove();
        console.log("Deleted alert: " + name + "");
        done();
    }
});

For a complete code example that you can run in your browser, see the firedalerts_delete.js example within the SDK's examples directory.
Alert APIs

Following are the alert-specific APIs in the Splunk SDK for JavaScript. This section includes the functions that each of the alert classes exposes.

    The splunkjs.Service.FiredAlert class
    The splunkjs.Service.FiredAlertGroup class
    The splunkjs.Service.FiredAlertGroupCollection class
    Alert-specific functions of the splunkjs.Service.SavedSearch class

The splunkjs.Service.FiredAlert class

The splunkjs.Service.FiredAlert class represents a fired alert, and extends the splunkjs.Service.Entity class. You can retrieve several of the fired alert's properties by calling the corresponding function. The splunkjs.Service.FiredAlert class exposes the functions listed in the following table.

    Note: The values that these functions return can also be accessed through the properties() function, which is inherited from the splunkjs.Service.Entity class.


Function	Description
actions()	Returns this alert's actions (such as e-mail notification, running a script, and so on).
alertType()	Returns this alert's type.
expirationTime()	Returns the rendered expiration time for this alert.
init(service, name, namespace)	Constructor for splunkjs.Service.FiredAlert.
isDigestMode()	Returns a Boolean value indicating whether the result is a set of events (a digest) or a single event (one per result).
path()	Returns this alert's REST endpoint path.
savedSearchName()	Returns this alert's report (saved search in Splunk Enterprise 5).
severity()	Returns this alert's severity on a scale of 1 (highest) to 10.
sid()	Returns this alert's search ID (SID).
triggerTime()	Returns the time this alert was triggered.
triggerTimeRendered()	Returns this alert's rendered trigger time.
triggeredAlertCount()	Returns the count of triggered alerts.
The splunkjs.Service.FiredAlertGroup class

The splunkjs.Service.FiredAlertGroup class represents a group of alerts represented by a report (saved search in Splunk Enterprise 5), and extends the splunkjs.Service.Entity class. You can retrieve several of the fired alert group's properties by calling the corresponding function. The splunkjs.Service.FiredAlertGroup class exposes the functions listed in the following table.

    Note: The values that these functions return can also be accessed through the properties() function, which is inherited from the splunkjs.Service.Entity class.


Function	Description
count()	Returns the count of triggered alerts (the triggered_alert_count property).
init(service, name, namespace)	Constructor for splunkjs.Service.FiredAlertGroup.
list()	Returns a list of fired instances of this group's alerts (splunkjs.Service.Job instances).
path()	Returns this group's REST endpoint path.
The splunkjs.Service.FiredAlertGroupCollection class

The splunkjs.Service.FiredAlertGroupCollection class represents a collection of fired alert groups, and extends the splunkjs.Service.Collection class. The splunkjs.Service.FiredAlertGroupCollection class exposes the functions listed in the following table.
Function	Description
init(service, namespace)	Constructor for splunkjs.Service.FiredAlertGroupCollection.
instantiateEntity()	Creates a local instance of a fired alert group, which is part of the fired alert group collection.
path()	Returns this collection's REST endpoint path.
remove()	Suppresses removing alerts via the fired alerts endpoint.
Alert-specific functions of the splunkjs.Service.SavedSearch class

The alert classes work with the following functions of the splunkjs.Service.SavedSearch class:
Function	Description
alertCount()	Returns the count of triggered alerts for an alert group represented by the specified report (saved search in Splunk Enterprise 5).
firedAlertGroup()	Returns the fired alert group (splunkjs.Service.FiredAlertGroup) associated with a given report (saved search in Splunk Enterprise 5).
Alert parameters

The properties that are available for alerts are the same as those for reports (saved searches in Splunk Enterprise 5). Alert-specific properties begin with "alert." The properties available for reports correspond to the parameters for the saved/searches endpoint in the REST API.

This table summarizes the properties you can set for an alert.
Parameter
	
Description
name	Required. A string that contains the name of the report.
search	Required. A string that contains the search query.
action.*	A string with wildcard arguments to specify specific action arguments.
action.email	A Boolean that indicates the state of the email alert action. Read only.
action.email.auth_password	A string that specifies the password to use when authenticating with the SMTP server. Normally this value is set while editing the email settings, but you can set a clear text password here that is encrypted when Splunk is restarted.
action.email.auth_username	A string that specifies the username to use when authenticating with the SMTP server. If this is empty string, authentication is not attempted.
action.email.bcc	A string that specifies the BCC email address to use if "action.email" is enabled.
action.email.cc	A string that specifies the CC email address to use if "action.email" is enabled.
action.email.command	A string that contains the search command (or pipeline) for running the action.
action.email.format	An enum value that indicates the format of text and attachments in the email ("plain", "html", "raw", or "csv"). Use "plain" for plain text.
action.email.from	A string that specifies the email sender's address.
action.email.hostname	A string that specifies the hostname used in the web link (URL) that is sent in email alerts. Valid forms are "hostname" and "protocol://hostname:port".
action.email.inline	A Boolean that indicates whether the search results are contained in the body of the email.
action.email.mailserver	A string that specifies the address of the MTA server to be used to send the emails.
action.email.maxresults	The maximum number of search results to send when "action.email" is enabled.
action.email.maxtime	A number indicating the maximum amount of time an email action takes before the action is canceled. The valid format is number followed by a time unit ("s", "m", "h", or "d"), for example "5d".
action.email.pdfview	A string that specifies the name of the view to deliver if "action.email.sendpdf" is enabled.
action.email.preprocess_results	A string that specifies how to pre-process results before emailing them.
action.email.reportCIDFontList	Members of an enumeration in a space-separated list specifying the set (and load order) of CID fonts for handling Simplified Chinese(gb), Traditional Chinese(cns), Japanese(jp), and Korean(kor) in Integrated PDF Rendering.
action.email.reportIncludeSplunkLogo	A Boolean that indicates whether to include the Splunk logo with the report.
action.email.reportPaperOrientation	An enum value that indicates the paper orientation ("portrait" or "landscape").
action.email.reportPaperSize	An enum value that indicates the paper size for PDFs ("letter", "legal", "ledger", "a2", "a3", "a4", or "a5").
action.email.reportServerEnabled	A Boolean that indicates whether the PDF server is enabled.
action.email.reportServerURL	A string that contains the URL of the PDF report server, if one is set up and available on the network.
action.email.sendpdf	A Boolean that indicates whether to create and send the results as a PDF.
action.email.sendresults	A Boolean that indicates whether to attach search results to the email.
action.email.subject	A string that specifies the subject line of the email.
action.email.to	A string that contains a comma- or semicolon-delimited list of recipient email addresses. Required if this search is scheduled and "action.email" is enabled.
action.email.track_alert	A Boolean that indicates whether running this email action results in a trackable alert.
action.email.ttl	The number of seconds indicating the minimum time-to-live (ttl) of search artifacts if this email action is triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
action.email.use_ssl	A Boolean that indicates whether to use secure socket layer (SSL) when communicating with the SMTP server.
action.email.use_tls	A Boolean that indicates whether to use transport layer security (TLS) when communicating with the SMTP server.
action.email.width_sort_columns	A Boolean that indicates whether columns should be sorted from least wide to most wide, left to right. This value is only used when "action.email.format"="plain", indicating plain text.
action.populate_lookup	A Boolean that indicates the state of the populate-lookup alert action. Read only.
action.populate_lookup.command	A string that specifies the search command (or pipeline) to run the populate-lookup alert action.
action.populate_lookup.dest	A string that specifies the name of the lookup table or lookup path to populate.
action.populate_lookup.hostname	A string that specifies the host name used in the web link (URL) that is sent in populate-lookup alerts. Valid forms are "hostname" and "protocol://hostname:port".
action.populate_lookup.maxresults	The maximum number of search results to send in populate-lookup alerts.
action.populate_lookup.maxtime	The number indicating the maximum amount of time an alert action takes before the action is canceled. The valid format is number followed by a time unit ("s", "m", "h", or "d").
action.populate_lookup.track_alert	A Boolean that indicates whether running this populate-lookup action results in a trackable alert.
action.populate_lookup.ttl	The number of seconds indicating the minimum time-to-live (ttl) of search artifacts if this populate-lookup action is triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
action.rss	A Boolean that indicates the state of the RSS alert action. Read only.
action.rss.command	A string that contains the search command (or pipeline) that runs the RSS alert action.
action.rss.hostname	A string that contains the host name used in the web link (URL) that is sent in RSS alerts. Valid forms are "hostname" and "protocol://hostname:port".
action.rss.maxresults	The maximum number of search results to send in RSS alerts.
action.rss.maxtime	The maximum amount of time an RSS alert action takes before the action is canceled. The valid format is number followed by a time unit ("s", "m", "h", or "d").
action.rss.track_alert	A Boolean that indicates whether running this RSS action results in a trackable alert.
action.rss.ttl	The number of seconds indicating the minimum time-to-live (ttl) of search artifacts if this RSS action is triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
action.script	A Boolean that indicates the state of the script alert action. Read only.
action.script.command	A string that contains the search command (or pipeline) that runs the script action.
action.script.filename	A string that specifies the file name of the script to call, which is required if "action.script" is enabled.
action.script.hostname	A string that specifies the hostname used in the web link (URL) that is sent in script alerts. Valid forms are "hostname" and "protocol://hostname:port".
action.script.maxresults	The maximum number of search results to send in script alerts.
action.script.maxtime	The maximum amount of time a script action takes before the action is canceled. The valid format is number followed by a time unit ("s", "m", "h", or "d").
action.script.track_alert	A Boolean that indicates whether running this script action results in a trackable alert.
action.script.ttl	The number of seconds indicating the minimum time-to-live (ttl) of search artifacts if this script action is triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
action.summary_index	A Boolean that indicates the state of the summary index alert action. Read only.
action.summary_index._name	A string that specifies the name of the summary index where the results of the scheduled search are saved.
action.summary_index.command	A string that contains the search command (or pipeline) that runs the summary-index action.
action.summary_index.hostname	A string that specifies the hostname used in the web link (URL) that is sent in summary-index alerts. Valid forms are "hostname" and "protocol://hostname:port".
action.summary_index.inline	A Boolean that indicates whether to run the summary indexing action as part of the scheduled search.
action.summary_index.maxresults	The maximum number of search results to send in summary-index alerts.
action.summary_index.maxtime	A number indicating the maximum amount of time a summary-index action takes before the action is canceled. The valid format is number followed by a time unit ("s", "m", "h", or "d"), for example "5d".
action.summary_index.track_alert	A Boolean that indicates whether running this summary-index action results in a trackable alert.
action.summary_index.ttl	The number of seconds indicating the minimum time-to-live (ttl) of search artifacts if this summary-index action is triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
actions	A string that contains a comma-delimited list of actions to enable, for example "rss,email".
alert.digest_mode	A Boolean that indicates whether Splunk applies the alert actions to the entire result set or digest ("true"), or to each individual search result ("false").
alert.expires	The amount of time to show the alert in the dashboard. The valid format is number followed by a time unit ("s", "m", "h", or "d").
alert.severity	A number that indicates the alert severity level (1=DEBUG, 2=INFO, 3=WARN, 4=ERROR, 5=SEVERE, 6=FATAL).
alert.suppress	A Boolean that indicates whether alert suppression is enabled for this scheduled search.
alert.suppress.fields	A string that contains a comma-delimited list of fields to use for alert suppression.
alert.suppress.period	A value that indicates the alert suppression period, which is only valid when "Alert.Suppress" is enabled. The valid format is number followed by a time unit ("s", "m", "h", or "d").
alert.track	An enum value that indicates how to track the actions triggered by this report. Valid values are: "true" (enabled), "false" (disabled), and "auto" (tracking is based on the setting of each action).
alert_comparator	A string that contains the alert comparator. Valid values are: "greater than", "less than", "equal to", "rises by", "drops by", "rises by perc", and "drops by perc".
alert_condition	A string that contains a conditional search that is evaluated against the results of the report.
alert_threshold	A value to compare to before triggering the alert action. Valid values are: integer or integer%. If this value is expressed as a percentage, it indicates the value to use when "alert_comparator" is set to "rises by perc" or "drops by perc".
alert_type	A string that indicates what to base the alert on. Valid values are: "always", "custom", "number of events", "number of hosts", and "number of sources". This value is overridden by "alert_condition" if specified.
args.*	A string containing wildcard arguments for any report template argument, such as "args.username"="foobar" when the search is search $username$.
auto_summarize	A Boolean that indicates whether the scheduler ensures that the data for this search is automatically summarized.
auto_summarize.command	A string that contains a search template that constructs the auto summarization for this search.
auto_summarize.cron_schedule	A string that contains the cron schedule for probing and generating the summaries for this report.
auto_summarize.dispatch.earliest_time	A string that specifies the earliest time for summarizing this report. The time can be relative or absolute; if absolute, use the "dispatch.time_format" parameter to format the value.
auto_summarize.dispatch.latest_time	A string that contains the latest time for summarizing this report. The time can be relative or absolute; if absolute, use the "dispatch.time_format" parameter to format the value.
auto_summarize.dispatch.ttl	The number of seconds indicating the time to live (in seconds) for the artifacts of the summarization of the scheduled search. If the value is a number followed by "p", it is the number of scheduled search periods.
auto_summarize.max_disabled_buckets	A number that specifies the maximum number of buckets with the suspended summarization before the summarization search is completely stopped, and the summarization of the search is suspended for the "auto_summarize.suspend_period" parameter.
auto_summarize.max_summary_ratio	A number that specifies the maximum ratio of summary size to bucket size, which specifies when to stop summarization and deem it unhelpful for a bucket. The test is only performed if the summary size is larger than the value of "auto_summarize.max_summary_size".
auto_summarize.max_summary_size	A number that specifies the minimum summary size, in bytes, before testing whether the summarization is helpful.
auto_summarize.max_time	A number that specifies the maximum time (in seconds) that the summary search is allowed to run. Note that this is an approximate time because the summary search stops at clean bucket boundaries.
auto_summarize.suspend_period	A string that contains the time indicating when to suspend summarization of this search if the summarization is deemed unhelpful.
auto_summarize.timespan	A string that contains a comma-delimited list of time ranges that each summarized chunk should span. This comprises the list of available granularity levels for which summaries would be available.
cron_schedule	A string that contains the cron-style schedule for running this report.
description	A string that contains a description of this report.
disabled	A Boolean that indicates whether the report is enabled.
dispatch.*	A string that specifies wildcard arguments for any dispatch-related argument.
dispatch.buckets	The maximum number of timeline buckets.
dispatch.earliest_time	A time string that specifies the earliest time for this search. Can be a relative or absolute time. If this value is an absolute time, use "dispatch.time_format" to format the value.
dispatch.latest_time	A time string that specifies the latest time for this report. Can be a relative or absolute time. If this value is an absolute time, use "dispatch.time_format" to format the value.
dispatch.lookups	A Boolean that indicates whether lookups for this search are enabled.
dispatch.max_count	The maximum number of results before finalizing the search.
dispatch.max_time	The maximum amount of time (in seconds) before finalizing the search.
dispatch.reduce_freq	The number of seconds indicating how frequently Splunk runs the MapReduce reduce phase on accumulated map values.
dispatch.rt_backfill	A Boolean that indicates whether to back fill the real-time window for this search. This value is only used for a real-time search.
dispatch.spawn_process	A Boolean that indicates whether Splunk spawns a new search process when running this report.
dispatch.time_format	A string that defines the time format that Splunk uses to specify the earliest and latest time.
dispatch.ttl	The number indicating the time to live (ttl) for artifacts of the scheduled search (the time before the search job expires and artifacts are still available), if no alerts are triggered. If the value is a number followed by "p", it is the number of scheduled search periods.
displayview	A string that contains the default UI view name (not label) in which to load the results.
is_scheduled	A Boolean that indicates whether this report runs on a schedule.
is_visible	A Boolean that indicates whether this report is visible in the report list.
max_concurrent	The maximum number of concurrent instances of this search the scheduler is allowed to run.
next_scheduled_time	A string that indicates the next scheduled time for this report. Read only.
qualifiedSearch	A string that is computed during run time. Read only.
realtime_schedule	A Boolean that specifies how the scheduler computes the next time a scheduled search is run:

    When "true": The schedule is based on the current time. The scheduler might skip some scheduled periods to make sure that searches over the most recent time range are run.
    When "false": The schedule is based on the last search run time (referred to as "continuous scheduling") and the scheduler never skips scheduled periods. However, the scheduler might fall behind depending on its load. Use continuous scheduling whenever you enable the summary index option ("action.summary_index").

The scheduler tries to run searches that have real-time schedules enabled before running searches that have continuous scheduling enabled.
request.ui_dispatch_app	A string that contains the name of the app in which Splunk Web dispatches this search.
request.ui_dispatch_view	A string that contains the name of the view in which Splunk Web dispatches this search.
restart_on_searchpeer_add	A Boolean that indicates whether a real-time search managed by the scheduler is restarted when a search peer becomes available for this report. The peer can be one that is newly added or one that has become available after being down.
run_on_startup	A Boolean that indicates whether this search is run when Splunk starts. If the search is not run on startup, it runs at the next scheduled time. It is recommended that you set this value to "true" for scheduled searches that populate lookup tables.
vsid	A string that contains the view state ID that is associated with the view specified in the "displayview" attribute.


