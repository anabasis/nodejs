# 설치 및 구성

## Splunk Add-on for AWS 설치

1. <https://splunkbase.splunk.com/app/1876>에서 다운로드하거나 Splunk Web 내의 응용 프로그램 브라우저를 사용하여 Splunk Add-on for AWS을 다운로드.
2. 이 페이지의 표를 사용하여 배치에서 이 Add-on을 설치하는 위치와 방법을 결정.
3. 아래 표에 필수 및 지정된 경우, 설치하기 전에 전제 조건 단계를 수행.
4. 설치를 완료.

특정 배포 환경에서 Add-on을 설치하는 방법에 대한 단계별 지침이 필요하면 [installation walkthroughs](http://docs.splunk.com/Documentation/AddOns/released/AWS/Distributeddeployment#Installation_walkthroughs) 섹션에서 단일 인스턴스 배포, 분산 배포, Splunk 클라우드 또는 Splunk Light와 관련된 설치 지침에 대한 링크를 확인

### 분산 배포

아래 표를 사용하여 Splunk Enterprise의 분산 배포에서이 Add-on을 설치하는 위치와 방법을 결정하고 Forwarder를 사용하여 데이터를 가져 오는 배포를 결정
환경, 기본 설정 및 Add-on을 설치하려면 여러 위치에 Add-on을 설치.

#### 이 Add-on을 설치할 위치

별도로 언급하지 않는 한, 지원되는 모든 Add-on을 분산 형 Splunk 플랫폼 배포의 모든 계층에 안전하게 설치.
자세한 내용은 Splunk Add-on의 [Where to install Splunk add-ons](http://docs.splunk.com/Documentation/AddOns/latest/Overview/Wheretoinstall)를 참조.

이 표는이 특정 Add-on을 Splunk Enterprise의 분산 배포에 설치하기위한 참조를 제공.

<table>
<tr><td>Splunk platform component</td><td>Supported</td><td>Required</td><td>Comments</td></tr>
<tr><td>Search Heads</td><td>Yes</td><td>Yes</td><td>AWS 지식 관리가 필요한 모든 검색헤드에이 Add-on을 설치</td></tr>
<tr><td>Indexers</td><td>Yes</td><td>No</td><td>Heavy Forwarder에서 구문 분석 작업이 수행되므로 필요하지 않음</td></tr>
<tr><td>Heavy Forwarders</td><td>Yes</td><td>Yes</td><td>이 Add-on에는 모듈러 입력을 통한 데이터 수집을 수행하고 Splunk Web에서 AWS를 사용하여 설정 및 인증을 수행 할 수 있도록 Heavy Forwarder가 필요</td></tr>
<tr><td>Universal Forwarders</td><td>No</td><td>No</td><td>이 Add-on에는 중량이 Heavy Forwarder가 필요</td></tr>
<tr><td>Light Forwarders</td><td>No</td><td>No</td><td>이 Add-on에는 중량이 Heavy Forwarder가 필요</td></tr>
</table>

#### 분산 배포 호환성

이 표는이 Add-on과 Splunk 배포 배포 기능의 호환성에 대한 빠른 참조를 제공합니다.

<table>
<tr><td>Distributed deployment feature</td><td>Supported</td><td>Comments</td></tr>
<tr><td>Search Head Clusters</td><td>Yes</td><td>이 검색 기능을 모든 검색 시간 기능의 검색헤드 클러스터에 설치할 수 있지만 데이터 수집이 중복되지 않도록 입력 장치를 구성.
이 Add-on을 클러스터에 설치하기 전에 Add-on 패키지를 다음과 같이 변경.

1.`eventgen.conf` 파일과 `samples` 폴더에있는 모든 파일들을 제거.
2.`inputs.conf` 파일을 제거.

</td></tr>
<tr><td>Indexer Clusters</td><td>Yes</td><td>이 Add-on을 클러스터에 설치하기 전에 Add-on 패키지를 다음과 같이 변경.

1.`eventgen.conf` 파일과`samples` 폴더에있는 모든 파일들을 제거.
2.`inputs.conf` 파일을 제거.

</td></tr>
<tr><td>Deployment Server</td><td>No</td><td>구성되지 않은 Add-on만 배포.
배포 서버를 사용하여 구성된 Add-on을 데이터 수집기 역할을하는 여러 Forwarder에 배포하면 데이터가 중복.
Add-on은 자격 증명을 사용하여 자격 증명을 보호하며이 자격 증명 관리 솔루션은 배포 서버와 호환되지 않음.</td></tr>
</table>

### 설치 연습

Splunk Add-on 설명서에는 Splunk 지원 Add-on을 성공적으로 설치하는 데 도움이되는 [Installing add-ons](http://docs.splunk.com/Documentation/AddOns/latest/Overview/Installingadd-ons) 안내서가 포함 Splunk 플랫폼에 연결.

설치 절차를 보려면 배포 시나리오와 일치하는 링크 참조.

- Single-instance Splunk Enterprise
- Distributed Splunk Enterprise
- Splunk Cloud
- Splunk Light

## Splunk Add-on for AWS 계정 관리

Splunk Web을 사용하여 데이터 수집 노드의 Splunk Add-on for AWS (일반적으로 무거운 Forwarder)의 계정, 프록시 연결 및 로그 수준을 관리. 구성 파일을 사용하여 이러한 항목을 관리하는 것은 지원되지 않음.

Splunk Add-on for AWS은 데이터를 수집하기 위해 AWS에 연결하는 두 가지 방법, 즉 EC2 IAM 역할 및 AWS 사용자 계정을 지원.

### EC2 IAM 역할 발견

상업 지역을 사용하여 자신의 관리 AWS 환경에서 Splunk 플랫폼의 데이터 수집 노드를 실행하는 경우 EC2에 대한 IAM 역할을 설정하고 해당 역할을 사용하여 데이터 수집 작업을 구성. AWS의 Splunk Add-on은 일단 설정되면 자동으로이 역할을 찾음.

> 자동 발견 된 EC2 IAM 역할을 사용하여 데이터를 수집하는 것은 중국 또는 GovCloud 지역에서 지원되지 않음.

1. AWS 설명서에 따라 EC2에 대한 IAM 역할을 설정. <http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html>.
2. 이 역할에 [Configure AWS permissions for the Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWSpermissions)에 지정된 필수 권한이 ​​모두 있는지 확인하십시오. 모든 입력에 필요한 모든 권한을 부여하지 않으려면이 역할의 권한이 적용되지 않는 다른 입력에 사용할 수있는 AWS 계정을 구성.
3. Splunk Web의 홈 페이지 왼쪽 탐색 모음에서 AWS의 Splunk Add-on을 클릭.
4. 앱 탐색 막대에서 구성을 클릭합니다. 기본적으로 Add-on은 계정 탭을 표시.
5. 자동 검색된 IAM 역할 열에서 EC2 IAM 역할을 찾습니다. 자체 AWS 관리 환경에 있고 EC2 IAM 역할이 구성된 경우이 계정 목록에 자동으로 나타남.

더 이상 구성 할 필요가 없음. EC2 IAM 역할과 사용자 계정을 모두 사용하여 AWS 데이터를 수집하려는 경우 AWS 계정을 구성 할 수도 있음.

> Add-on에서 EC2 IAM 역할을 편집하거나 삭제할 수 없음.

### AWS 계정 관리

AWS 계정을 추가하려면 다음을 수행.

1. Splunk Web의 홈 페이지에서 왼쪽 탐색 모음의 Splunk Add-on for AWS을 클릭.
2. 앱 탐색 막대에서 구성을 클릭합니다. 기본적으로 Add-on은 계정 탭을 표시.
3. 추가를 클릭.
4. AWS 계정의 이름을 입력. 계정을 구성한 후에는 친숙한 이름을 변경할 수 없음.
5. Splunk 플랫폼이 AWS 데이터에 액세스 할 때 사용해야 하는 AWS 계정의 자격 증명 (키 ID 및 비밀 키)을 입력. 여기에서 구성하는 계정에는 수집하려는 AWS 데이터에 액세스 할 수있는 적절한 권한이 있어야 함. 자세한 내용은 [Configure AWS permissions for the Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWSpermissions) 참조.
6. 계정의 Region Category를 선택. 가장 흔한 것은 Global.
7. 추가를 클릭.

Action 열에서 편집을 클릭하여 기존 계정을 편집.

Action 열에서 삭제를 클릭하여 기존 계정을 삭제.
Add-on을 사용하면 하나 이상의 입력에서 사용중인 계정을 삭제할 수 없으며 입력이 비활성화 된 경우에도 삭제할 수 없음.
입력을 삭제하거나 다른 계정을 사용하도록 편집한 다음 계정을 삭제할 수 있음.

> 사용자 정의 명령 및 경고 조치를 사용하려면 Splunk 플랫폼 배치의 검색헤드 또는 검색헤드 클러스터에 하나 이상의 AWS 계정을 설정.

### IAM 역할 관리

**Configuration** 메뉴에서 AWS의 Splunk 애드온에 대한 IAM 계정이 가정 할 수있는 AWS IAM 역할을 관리하여 다음 AWS 자원(General S3, Incremental S3, SQS-Based S3, Billing, Description , CloudWatch, Kinesis)에 액세스

> 주의 : `splunk_ta_aws_iam_roles.conf`를 직접 편집하여 역할을 구성하는 것은 지원되지 않음.
> Splunk Web의 Add-on 구성 메뉴를 사용하여 IAM 역할을 관리.
> Add-on에서 EC2 IAM 역할을 편집하거나 삭제할 수 없음.
> IAM 역할을 추가하려면 다음을 수행.

1. Splunk Web의 홈 페이지에서 왼쪽 탐색 줄의 **Splunk Add-on for AWS**을 클릭.
2. 앱 탐색 막대에서 **Configuration**을 클릭 한 다음 **IAM Role** 탭을 클릭.
3. **Add**를 클릭.
4. 이름 필드에, Splunk 플랫폼에서 관리되는 권한이 부여 된 AWS 계정이 맡을 역할에 대한 고유 한 이름을 입력. Role을 구성한 후에는 친숙한 이름을 변경할 수 없음.
5. **ARN** 필드에서 `arn\:aws\:iam\:\:\<aws_resource_id\>\:role\/\<role_name\>` 과 같은 유효한 형식으로 역할의 Amazon Resource Name을 입력 (예 :`arn:aws:iam::123456789012:role/s3admin`.
    여기서 구성하는 IAM 역할에는 수집하려는 AWS 데이터에 액세스 할 수있는 적절한 권한이 있어야 함.
    자세한 내용은 [Configure AWS permissions for the Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWSpermissions) 참조.
6. 추가를 클릭.

Actions 열의 **Edit**를 클릭하여 기존 IAM 역할을 편집.

작업 열에서 **Delete**를 클릭하여 기존 역할을 삭제
Add-on을 사용하면 하나 이상의 입력에 의해 사용 중이거나 사용 중이었던 역할을 삭제할 수 없음. 입력이 비활성화 된 경우에도 마찬가지. 입력을 삭제하거나 다른 가정 된 역할을 사용하도록 편집하면 역할을 삭제할 수 있음.

### proxy connection 구성

1. Splunk Web의 홈 페이지 왼쪽 탐색 모음에서 **Splunk Add-on for AWS**을 클릭.
2. 앱 탐색 바에서 **Configuration**을 클릭.
3. **Proxy** 탭을 클릭.
4. **Enabled** 상자를 선택하여 프록시 연결을 활성화하고 프록시에 필요한 필드를 입력.
5. **Save**을 클릭.

언제든지 프록시를 비활성화하고 구성을 저장하려면 ** 사용 ** 확인란의 선택을 취소하십시오. Add-on은 나중에 쉽게 다시 사용할 수 있도록 프록시 구성을 저장합니다.

프록시 구성을 삭제하려면 필드의 값을 삭제하십시오.

## Splunk Add-on for AWS 입력 구성

### 입력 구성 개요

Splunk Add-on for AWS을 사용하여 AWS에서 유용한 유형의 유용한 데이터를 수집하여 클라우드 인프라에 대한 중요한 통찰력을 얻을 수 있음. 지원되는 각 데이터 유형에 대해 하나 이상의 input types이 데이터 수집을 위해 제공됨.
다음 단계에 따라 AWS 입력 구성을 계획하고 수행.

> 새 입력을 추가하는 사용자는 admin_all_objects 역할을 활성화.

1. 지원되는 데이터 형식 목록을보고 행렬을 사용하려면 [Supported data types and corresponding AWS input types](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureInputs#Supported_data_types_and_corresponding_AWS_input_types) 섹션을 참조. 데이터 수집을 위해 구성할 input types을 결정.
2. [AWS input types](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureInputs#AWS_input_types)에서 구성하려는 input types의 세부 정보. 여기에서 input types 링크를 클릭하여 입력 구성 세부 사항으로 이동.
3. 입력 구성 세부 사항에 설명된 단계에 따라 구성을 완료.

### 지원되는 데이터 유형 및 해당 AWS input types

다음 매트릭스는 Splunk Add-on for AWS과이 데이터를 수집하도록 구성 할 수있는 해당 input types을 사용하여 수집 할 수있는 모든 데이터 유형을 나열.

일부 데이터 유형의 경우 Splunk Add-on for AWS을 통해 특정 요구 사항 (예 : 새로 생성 된 로그 만 수집하는 것과 달리 기록 로그를 수집)에 따라 여러 input types 중에서 선택할 수있는 유연성을 제공. 적용 가능한 경우 SQS-Based S3는 모든 수집 가능한 데이터 유형에 사용할 권장 input types.

> **S \= 지원됨, R \= 권장됨 \(지원되는 input types이 여러 개 있을 경우\)**

<table>
<tr><td>Data Type</td><td>Source type</td><td>SQS-based S3</td><td>Generic S3</td><td>Incremental S3</td><td>AWS Config</td><td>Config Rules</td><td>Inspector</td><td>Cloud-
Trail</td><td>Cloud-
Watch</td><td>Cloud-
Watch Logs</td><td>Description</td><td>Billing</td><td>Kinesis</td><td>SQS</td></tr>
<tr><td>Billing</td><td>aws:
billing</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td></tr>
<tr><td>Cloud-
Watch</td><td>aws:
cloudwatch</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>CloudFront Access Logs</td><td>aws:
cloudfront:
accesslogs</td><td>R</td><td>S</td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Config</td><td>aws:
config, aws:
config:
notification</td><td>R</td><td></td><td></td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Config Rules</td><td>aws:
config:
rule</td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Description</td><td>aws:
description</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td></td></tr>
<tr><td>ELB Access Logs</td><td>aws:elb:
accesslogs</td><td>R</td><td>S</td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Inspector</td><td>aws:
inspector</td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Cloud-
Trail</td><td>aws:
cloudtrail</td><td>R</td><td>S</td><td>S</td><td></td><td></td><td></td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>S3 Access Logs</td><td>aws:s3:
accesslogs</td><td>R</td><td>S</td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>VPC Flow Logs</td><td>aws:
cloudwatchlogs:
vpcflow</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td>R</td><td></td></tr>
<tr><td>SQS</td><td>aws:sqs</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td></tr>
<tr><td>Others</td><td>custom source types</td><td>S</td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td>S</td><td>S</td></tr>
</table>

### AWS input types

Splunk Add-on for AWS은 AWS 환경에서 유용한 데이터를 수집하는 두 가지 유형의 input types을 제공.

- 전용(단일 목적) input types : 하나의 특정 데이터 유형을 처리하도록 설계.
- 다용도 input types : S3 버킷에서 여러 데이터 유형 수집

일부 데이터 유형은 전용 input types 또는 다목적 input types 중 하나를 사용하여 수집.
예를 들어 CloudTrail 로그는 CloudTrail, S3 또는 SQS-Based S3 중 하나를 사용하여 수집 할 수 있습니다. SQS-Based S3 input types은 확장성이 뛰어나고 처리 성능이 높기 때문에 권장되는 옵션.

#### 전용 input types

특정 유형의 로그를 수집하려면 로그 유형을 수집하도록 설계된 해당 전용 입력을 구성. 구성 방법에 대한 지시 사항은 아래 표에서 input types 이름을 클릭.

<table>
<tr><td>Input</td><td>Description</td></tr>
<tr><td>AWS Config</td><td>AWS Configuration에서 Configuration 스냅 샷, 기록 구성 데이터 및 변경 알림</td></tr>
<tr><td>Config Rules</td><td>Compliance준수 세부 정보, 준수 요약 및 AWS Config Rules의 평가 상태</td></tr>
<tr><td>Inspector</td><td>Amazon Inspector 서비스의 평가 실행 및 결과 데이터</td></tr>
<tr><td>CloudTrail</td><td>AWS CloudTrail 서비스의 AWS API 호출 내역</td></tr>
<tr><td>CloudWatch Logs</td><td>VPC Flow Logs를 비롯한 CloudWatch Logs 서비스의 로그. VPC Flow Logs를 사용하면 리소스의 네트워크 인터페이스에 대한 IP 트래픽 흐름 데이터를 캡처 </td></tr>
<tr><td>CloudWatch</td><td>AWS CloudWatch 서비스의 실적 및 Billing metrics 항목</td></tr>
<tr><td>Description</td><td>AWS 환경에 대한 메타 데이터</td></tr>
<tr><td>Billing</td><td>Billing 및 비용 관리 콘솔에서 수집한 Billing 보고서의 청구 데이터</td></tr>
<tr><td>Kinesis</td><td>Kinesis streams 데이터.

> **Note** : Kinesis 스트림을 통해 VPC Flow Logs 및 CloudWatch Logs를 수집하는 것이 가장 좋습니다. 그러나 AWS Kinesis 입력에는 현재 다음과 같은 제한 사항이 있습니다.

- 단일 스트림에서 데이터를 수집하는 여러 입력으로 인해 Splunk에서 중복 이벤트가 발생합니다.
- 동적 조각 재 파티션 모니터링을 지원하지 않습니다. 즉, 조각 분할 또는 병합이있는 경우 Add-on이 다시 시작될 때까지 새 조각에서 데이터를 자동으로 검색하고 수집 할 수 없습니다. 조각을 다시 분할 한 후에는 데이터 수집 노드를 다시 시작하여 파티션에서 데이터를 수집해야합니다.

Amazon Kinesis Firehose 용 Splunk Add-on을 사용하여 Kinesis 스트림에서 데이터를 수집 할 수도 있습니다. Amazon Kinesis Firehose 용 Splunk Add-on은 일부 구성 단계를 단순화하지만 스트림에서 데이터를 수집하는 것과 동일한 제한 사항이이 Add-on의 Kinesis 입력에 적용됩니다. 자세한 내용은 Amazon Kinesis Firehose의 Splunk Add-on 정보를 참조
</td></tr>
<tr><td>SQS</td><td>AWS SQS의 데이터</td></tr>
</table>

#### 다목적 input types

지원되는 로그 유형을 수집하도록 다목적 input types을 구성.

Splunk은 SQS 기반 input types을 사용하여 지원되는 로그 유형을 수집 할 것을 권장. General S3 입력을 사용하여 로그를 수집중인 경우에도 SQS 기반 입력을 생성하고 기존 General S3 입력을 새 입력으로 마이그레이션. 자세한 마이그레이션 단계는이 설명서의 [Migrate from the S3 input to the SQS-based input](http://docs.splunk.com/Documentation/AddOns/released/AWS/SQS-basedS3#Migrate_from_the_S3_input_to_the_SQS-based_S3_input) 참조.

수집하려는 로그 유형이 SQS 기반 input types에 의해 지원되지 않으면 대신 General S3 input types을 사용.

다목적 S3 컬렉션 input types 간의 차이점을 보려면 [multi-purpose input types comparison table](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureInputs#Multi-purpose_input_types_comparison_table) 참조

구성 방법에 대한 지시 사항은 아래 표에서 input types 이름을 클릭.

<table>
<tr><td>Input</td><td>Description</td></tr>
<tr><td>SQS-based S3 (recommended)</td><td>A more scalable and higher-performing alternative to the generic and incremental S3 inputs, the SQS-based S3 input polls messages from SQS that subscribes to SNS notification events from AWS services and collects the corresponding log files - generic log data, CloudTrail API call history, Config logs, and access logs - from your S3 buckets in real time.

Unlike the other S3 input types, the SQS-based S3 input type takes advantage of the SQS visibility timeout setting and enables you to configure multiple inputs to scale out data collection from the same folder in an S3 bucket without ingesting duplicate data. Also, the SQS-based S3 input automatically switches to multipart, in-parallel transfers when a file is over a specific size threshold, thus preventing timeout errors caused by large file size.</td></tr>
<tr><td>Generic S3</td><td>General-purpose input type that can collect any log type from S3 buckets: CloudTrail API call history, access logs, and even custom non-AWS logs.
The generic S3 input lists all the objects in the bucket and examines each file's modified date every time it runs to pull uncollected data from an S3 bucket. When the number of objects in a bucket is large, this can be a very time-consuming process with low throughput.</td></tr>
<tr><td>Incremental S3</td><td>The incremental S3 input type collects four AWS service log types.
There are four types of logs you can collect using the Incremental S3 input:

- CloudTrail Logs: This add-on will search for the cloudtrail logs under `<bucket_name>/<log_file_prefix>/AWSLogs/<Account ID>/CloudTrail/<Region ID>/<YYYY/MM/DD>/<file_name>.json.gz`.
- ELB Access Logs: This add-on will search the elb access logs under `<bucket_name>/<log_file_prefix>/AWSLogs/<Account ID>/elasticloadbalancing/<Region ID>/<YYYY/MM/DD>/<file_name>.log.gz`.
- S3 Access Logs: This add-on will search the S3 access logs under `<bucket_name>/<log_file_prefix><YYYY-mm-DD-HH-MM-SS><UniqueString>`.
- CloudFront Access Logs: This add-on will search the cloudfront access logs under `<bucket_name>/<log_file_prefix><distributionID><YYYY/MM/DD>.<UniqueID>.gz`

The incremental S3 input only lists and retrieves objects that have not been ingested from a bucket by comparing datetime information included in filenames against checkpoint record, which significantly improves ingestion performance.</td></tr>
</table>

<table>
<tr> <td> 입력 </td> <td> 설명 </td> </tr>
<BR> <BR> <BR> SQS-Based S3 (권장) <BR> <BR> 일반 및 Incremental S3 입력 대신 확장 성과 성능이 향상된 SQS-Based S3 입력은 SNS에 가입 한 SQS의 메시지를 폴링합니다 알림 이벤트를 생성하고 S3 버킷의 실시간 로그 데이터 (일반적인 로그 데이터, CloudTrail API 호출 내역, 구성 로그 및 액세스 로그)를 수집합니다.

다른 S3 input types과 달리 SQS-Based S3 input types은 SQS 가시성 타임 아웃 설정을 이용하며 중복 데이터를 처리하지 않고도 S3 버킷의 동일한 폴더에서 데이터 수집을 확장하도록 여러 입력을 구성 할 수 있습니다. 또한 SQS-Based S3 입력은 파일이 특정 크기 임계 값을 초과하면 멀티 파트, 병렬 전송으로 자동 전환되므로 큰 파일 크기로 인한 시간 초과 오류가 발생하지 않습니다. </td> </tr>
<tr> <td> General S3 </td> S3 버킷에서 모든 로그 유형을 수집 할 수있는 범용 input types : CloudTrail API 호출 내역, 액세스 로그 및 비 AWS 사용자 정의 로그.
General S3 입력은 버킷의 모든 객체를 나열하고 수집 할 때마다 각 버킷에서 수집되지 않은 데이터를 가져 오기 위해 각 파일의 수정 날짜를 검사합니다. 버킷에있는 객체의 수가 많으면 처리량이 낮고 시간이 오래 걸릴 수 있습니다. </td> </tr>
<tr> <td> Incremental S3 </td> <td> Incremental S3 input types은 4 개의 AWS 서비스 로그 유형을 수집합니다.
Incremental S3 입력을 사용하여 수집 할 수있는 로그 유형에는 다음과 같은 네 가지 유형이 있습니다.

- CloudTrail 로그 :이 Add-on은 <bucket_name> / <log_file_prefix> / AWSLogs / <Account ID> / CloudTrail / <지역 ID> / <YYYY / MM / DD> / <파일 이름>에서 클라우드 레일 로그를 검색합니다. json.gz`.
- ELB 액세스 로그 :이 Add-on은 <bucket_name> / <log_file_prefix> / AWSLogs / <Account ID> / elasticloadbalancing / <지역 ID> / <YYYY / MM / DD> / <file_name>에서 elb 액세스 로그를 검색합니다. .log.gz`.
- S3 Access Logs :이 Add-on은`<bucket_name> / <log_file_prefix> <YYYY-mm-DD-HH-MM-SS> <UniqueString>`아래에서 S3 액세스 로그를 검색합니다.
- CloudFront Access Logs :이 Add-on은 <bucket_name> / <log_file_prefix> <distributionID> <YYYY / MM / DD>. <UniqueID> .gz`에서 클라우드 프론트 액세스 로그를 검색합니다.

Incremental S3 입력은 파일 이름에 포함 된 날짜 정보와 검사 점 레코드를 비교하여 버킷에서 처리되지 않은 개체 만 나열하고 검색하므로 처리 성능이 크게 향상됩니다. </td> </tr>
</table>

#### 다목적 input types 비교표

<table>
Incremental S3 </td> <td> SQS-Based S3 (권장) </td> </td>
<td> 지원되는 로그 유형 </td> <td> AWS 이외의 사용자 정의 로그를 포함한 모든 로그 유형 <td> 4 가지 AWS 서비스 로그 유형 : CloudTrail 로그, S3 액세스 로그, CloudFront 액세스 로그, ELB 액세스 로그 </td> </td> </td> </td> 5 가지 AWS 서비스 로그 유형 (구성 로그, CloudTrail 로그, S3 액세스 로그, CloudFront 액세스 로그, ELB 액세스 로그) >
<td> 데이터 수집 방법 </td> 버킷의 모든 객체를 나열하고 수정 날짜와 검사 점을 비교합니다. <td> 파일 이름이 datetime </ td로 구분되는 AWS 로그 파일을 직접 검색합니다. > <td> SQS 메시지를 디코딩하고 S3 버킷에서 해당 로그를 수집합니다. </td> </tr>
<td> 높은 </td> </td> </td> </td> </td>
</td> 예 </td> <td> 예 </td> </td> </td> </td> </td>
<tr> <td> Scalable? </td> <td> 아니요 </td> 아니요 </td> <td> 예

중복 된 데이터를 생성하지 않고 동일한 S3 버킷의 로그를 수집하도록 여러 입력을 구성하여 데이터 수집을 확장 할 수 있습니다. </td> </tr>
<tr> <td> 내결함성? </td> <td> No

각각의 General S3 입력은 단일 실패 지점입니다. </td> <td> No

각 Incremental S3 입력은 단일 실패 지점입니다. </td> <td> 예

SQS 가시성 제한 시간 설정을 이용합니다. SQS-Based S3 입력에 의해 제 시간에 성공적으로 처리되지 않은 모든 SQS 메시지가 대기열에 다시 나타나고 검색되어 다시 처리됩니다.

또한 하나의 SQS-Based S3 입력에 오류가 발생해도 다른 입력은 계속해서 SQS 대기열에서 메시지를 가져 와서 S3 버킷의 해당 데이터를 수집 할 수 있도록 데이터 수집을 수평 확장 할 수 있습니다. </td> </tr>
</table>