# Installation & Configuration

## Install the Splunk Add-on for AWS

Install the Splunk Add-on for AWS
Get the Splunk Add-on for AWS by downloading it from https://splunkbase.splunk.com/app/1876 or browsing to it using the app browser within Splunk Web.
Determine where and how to install this add-on in your deployment, using the tables on this page.
Perform any prerequisite steps before installing, if required and specified in the tables below.
Complete your installation.
If you need step-by-step instructions on how to install an add-on in your specific deployment environment, see the installation walkthroughs section at the bottom of this page for links to installation instructions specific to a single-instance deployment, distributed deployment, Splunk Cloud, or Splunk Light.

Distributed deployments
Use the tables below to determine where and how to install this add-on in a distributed deployment of Splunk Enterprise or any deployment for which you are using forwarders to get your data in. Depending on your environment, your preferences, and the requirements of the add-on, you may need to install the add-on in multiple places.

Where to install this add-on
Unless otherwise noted, all supported add-ons can be safely installed to all tiers of a distributed Splunk platform deployment. See Where to install Splunk add-ons in Splunk Add-ons for more information.

This table provides a reference for installing this specific add-on to a distributed deployment of Splunk Enterprise.

Splunk platform component	Supported	Required	Comments
Search Heads	Yes	Yes	Install this add-on to all search heads where AWS knowledge management is required.
Indexers	Yes	No	Not required, as the parsing operations occur on the heavy forwarders.
Heavy Forwarders	Yes	Yes	This add-on requires heavy forwarders to perform data collection via modular inputs and to perform the setup and authentication with AWS in Splunk Web.
Universal Forwarders	No	No	This add-on requires heavy forwarders.
Light Forwarders	No	No	This add-on requires heavy forwarders.
Distributed deployment compatibility
This table provides a quick reference for the compatibility of this add-on with Splunk distributed deployment features.

Distributed deployment feature	Supported	Comments
Search Head Clusters	Yes	You can install this add-on on a search head cluster for all search-time functionality, but configure inputs on forwarders to avoid duplicate data collection. 
Before installing this add-on to a cluster, make the following changes to the add-on package: 
1. Remove the eventgen.conf files and all files in the samples folder 
2. Remove the inputs.conf file.
Indexer Clusters	Yes	Before installing this add-on to a cluster, make the following changes to the add-on package: 
1. Remove the eventgen.conf files and all files in the samples folder 
2. Remove the inputs.conf file.
Deployment Server	No	Supported for deploying unconfigured add-ons only.
Using a deployment server to deploy the configured add-on to multiple forwarders acting as data collectors causes duplication of data.
The add-on uses the credential vault to secure your credentials, and this credential management solution is incompatible with the deployment server.
Installation walkthroughs
The Splunk Add-Ons manual includes an Installing add-ons guide that helps you successfully install any Splunk-supported add-on to your Splunk platform.

For a walkthrough of the installation procedure, follow the link that matches your deployment scenario:

Single-instance Splunk Enterprise
Distributed Splunk Enterprise
Splunk Cloud
Splunk Light

## Manage accounts for the Splunk Add-on for AWS

Manage accounts for the Splunk Add-on for AWS
Manage your accounts, proxy connections, and log levels for the Splunk Add-on for AWS on your data collection node, usually a heavy forwarder, using Splunk Web. Managing these items using the configuration files is not supported.

The Splunk Add-on for AWS supports two methods for connecting to AWS to collect data: EC2 IAM roles and AWS user accounts.

Discover an EC2 IAM role
If you are running your data collection node of your Splunk platform in your own managed AWS environment using commercial regions, you can set up an IAM role for the EC2 and use that role to configure data collection jobs. The Splunk Add-on for AWS automatically discovers this role once it is set up.

Collecting data using an auto-discovered EC2 IAM role is not supported in China or GovCloud regions.

Follow the AWS documentation to set up an IAM role for your EC2: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html.
Ensure that this role has all of the required permissions specified in Configure AWS permissions for the Splunk Add-on for AWS. If you do not want to give the role all of the permissions required for all inputs, you also need to configure AWS accounts that you can use for the other inputs not covered by the permissions in this role.
Click Splunk Add-on for AWS in your left navigation bar on Splunk Web's home page.
Click Configuration in the app navigation bar. By default, the add-on displays the Account tab.
Look for the EC2 IAM role in the Autodiscovered IAM Role column. If you are in your own managed AWS environment and you have an EC2 IAM role configured, it appears in this account list automatically.
No further configuration is required. You can also configure AWS accounts if you want to use both EC2 IAM roles and user accounts to ingest your AWS data.

You cannot edit or delete EC2 IAM roles from the add-on.

Manage AWS accounts
To add an AWS account:

On Splunk Web's home page, click Splunk Add-on for AWS in your left navigation bar.
Click Configuration in the app navigation bar. By default, the add-on displays the Account tab.
Click Add.
Enter a friendly name for the AWS account. You cannot change the friendly name once you have configured the account.
Enter the credentials (Key ID and Secret Key) for an AWS account that the Splunk platform should use to access your AWS data. The accounts you configure here must have adequate permissions to access the AWS data that you want to collect. See Configure AWS permissions for the Splunk Add-on for AWS for more information.
Select the Region Category for the account. The most common is Global.
Click Add.
You can edit existing accounts by clicking Edit in the Actions column.

You can delete an existing account by clicking Delete in the Actions column. The add-on prevents you from deleting accounts that are in use by one or more inputs, even if those inputs are disabled. Delete the inputs or edit them to use a different account, then you can delete the account.

If you want to use custom commands and alert actions, you must set up at least one AWS account on your Splunk platform deployment's search head or search head cluster.

Manage IAM roles
Under the Configuration menu, you manage AWS IAM roles that can be assumed by IAM accounts for the Splunk Add-on for AWS to access the following AWS resources: Generic S3, Incremental S3, SQS-Based S3, Billing, Description, CloudWatch, Kinesis.

Note: Configuring roles by directly editing splunk_ta_aws_iam_roles.conf is not supported. Use the add-on's Configuration menu in Splunk Web to manage IAM roles. You cannot edit or delete EC2 IAM roles from the add-on. To add an IAM Role:

On Splunk Web's home page, click Splunk Add-on for AWS in your left navigation bar.
Click Configuration in the app navigation bar, and then click the IAM Role tab.
Click Add.
In the Name field, enter a unique friendly name for the role to be assumed by authorized AWS accounts managed on the Splunk platform. You cannot change the friendly name once you have configured the role.
In the ARN field, enter the role's Amazon Resource Name in the valid format: arn:aws:iam::<aws_resource_id>:role/<role_name>, for example, arn:aws:iam::123456789012:role/s3admin. 
The IAM role you configure here must have adequate permissions to access the AWS data that you want to collect. See Configure AWS permissions for the Splunk Add-on for AWS for more information.
Click Add.
You can edit existing IAM roles by clicking Edit in the Actions column.

You can delete an existing role by clicking Delete in the Actions column. The add-on prevents you from deleting roles that are being assumed and in use by one or more inputs, even if those inputs are disabled. Delete the inputs or edit them to use a different assumed role, then you can delete the role.

Configure a proxy connection
Click Splunk Add-on for AWS in your left navigation bar on Splunk Web's home page.
Click Configuration in the app navigation bar.
Click the Proxy tab.
Select the Enable box to enable the proxy connection and fill in the fields required for your proxy.
Click Save.
If, at any time, you want to disable your proxy but save your configuration, clear the Enable box. The add-on stores your proxy configuration so you can easily enable it again later.

To delete your proxy configuration, delete the values in the fields.

## Configure inputs for the Splunk Add-on for AWS

Configure inputs for the Splunk Add-on for AWS
Input configuration overview
You can use the Splunk Add-on for AWS to collect many types of useful data from AWS to gain valuable insight of your cloud infrastructure. For each supported data type, one or more input types are provided for data collection.

Follow these steps to plan and perform your AWS input configuration:

Read the Supported data types and corresponding AWS input types section to view a list of supported data types and use the matrix to determine which input type to configure for your data collection.
View the details of the input type you plan to configure under AWS input types. From there, click the input type link to go to the input configuration details.
Follow the steps described in the input configuration details to complete the configuration.
Supported data types and corresponding AWS input types
The following matrix lists all the data types that can be collected using the Splunk Add-on for AWS and the corresponding input types that you can configure to collect this data.

For some data types, the Splunk Add-on for AWS provides you with the flexibility to choose from multiple input types based on specific requirements (for example, collect historical logs as opposed to only collect newly created logs). Whenever applicable, SQS-based S3 is the recommended input type to use for all of its collectible data types.

S = Supported, R = Recommended (when there are multiple supported input types)

Data Type	Source type	SQS-based S3	Generic S3	Incremental S3	AWS Config	Config Rules	Inspector	Cloud-
Trail	Cloud-
Watch	Cloud-
Watch Logs	Description	Billing	Kinesis	SQS
Billing	aws:
billing											S		
Cloud-
Watch	aws:
cloudwatch								S					
CloudFront Access Logs	aws:
cloudfront:
accesslogs	R	S	S										
Config	aws:
config, aws:
config:
notification	R			S									
Config Rules	aws:
config:
rule					S								
Description	aws:
description										S			
ELB Access Logs	aws:elb:
accesslogs	R	S	S										
Inspector	aws:
inspector						S							
Cloud-
Trail	aws:
cloudtrail	R	S	S				S						
S3 Access Logs	aws:s3:
accesslogs	R	S	S										
VPC Flow Logs	aws:
cloudwatchlogs:
vpcflow									S			R	
SQS	aws:sqs													S
Others	custom source types	S	S							S			S	S
AWS input types
The Splunk Add-on for AWS provides two categories of input types to gather useful data from your AWS environment:

Dedicated (single-purpose) input types: designed to ingest one specific data type
Multi-purpose input types: collect multiple data types from the S3 bucket
Some data types can be ingested using either a dedicated input type or a multi-purpose input type. For example, CloudTrail logs can be collected using any of the following input types: CloudTrail, S3, or SQS-based S3. The SQS-based S3 input type is the recommended option because it is more scalable and provides higher ingestion performance.

Dedicated input types
To ingest a specific type of logs, configure the corresponding dedicated input designed to collect the log type. Click the input type name in the table below for instructions on how to configure it.

Input	Description
AWS Config	Configuration snapshots, historical configuration data, and change notifications from the AWS Config service.
Config Rules	Compliance details, compliance summary, and evaluation status of your AWS Config Rules.
Inspector	Assessment Runs and Findings data from the Amazon Inspector service.
CloudTrail	AWS API call history from the AWS CloudTrail service.
CloudWatch Logs	Logs from the CloudWatch Logs service, including VPC Flow Logs. VPC Flow Logs allow you to capture IP traffic flow data for the network interfaces in your resources.
CloudWatch	Performance and billing metrics from the AWS CloudWatch service.
Description	Metadata about your AWS environment.
Billing	Billing data from the billing reports that you collect in the Billing & Cost Management console.
Kinesis	Data from your Kinesis streams. 
Note: It is a best practice to collect VPC flow logs and CloudWatch logs through Kinesis streams. However, the AWS Kinesis input currently has the following limitations:
Multiple inputs collecting data from a single stream cause duplicate events in Splunk.
Does not support monitoring of dynamic shards repartition, which means when there is a shard split or merge, the add-on cannot automatically discover and collect data in the new shards until it is restarted. After you repartition shards, you must restart your data collection node to collect data from the partitions. 
You can also collect data from Kinesis streams using the Splunk Add-on for Amazon Kinesis Firehose. The Splunk Add-on for Amazon Kinesis Firehose simplifies some of the configuration steps, but the same limitations about collecting data from streams apply there as to the Kinesis input in this add-on. For more information, see About the Splunk Add-on for Amazon Kinesis Firehose.

SQS	Data from your AWS SQS.
Multi-purpose input types
Configure multi-purpose inputs to ingest supported log types.

Splunk recommends that you use the SQS-based input type to collect its supported log types. If you are already collecting logs using generic S3 inputs, you can still create SQS-based inputs and migrate your existing generic S3 inputs to the new inputs. For detailed migration steps, see Migrate from the S3 input to the SQS-based input in this manual.

If the log types you want to collect are not supported by the SQS-based input type, use the generic S3 input type instead.

Read the multi-purpose input types comparison table to view the differences between the multi-purpose S3 collection input types.

Click the input type name in the table below for instructions on how to configure it.

Input	Description
SQS-based S3 (recommended)	A more scalable and higher-performing alternative to the generic and incremental S3 inputs, the SQS-based S3 input polls messages from SQS that subscribes to SNS notification events from AWS services and collects the corresponding log files - generic log data, CloudTrail API call history, Config logs, and access logs - from your S3 buckets in real time. 
Unlike the other S3 input types, the SQS-based S3 input type takes advantage of the SQS visibility timeout setting and enables you to configure multiple inputs to scale out data collection from the same folder in an S3 bucket without ingesting duplicate data. Also, the SQS-based S3 input automatically switches to multipart, in-parallel transfers when a file is over a specific size threshold, thus preventing timeout errors caused by large file size.
Generic S3	General-purpose input type that can collect any log type from S3 buckets: CloudTrail API call history, access logs, and even custom non-AWS logs. 
The generic S3 input lists all the objects in the bucket and examines each file's modified date every time it runs to pull uncollected data from an S3 bucket. When the number of objects in a bucket is large, this can be a very time-consuming process with low throughput.
Incremental S3	The incremental S3 input type collects four AWS service log types. 
There are four types of logs you can collect using the Incremental S3 input:
CloudTrail Logs: This add-on will search for the cloudtrail logs under <bucket_name>/<log_file_prefix>/AWSLogs/<Account ID>/CloudTrail/<Region ID>/<YYYY/MM/DD>/<file_name>.json.gz.
ELB Access Logs: This add-on will search the elb access logs under <bucket_name>/<log_file_prefix>/AWSLogs/<Account ID>/elasticloadbalancing/<Region ID>/<YYYY/MM/DD>/<file_name>.log.gz.
S3 Access Logs: This add-on will search the S3 access logs under <bucket_name>/<log_file_prefix><YYYY-mm-DD-HH-MM-SS><UniqueString>.
CloudFront Access Logs: This add-on will search the cloudfront access logs under <bucket_name>/<log_file_prefix><distributionID><YYYY/MM/DD>.<UniqueID>.gz
The incremental S3 input only lists and retrieves objects that have not been ingested from a bucket by comparing datetime information included in filenames against checkpoint record, which significantly improves ingestion performance.

Multi-purpose input types comparison table
Generic S3	Incremental S3	SQS-based S3 (recommended)
Supported log types	Any log type, including non-AWS custom logs	4 AWS services log types: CloudTrail logs, S3 access logs, CloudFront access logs, ELB access logs	5 AWS services log types (Config logs, CloudTrail logs, S3 access logs, CloudFront access logs, ELB access logs), as well as non-AWS custom logs
Data collection method	Lists all objects in the bucket and compares modified date against the checkpoint	Directly retrieves AWS log files whose filenames are distinguished by datetime	Decodes SQS messages and ingests corresponding logs from the S3 bucket
Ingestion performance	Low	High	High
Can ingest historical logs (logs generated in the past)?	Yes	Yes	No
Scalable?	No	No	Yes 
You can scale out data collection by configuring multiple inputs to ingest logs from the same S3 bucket without creating duplicate data
Fault-tolerant?	No 
Each generic S3 input is a single point of failure.	No 
Each incremental S3 input is a single point of failure.	Yes 
Takes advantage of the SQS visibility timeout setting. Any SQS message not successfully processed in time by the SQS-based S3 input will reappear in the queue and will be retrieved and processed again. 
In addition, data collection can be horizontally scaled out so that if one SQS-based S3 input fails, other inputs can still continue to pick up messages from the SQS queue and ingest corresponding data from the S3 bucket.