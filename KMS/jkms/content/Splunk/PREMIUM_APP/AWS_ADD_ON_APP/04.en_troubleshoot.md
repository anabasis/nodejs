# Troubleshooting

## Troubleshoot the Splunk Add-on for AWS

Troubleshoot the Splunk Add-on for AWS
General troubleshooting
For helpful troubleshooting tips that you can apply to all add-ons, see Troubleshoot add-ons. You can also access these support and resource links.

Health Check Dashboards
You can choose the dashboards from the Health Check menu to troubleshoot data collection errors and performance issues.

The Health Overview dashboard gives you an at-a-glance view of data collection errors and performance metrics for all input types:

Errors count by error category
Error count over time by input type, host, data input, and error category
Throughput over time by host, input type, and data input
The S3 Health Details dashboard focuses on the generic, incremental, and SQS-based S3 input types and provides indexing time lag and detailed error information of these multi-purpose inputs.

You can customize the health dashboard. Refer to the About the Dashboard Editor topic in the Dashboards and Visualizations manual.

Internal logs
You can directly access internal log data for help with troubleshooting. Data collected with these source types are used in the Health Check dashboards.

Data source	Source type
Logs from splunk_ta_aws_cloudtrail_cloudtrail_{input_name}.log.	aws:cloudtrail:log
Logs from splunk_ta_aws_cloudwatch.log.	aws:cloudwatch:log
Logs from splunk_ta_aws_cloudwatch_logs.log.	aws:cloudwatchlogs:log
Logs from splunk_ta_aws_config_{input_name}.log.	aws:config:log
Logs from splunk_ta_aws_config_rule.log.	aws:configrule:log
Logs from splunk_ta_aws_inspector_main.log, splunk_ta_aws_inspector_app_env.log, splunk_ta_aws_inspector_proxy_conf.log, and splunk_ta_aws_inspector_util.log.	aws:inspector:log
Logs from splunk_ta_aws_description.log.	aws:description:log
Logs from splunk_ta_aws_billing_{input_name}.log.	aws:billing:log
Logs from splunk_ta_aws_generic_s3_{input_name}.	aws:s3:log
Logs from splunk_ta_aws_logs_{input_name}.log, each incremental S3 input has one log file with the input name in the log file.	aws:logs:log
Logs from splunk_ta_aws_kinesis.log.	aws:kinesis:log
Logs from splunk_ta_aws_ sqs_based_s3_{input_name} .	aws:sqsbaseds3:log
Logs from splunk_ta_aws_sns_alert_modular.log and splunk_ta_aws_sns_alert_search.log.	aws:sns:alert:log
Logs from splunk_ta_aws_rest.log, populated by REST API handlers called when setting up the add-on or data input.	aws:resthandler:log
Logs from splunk_ta_aws_proxy_conf.log, the proxy handler used in all AWS data inputs.	aws:proxy-conf:log
Logs from splunk_ta_aws_s3util.log, populated by the S3, CloudWatch, and SQS connectors.	aws:resthandler:log
Logs from splunk_ta_aws_util.log, a shared utilities library.	aws:util:log
Configure log levels
1. Click Splunk Add-on for AWS in your left navigation bar on Splunk Web's home page.

2. Click Configuration in the app navigation bar.

3. Click the Logging tab.

4. Adjust the log levels for each of the AWS services as needed by changing the default of INFO to one of the other available options, DEBUG or ERROR.

Note: These log level configurations apply only to runtime logs. Some REST endpoint logs from configuration activity log at DEBUG, and some validation logs log at ERROR. These levels cannot be configured.

Problem saving during account or input configuration
If you experience errors or trouble saving while configuring your AWS accounts on the setup page, go to $SPLUNK_HOME/etc/system/local/web.conf and change your timeout settings as shown below.

  [settings]
  splunkdConnectionTimeout = 300
Problems deploying with a deployment server
If you use a deployment server to deploy the Splunk Add-on for Amazon Web Services to multiple heavy forwarders, you must configure the Amazon Web Services accounts using the Splunk Web setup UI for each instance separately, because the deployment server does not support sharing hashed password storage across instances.

S3 issues
S3 input performance issues
You can configure multiple S3 inputs for a single S3 bucket to improve performance. The Splunk platform dedicates one process for each data input, so provided that your system has sufficient processing power, performance will improve with multiple inputs. See Performance reference for the S3 input in the Splunk Add-on for AWS.

Note: Be sure that the S3 key names in multiple inputs against the same bucket do not overlap, to prevent indexing duplicate data.

S3 key name whitelist/blacklist filtering issues
Whitelist and blacklist matches the full key name, not just the last segment.

Whitelist .*abc/.* will match /a/b/abc/e.gz.

For more help with regex:

watch the video in this blog post: http://blogs.splunk.com/2008/10/22/all-my-regexs-live-in-texas/
read "About Splunk regular expressions" in the Knowledge Manager Manual, part of the Splunk Enterprise documentation.
S3 event line breaking issues
If your indexed S3 data has incorrect line breaking, configure a custom source type in props.conf to control how the lines should be broken for your events.

If S3 events are too long and get truncated, set TRUNCATE = 0 in props.conf to prevent line truncating.

More more information, see Configure event line breaking in the Getting Data In manual, part of the Splunk Enterprise documentation.

CloudWatch configuration issues
Throttling
If you have a high volume of CloudWatch data, search index=_internal Throttling to determine if you are experiencing an API throttling issue. If you are, contact AWS support to increase your CloudWatch API rate. You can also decrease the number of metrics you collect or increase the granularity in order to make fewer API calls.

Granularity
If the granularity of your indexed data does not match your expectations, check that your configured granularity falls within what AWS supports for the metric you have selected. Different AWS metrics support different minimum granularities, based on the sampling period that AWS allows for that metric. For example, CPUUtilization has a sampling period of 5 minutes, whereas Billing Estimated Charge has a sampling period of 4 hours.

If you configured a granularity that is less than the sampling period for the selected metric, the reported granularity in your indexed data reflects the actual sampling granularity but is labeled with your configured granularity. Clear the local/inputs.conf cloudwatch stanza with the problem, adjust the granularity configuration to match the supported sampling granularity so that newly indexed data is correct, and reindex the data.

CloudTrail data indexing problems
If you are not seeing CloudTrail data in the Splunk platform, follow this troubleshooting process.

1. Review the internal logs by searching for: index=_internal source=*cloudtrail*

2. Check to see if the Splunk platform is connecting to SQS successfully by searching for the string "Connected to SQS".

3. Check to see if the Splunk platform is processing messages successfully. Look for strings that follow the pattern "X completed, Y failed while processing notification batch".

4. Check to see if the Splunk platform is discarding messages. Look for strings that follow the pattern "fetched X, wrote Y, discarded Z".

5. Review your Amazon Web Services configuration to verify that SQS messages are being placed into queue. If messages are being removed and the logs do not show that our input is removing them, then there may be another script or input consuming messages from the queue. Review your data inputs to ensure there is not another input configured to consume the same queue.

6. Go to the AWS console to view CloudWatch metrics with the detail set to 1 minute to view the trend. For more details, see https://aws.amazon.com/blogs/aws/amazon-cloudwatch-search-and-browse-metrics-in-the-console/. If you see messages consumed but no Splunk platform inputs are consuming them, check for remote services that might be accessing the same queue.

Billing Report issues
Problems accessing billing reports from AWS
Ensure there are Billing Reports available on the S3 bucket you select when you configure the billing input and that the AWS account you specify has the permission to read the files inside that bucket.

Problems understanding the billing report data
Splunk recommends accessing the saved searches included with the add-on to analyze billing report data.

Problems configuring the billing data interval
Note: The default billing data ingestion collection intervals for billing report data is designed to minimize license usage. Review the default behavior and make adjustments with caution.

To configure the interval by which Splunk Enterprise pulls Monthly and Detailed Billing Reports:

In Splunk Web, go to the Splunk Add-on for AWS inputs screen.
Create a new Billing input or click to edit your existing one.
Click the Settings tab.
Customize the value in the Interval field.
SNS alert issues
Because the modular input module is inactive, it cannot check whether the AWS is correct or existing in the AWS SNS. If you cannot send the message to AWS SNS account, you can perform the following procedures.

Ensure the SNS topic name exists in AWS and the region ID is correctly configured.
Ensure the AWS account is correctly configured in Splunk add-on for AWS.
If you still have the issue, you can perform the following search to check the log for AWS SNS

index=_internal sourcetype=aws:sns:alert:log"

.
Proxy settings for VPC endpoints
When using proxy with VPC endpoints, check the proxy setting defined in $SPLUNK_HOME/etc/splunk-launch.conf, for example:

no_proxy = 169.254.169.254,127.0.0.1,s3.amazonaws.com,[s3-ap-southeast-2.amazonaws.com|http://s3-ap-southeast-2.amazonaws.com/]

You must add each S3 region endpoint to the no_proxy setting, and use the correct hostname in your region: s3-<your_aws_region>.amazonaws.com. The no_proxy setting does not allow for any spaces between the IP addresses.

## Access billing data for the Splunk Add-on for AWS

Access billing data for the Splunk Add-on for AWS
The Splunk Add-on for Amazon Web Services supports extracting generic data from your S3 buckets. One example of data that you might want to extract from your S3 buckets is your AWS billing report data. Use the billing input to collect your AWS billing reports, then extract useful information from them using pre-built reports included with this add-on. The add-on's pre-built reports are based on the AWS report formats to make it easier for you to access and work with this data. You can use these reports as examples of how to use the Splunk platform to explore your other S3 data.

Note: The Billing input does not collect billing reports for your AWS Marketplace charges.

Billing report collection behavior
Amazon Web Services offers four distinct billing reports, which you can read more about in the AWS documentation: http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html#d0e2817. You can collect any of the four types of billing report using the Splunk Add-on for AWS.

The Amazon Billing service updates all four of these reports continuously over the month until they are finalized a few days after the last day of the month. Until the reports are finalized, the daily updates add new billing information for the most recent day's activity and update previously reported line items. For example, the billing service may recalculate pricing discounts for events that already passed.

Because any portion of the data in the report may be updated, the add-on pulls the entire billing report every time it pulls any of the four report types. The Monthly report and Monthly cost allocation report are quite small, so the add-on retrieves these daily by default, resulting in many copies of the same (small) report. Detailed reports, however, are very large, so the add-on collects these reports only for months prior to the current month.

By default, the add-on collects all available reports that match the collection criteria for all available months. For example, assume that you install the Splunk Add-on for AWS on February 15, 2016 and configure your inputs to collect both Monthly cost allocation reports and Detailed billing reports with resources and tags. On the first day, the add-on collects the Monthly report for February 2016 current up to February 16, 2015 and the monthly reports for all previous months available for your account. The add-on also collects the Detailed report for January 2016, as well as the Detailed report for all previous months available for your account. On every subsequent day, the add-on downloads a new version of the Monthly report, current up to the latest day. It also checks the etag of the most recently completed month's (in this case, January's) Detailed report against the latest January Detailed report available from the Amazon Billing service. If they are the same, it does not download a Detailed report again until the month ends. On March 1, the add-on begins downloading Detailed reports for the month of February every day until the etag for that report is unchanged.

See the guidelines for configuring billing inputs for details of how to use cron schedules and regex to control the number of reports that you collect.

When you want to access any of the report data in the Splunk platform, use the reports included with the add-on to view the latest snapshots. Adjusting the reports is not supported.

Billing report types
Monthly report
The Monthly report lists AWS usage for each product dimension used by an account and its IAM users in monthly line items. You can download this report from the Bills page of the Billing and Cost Management console.

File Name Format: {AWS account number}-aws-billing-csv-yyyy-mm.csv

This report is small in size, so the add-on pulls the entire report once daily to get the latest snapshot.

Monthly cost allocation report
The Monthly cost allocation report contains the same data as the monthly report, but it also includes any cost allocation tags that you have created. You must obtain this report from the Amazon S3 bucket that you specify. Standard AWS storage rates apply.

File Name Format: {AWS account number}-aws-cost-allocation-yyyy-mm.csv

This report is small in size, so the add-on pulls the entire report once daily to get the latest snapshot.

Detailed billing report
The Detailed billing report lists AWS usage for each product dimension used by an account and its IAM users in hourly line items. You must obtain this report from the Amazon S3 bucket that you specify. Standard AWS storage rates apply.

File Name Format: {AWS account number}-aws-billing-detailed-line-items-yyyy-mm.csv.zip

This report can grow very large, so the add-on collects the report only after the month has ended. The add-on continues to collect the report once per day until it is finalized by Amazon billing services.

Detailed billing report with resources and tags
The Detailed billing report with resources and tags contains the same data as the detailed billing report, but also includes any cost allocation tags you have created and ResourceIDs for the AWS resources used by your account. You must obtain this report from the Amazon S3 bucket that you specify. Standard AWS storage rates apply.

File Name Format: {AWS account number}-aws-billing-detailed-line-items-with-resources-and-tags-yyyy-mm.csv.zip

This report can grow very large, so the add-on collects the report only after the month has ended. The add-on continues to collect the report once per day until it is finalized by Amazon billing services.

Event types for billing data
The Splunk platform indexes two types of billing data using the sourcetype aws:billing:

monthly reports, which have the event type aws_billing_monthly_report
detailed reports, which have the event type aws_billing_detail_report
Access the add-on's preconfigured reports
The Splunk Add-on for Amazon Web Services includes several reports based on the indexed billing report data. You can find these saved reports in Splunk Web by clicking Home > Reports and looking for items with prefix AWS Bill - . Some of the saved searches return a table. Others return just a single value, such as AWS Bill - Total Cost till Now.

The Splunk platform typically indexes multiple monthly report snapshots, because AWS places multiple snapshots of the current month's billing report into the S3 bucket. To obtain the most recent monthly report snapshot click Home > Reports and open the saved report called AWS Bill - Monthly Latest Snapshot. Or, search for it using the search string: | savedsearch "AWS Bill - Monthly Latest Snapshot"

You can obtain the most recent detailed report by clicking Home > Reports and opening the saved report called AWS Bill - Daily Cost. Or, search for it using the search string | savedsearch "AWS Bill - Daily Cost". Searching against detailed reports can be slow due to the volume of data in the report. Splunk recommends accelerating the searches against detailed reports.

Report sources
These saved reports are based on AWS Billing Reports instead of the billing metric data in CloudWatch. By default, Total or Monthly reports are based on data indexed from the AWS Monthly Reports (*-aws-billing-csv-yyyy-mm.csv or *-aws-cost-allocation-yyyy-mm.csv) on the S3 bucket, while Daily reports are based on AWS Detail Reports (*-aws-billing-detailed-line-items-yyyy-mm.csv.zip or *-aws-billing-detailed-line-items-with-resources-and-tags-yyyy-mm.csv.zip).

Default index behavior
By default, reports are looking for data in the default index, main. If you changed the default index when you configured the data input, the reports will not work unless you include the index in the default search indexes list or change the two reports so they filter to the custom index.

To include a custom index in the default search indexes list:

Click Settings > Users and authentication > Access controls > Roles > [Role that uses the saved searches] > Indexes searched by default.
Include the custom index in default search indexes list.
Repeat for each role that uses the saved searches.
To change the saved searches to filter to a custom index:

Open the saved search AWS Bill - Monthly Latest Snapshot.
Add a filter to specify the index you configured. For example, index=new_index.
Save your changes to the saved search.
Repeat these steps for the other saved search, AWS Bill - Detailed Cost.

## Lookups for the Splunk Add-on for AWS

Lookups for the Splunk Add-on for AWS
The Splunk Add-on for AWS includes eight lookups. The lookup files map fields from AWS services to CIM-compliant or human-readable values in the Splunk platform. The lookup files are located in $SPLUNK_HOME/etc/apps/Splunk_TA_aws/lookups.

Filename	Purpose
aws_config_action_lookup.csv	Maps the status field to a CIM-compliant value for the action field.
aws_config_object_category_lookup.csv	Sorts the various AWS Config object categories into a CIM-compliant values for the object_category field.
aws-cloudtrail-action-status.csv	Maps the eventName and errorCode fields to CIM-compliant values for action and status.
aws-cloudtrail-changetype.csv	Maps the eventSource to a CIM-compliant value for the change_type field.
aws-health-error-type.csv	Maps ErrorCode to ErrorDetail, ErrorCode, ErrorDetail	
aws-log-sourcetype-modinput.csv	Maps sourcetype to modinput	
cloudfront_edge_location_lookup	Maps the x_edge_location value to a human-readable edge_location_name.
vendor-product-aws-cloudtrail.csv	Defines CIM-compliant values for the vendor, product, and appfields based on the source type.
vpcflow_action_lookup.csv	Maps the numerical protocol code to a CIM-compliant protocol field and a human-readable protocol_full_name.
vpcflow_protocol_code_lookup.csv	Maps the vpcflow_action field to a CIM-compliant action field.
VmSizeToResources.csv	Map the instance_type field to CIM-compliant cpu_cores, mem_capacity field.

## Performance reference for the Splunk Add-on for AWS data inputs

Performance reference for the Splunk Add-on for AWS data inputs
Many factors impact throughput performance. The rate at which the Splunk Add-on for AWS ingests input data varies depending on a number of variables: deployment topology, number of keys in a bucket, file size, file compression format, number of events in a file, event size, and of course, hardware and networking conditions.

This section provides measured throughput data achieved under certain operating conditions and draws from the performance testing results some rough conclusions and guidelines on tuning AWS add-on throughput performance. Use the information here as a basis for estimating and optimizing the AWS add-on throughput performance in your own production environment. As performance may vary based on user characteristics, application usage, server configurations, and other factors, specific performance results cannot be guaranteed. Consult Splunk Support for accurate performance tuning and sizing.

Reference hardware and software environment
The throughput data and conclusions provided here are based on performance testing using Splunk instances (dedicated heavy forwarders and indexers) running on the following environment.

Instance type	M4 Double Extra Large (m4.4xlarge)
Memory	64 GB
Compute Units (ECU)	53.5
vCPU	16
Storage (GB)	0 (EBS only)
Arch	64-bit
EBS Optimized (Max Bandwidth)	2000 Mbps
Network performance	High
The following settings are configured in outputs.conf on the heavy forwarder:

useACK = true

maxQueueSize = 15MB

Measured performance data
The throughput data provided here is the maximum performance for each single input achieved in performance testing under specific operating conditions and is subject to change when any of the hardware and software variables changes. Use this data for very rough reference only.

Single-input max throughput
Data Input	Sourcetype	Max Throughput (KB/s)	Max EPS (event/s)	Max Throughput (GB/day)
Generic S3	aws:elb:accesslogs
(plain text, syslog, event size 250B, S3 key size 2MB)	17,000	86,000	1,470
Generic S3	aws:cloudtrail 
(gz, json, event size 720B, S3 key size 2MB)	11,000	35,000	950
Incremental S3	aws:elb:accesslogs
(plain text, syslog, event size 250B, S3 key size 2MB)	11,000	43,000	950
Incremental S3	aws:cloudtrail 
(gz, json, event size 720B, S3 key size 2MB)	7,000	10,000	600
SQS-based S3	aws:elb:accesslogs 
(plain text, syslog, event size 250B, S3 key size 2MB)	12,000	50,000	1,000
SQS-based S3	aws:elb:accesslogs 
(gz, syslog, event size 250B, S3 key size 2MB)	24,000	100,000	2,000
SQS-based S3	aws:cloudtrail 
(gz, json, event size 720B, S3 key size 2MB)	13,000	19,000	1,100
CloudWatch logs [1]	aws:cloudwatchlog:vpcflow	1,000	6,700	100
CloudWatch 
(ListMetric, 10,000 metrics)	aws:cloudwatch	240 (Metrics/s)	NA	NA
CloudTrail	aws:cloudtrail 
(gz, json, sqs=1000, 9K events/key)	5,000	7,000	400
Kinesis	aws:cloudwatchlog:vpcflow 
(json, 10 shards)	15,000	125,000	1,200
SQS	aws:sqs 
(json, event size 2.8K)	N/A	160	N/A
[1] API throttling error occurs if input streams > 1k

Multi-inputs max throughput
The following throughput data was measured with multiple inputs configured on a heavy forwarder in an indexer cluster distributed environment.

Note: Configuring more AWS accounts increases CPU usage and lowers throughput performance due to increased API calls. It is recommended that you consolidate AWS accounts when configuring the Splunk Add-on for AWS.

Data Input	Sourcetype	Max Throughput (KB/s)	Max EPS (events/s)	Max Throughput (GB/day)
Generic S3	aws:elb:accesslogs
(plain text, syslog, event size 250B, S3 key size 2MB)	23,000	108,000	1,980
Generic S3	aws:cloudtrail 
(gz, json, event size 720B, S3 key size 2MB)	45,000	130,000	3,880
Incremental S3	aws:elb:accesslogs
(plain text, syslog, event size 250B, S3 key size 2MB)	34,000	140,000	2,930
Incremental S3	aws:cloudtrail 
(gz, json, event size 720B, S3 key size 2MB)	45,000	65,000	3,880
SQS-based S3 [1]	aws:elb:accesslogs 
(plain text, syslog, event size 250B, S3 key size 2MB)	35,000	144,000	3,000
SQS-based S3 [1]	aws:elb:accesslogs 
(gz, syslog, event size 250B, S3 key size 2MB)	42,000	190,000	3,600
SQS-based S3 [1]	aws:cloudtrail 
(gz, json, event size 720B, S3 key size 2MB)	45,000	68,000	3,900
CloudWatch logs	aws:cloudwatchlog:vpcflow	1,000	6,700	100
CloudWatch (ListMetric)	aws:cloudwatch 
(10,000 metrics)	240 (metrics/s)	NA	NA
CloudTrail	aws:cloudtrail 
(gz, json, sqs=100, 9K events/key)	20,000	15,000	1,700
Kinesis	aws:cloudwatchlog:vpcflow 
(json, 10 shards)	18,000	154,000	1,500
SQS	aws:sqs 
(json, event size 2.8K)	N/A	670	N/A
[1] Performance testing of the SQS-based S3 input indicates that optimal performance throughput is reached when running four inputs on a single heavy forwarder instance. To achieve higher throughput performance beyond this bottleneck, you can further scale out data collection by creating multiple heavy forwarder instances each configured with up to four SQS-based S3 inputs to concurrently ingest data by consuming messages from the same SQS queue.

Max inputs benchmark per heavy forwarder
The following input number ceiling was measured with multiple inputs configured on a heavy forwarder in an indexer cluster distributed environment, where CPU and memory resources were utilized to their fullest.

If you have a smaller event size, fewer keys per bucket, or more available CPU and memory resources in your environment, you may configure more inputs than the maximum input number indicated in the table.

Data Input	Sourcetype	Format	Number of Keys/Bucket	Event Size	Max Inputs
S3	aws:s3	zip, syslog	100K	100B	300
S3	aws:cloudtrail	gz, json	1,300K	1KB	30
Incremental S3	aws:cloudtrail	gz, json	1,300K	1KB	20
SQS-based S3	aws:cloudtrail, aws:config	gz, json	1,000K	1KB	50
Memory usage benchmark for generic S3 inputs
Event Size	Number of Events per Key	Total Number of Keys	Archive Type	Number of Inputs	Memory Used
1K	1,000	10,000	zip	20	20G
1K	1,000	1,000	zip	20	12G
1K	1,000	10,000	zip	10	18G
100B	1,000	10,000	zip	10	15G
Performance tuning and sizing guidelines
If you do not achieve the expected AWS data ingestion throughput, follow these steps to tune the throughput performance:

1. Identify the bottleneck in your system that prevents it from achieving a higher level of throughput performance. The bottleneck in AWS data ingestion may lie in one of the following components:

The Splunk Add-on for AWS: its capacity to pull in AWS data through API calls
Heavy forwarder: its capacity to parse and forward data to the indexer tier, which involves the throughput of the parsing, merging, and typing pipelines
Indexer: the index pipeline throughput
To troubleshoot the indexing performance on the heavy forwarder and indexer, refer to Troubleshooting indexing performance in the Capacity Planning Manual. 
A chain is as only as strong as its weakest link. The capacity of the bottleneck is the capacity of the entire system as a whole. Only by identifying and tuning the performance of the bottleneck component can you improve the overall system performance.

2. Tune the performance of the bottleneck component. 
If the bottleneck lies in heavy forwarders or indexers, refer to the Summary of performance recommendations in the Capacity Planning Manual. 
If the bottleneck lies in the Splunk Add-on for AWS, adjust the following key factors that usually impact the AWS data input throughput:

Parallelization settings
To achieve optimal throughput performance, you can set the parallelIngestionPipelines value to 2 in server.conf if your resource capacity permits. For information about parallelIngestionPipelines, see Parallelization settings in the Splunk Enterprise Capacity Planning Manual.
AWS data inputs
When there is no shortage of resources, adding more inputs in the add-on increases throughput but it also consumes more memory and CPU. Increase the number of inputs to improve throughput until memory or CPU is running short. 
If you are using SQS-based S3 inputs, you can horizontally scale out data collection by configuring more inputs on multiple heavy forwarders to consume messages from the same SQS queue.
Number of keys in a bucket
For both the Generic S3 and Incremental S3 inputs, the number of keys (or objects) in a bucket is a factor that impacts initial data collection performance. The first time a Generic or Incremental S3 input collects data from a bucket, the more keys the bucket contains, the longer time it takes to complete the list operation, and the more memory is consumed. A large number of keys in a bucket require a huge amount of memory for S3 inputs in the initial data collection and limit the number of inputs you can configure in the add-on. 
If applicable, you can use log file prefix to subset keys in a bucket into smaller groups and configure different inputs to ingest them separately. For information about how to configure inputs to use log file prefix, see Add an S3 input for Splunk Add-on for AWS. 
For SQS-based S3 inputs, the number of keys in a bucket is not a primary factor since data collection can be horizontally scaled out based on messages consumed from the same SQS queue.
File format 
Compressed files consume much more memory than plain text files.
3. When you have resolved the bottleneck, see if the improved performance meets your requirements. If not, continue the previous steps to identify the next bottleneck in the system and address it until the expected overall throughput performance is achieved.

## Performance reference for the Kinesis input in the Splunk Add-on for AWS

Performance reference for the Kinesis input in the Splunk Add-on for AWS
This page provides reference information about Splunk's performance testing of the Kinesis input in Splunk Add-on for AWS. The testing was performed on version 4.0.0, when the Kinesis input was first introduced. Use this information to enhance the performance of your own Kinesis data collection tasks.

Note: Many factors impact performance results, including file size, file compression, event size, deployment architecture, and hardware. These results represent reference information and do not represent performance in all environments.

Summary
While results in different environments will vary, Splunk's performance testing of the Kinesis input showed:

Each Kinesis input can handle up to 6 MB/s of data, with a daily ingestion volume of 500 GB.
More shards can slightly improve the performance. Three shards are recommended for large streams.
Testing architecture
Splunk tested the performance of the Kinesis input using a single-instance Splunk Enterprise 6.4.0 on an m4.4xlarge AWS EC2 instance to ensure CPU, memory, storage, and network did not introduce any bottlenecks. Instance specs:

Instance type	M4 Quadruple Extra Large (m4.4xlarge)
Memory	64 GB
ECU	53.5
Cores	16
Storage	0 GB (EBS only)
Architecture	64-bit
Network performance	High
EBS Optimized: Max Bandwidth	250 MB/s
Test scenario
Splunk tested the following parameters to target the use case of high-volume VPC flow logs ingested through a Kinesis stream.

Shard numbers: 3, 5, and 10 shards
Event size: 120 bytes per event
Number of events: 20,000,000
Compression: gzip
Initial stream position: TRIM_HORIZON
AWS reports that each shard is limited to 5 read transactions per second, up to a maximum read rate of 2MB per second. Thus, with 10 shards, the theoretical upper limit is 20 MB per second.

Test results
Splunk observed a data ingestion rate of 6 million events per minute at peak, which is 100,000 events per second. Because each event is 120 bytes, this indicates a maximum throughput of 10 MB/s.

Splunk observed an average throughput of 6 MB/s for a single Kinesis modular input, or a daily ingestion throughput of approximately 500 GB.

After reducing the shard number from 10 shards to 3 shards, Splunk observed a throughput downgrade of approximately 10%.

During testing, Splunk observed the following resource usage on the instance:

normalized CPU usage of approximately 30%
Python memory usage of approximately 700 MB
The indexer is largest consumer of CPU, and the modular input is largest consumer of memory.

Note: AWS throws a ProvisionedThroughputExceededException if a call returns 10 MB of data and subsequent calls are made within the next 5 seconds. Splunk observed this error while testing with three shards only every one to five minutes.

## Use SNS Alert for the Splunk Add-on for AWS

Use SNS Alert for the Splunk Add-on for AWS
Note: To use custom search commands and custom alert actions, you must either be a Splunk administrator or a user with the appropriate capability:

list_storage_passwords if you are using Splunk platform 6.5.0 or later
admin_all_objects if you are using an earlier version of the Splunk platform
Use the custom search command
The Splunk Add-on for AWS includes a custom search command to send alerts to AWS SNS.

...| eval message="My Message" | eval entity="My Entity" | eval correlation_id="1234567890" | awssnsalert account=real region="ap-southeast-1" topic_name="ta-aws-sns-ingestion" publish_all=1

Attribute	Description
account	Required. AWS account name configured in add-on
region	Required. AWS region name
topic_name	Required. The alert message is sent to this AWS SNS topic name.
message	Required. The message that the Splunk Add-on for AWS sent to AWS SNS.
publish_all	You can set publish_all to 0 or 1. If you set publish_all=1, it means that this add-on will send all the records in this search. If you set publish_all=0, it means that this add-on will only send the first result to this search. The default value of this field is 0.
Use the alert action
The Splunk Add-on for AWS supports automatic incident and event creation and incident update from custom alert actions. Custom alert actions are available in Splunk platform version 6.3.0 and later.

To create a new incident or event from a custom alert action:

Write a search string that you want to use to trigger incident or event creation in AWS SNS.
Click Save As > Alert.
Fill out the Alert form. Give your alert a unique name and indicate whether the alert should be a real-time alert or a scheduled alert. See Getting started with Alerts in the Alerting Manual, part of the Splunk Enterprise documentation, for more information.
Under Trigger Actions, click Add Actions.
From the list, select AWS SNS Alert if you want the alert to create an event in AWS SNS.
Enter values for all required fields, as shown.
Field	Example Value
Account	Required. The account name configured in Splunk Add-on for AWS.
Region	Required. The region of AWS SNS the events will be sent to. You have to make sure the region is consistent with AWS SNS.
Topic Name	Required. The name of the topic the events will be sent to. You have to make sure the topic name exists in AWS SNS.
Correlation ID	Optional. The ID that correlates this alert with the other events. If you leave this field empty, it will use $result.correlation_id$ by default.
Entity	Optional. Object related to the event or alert, such as host, database, EC2 instance. If you leave this field empty, Splunk will use $result.entity$ by default.
Source	Optional. The source of the event or alert. If you leave this field empty, the Splunk platform will use $result.source$ by default.
Timestamp	Optional. The time of the event occurs. If you leave this field empty, the Splunk platform will use $result._time$ by default.
Event	Optional. The details of the event. If you leave this field empty, the Splunk platform will use $result._raw$ by default.
Message	Required. The message that the Splunk Add-on for AWS sent to AWS SNS.