# Input configuration details

## Configure Config inputs for the Splunk Add-on for AWS

> The Config input type supports the collection of Config data (source type: `aws:config, aws:config:notification`). However, it is highly recommended that you configure SQS-based S3 inputs to collect this type of data.

Configure an AWS Config input for the Splunk Add-on for Amazon Web Services on your data collection node through Splunk Web (recommended), or in `local/inputs.conf`. This data source is only available in a subset of AWS regions, which does not currently include China or GovCloud. See the AWS documentation for a full list of supported regions: <http://docs.aws.amazon.com/general/latest/gr/rande.html#awsconfig_region>.

You should only have a single enabled Config modular input for each unique SQS. Multiple enabled modular inputs can cause conflicts when trying to delete SQS messages or S3 records that another modular input is attempting to access and parse. Be sure to disable or delete testing configs before going to production.

Configure a Config input on the data collection node using one of the following ways:

- [Configure a Config input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/Config#Configure_a_Config_input_using_Splunk_Web) (recommended)
- [Configure a Config input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/Config#Configure_a_Config_input_using_configuration_file)

### Configure a Config input using Splunk Web

To configure inputs using Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then click **Create New Input > Config > Config**.

<table>
<tr><td>Argument in configuration file</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td>aws_account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access your Config data. In Splunk Web, select an account from the drop-down list. In inputs.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.</td></tr>
<tr><td>aws_region</td><td>AWS Region</td><td>The AWS region that contains the log notification SQS queue. In inputs.conf, enter the region ID. See the AWS documentation for more information.</td></tr>
<tr><td>sqs_queue</td><td>SQS queue name</td><td>The name of the queue to which AWS sends new Config notifications. In Splunk Web, you can select a queue from the drop-down list, if your account permissions allow you to list queues, or enter the queue name manually. The queue name is the final segment of the full queue URL. For example, if your SQS queue URL is http://sqs.us-east-1.amazonaws.com/123456789012/testQueue, then your SQS queue name is testQueue.</td></tr>
<tr><td>sourcetype</td><td>Source type</td><td>A source type for the events. Enter a value only if you want to override the default of aws:config. Event extraction relies on the default value of source type. If you change the default value, you must update props.conf as well.
Note that the Splunk platform indexes AWS Config events using three variations of this source type, as follows:

- Configuration snapshots are indexed as sourcetype=aws:config.
- Configuration change notifications are indexed as sourcetype=aws:config:notification
- Logs from aws_config.log are indexed as sourcetype=aws:config:log

If you modify the default value of aws:config, you will see &lt;yourcustomsourcetype&gt;:notification and &lt;yourcustomsourcetype&gt;:log.</td></tr>

<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform puts the Config data. The default is main.</td></tr>
<tr><td>Polling interval</td><td>Interval</td><td>The number of seconds to wait before the Splunk platform runs the command again. Default is 30 seconds.</td></tr>
</table>

### Configure a Config input using configuration file

To configure inputs manually in `inputs.conf`, create a stanza using the following template and add it to `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/inputs.conf`. If the file or path does not exist, create it.

```properties
[aws_config://<name>]
aws_account = <value>
aws_region = <value>
sqs_queue = <value>
interval = <value>
sourcetype = <value>
index = <value>
```

Some of these settings have default values that can be found in $SPLUNK_HOME/etc/apps/Splunk_TA_aws/default/inputs.conf:

```properties
[aws_config]
aws_account =
sourcetype = aws:config
queueSize = 128KB
persistentQueueSize = 24MB
interval = 30
```

The values above correspond to the default values in Splunk Web as well as some internal values that are not exposed in Splunk Web for configuration. If you choose to copy this stanza to `/local` and use it as a starting point to configure your `inputs.conf` manually, change the stanza title from `aws_config` to `aws_config://<name>` and add the additional parameters that you need.

### Switch from a Config input to an SQS-based S3 input

The SQS-based S3 input is a more fault-tolerant and higher-performing alternative to the Config input for collecting CloudTrail data. If you are already collecting Config data using a Config input, you can configure an SQS-based S3 input and seamlessly switch to the new input for Config data collection with little disruption.

1. Disable the Config input you are using to collect Config data.
2. Set up a dead-letter queue and the SQS visibility timeout setting for the SQS queue from which you are collecting Config data. See [Configure SQS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWS#Configure_SQS).
3. Create an SQS-based S3 input, pointing to the SQS Queue you configured in the last step. Refer to [Configure SQS-based S3 inputs for the Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/SQS-basedS3#Configure_SQS-based_S3_inputs_for_the_Splunk_Add-on_for_AWS) for the detailed configuration steps.

Once configured, the new SQS-based S3 input will replace the old Config input to collect Config data from the same SQS queue.

## Configure Config Rules inputs for the Splunk Add-on for AWS

Configure Config Rules inputs to collect Config Rules data (source type: aws:config:rules).

Configure an Config Rules input for the Splunk Add-on for AWS on your data collection node through Splunk Web (recommended), or in local/aws_config_rule_tasks.conf. This data source is only available in a subset of AWS regions, which does not currently include China or GovCloud. See the AWS documentation for a full list of supported regions: <http://docs.aws.amazon.com/general/latest/gr/rande.html#awsconfig_region>.

Choose a configuration option:

- Configure a Config Rules input using Splunk Web (recommended)
- Configure a Config Rules input using configuration file

### Configure a Config Rules input using Splunk Web

To configure inputs using Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then click **Create New Input > Config Rules**.

<table>
<tr><td>Argument in configuration file</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td>aws_account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access your Config Rules data. In Splunk Web, select an account from the drop-down list. In aws_config_rule_tasks.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.</td></tr>
<tr><td>region</td><td>Region</td><td>The AWS region that contains the Config Rules. In aws_config_rule_tasks.conf, enter the region ID. See the AWS documentation for more information.</td></tr>
<tr><td>rule_names</td><td>Config Rules</td><td>Config Rules names in a comma-separated list. Leave blank to collect all rules.</td></tr>
<tr><td>sourcetype</td><td>Source Type</td><td>A source type for the events. Enter a value only if you want to override the default of aws:config:rule. Event extraction relies on the default value of source type. If you change the default value, you must update props.conf as well.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform puts the Config Rules data. The default is main.</td></tr>
<tr><td>polling_interval</td><td>Polling Interval</td><td>The data collection interval, in seconds. The default is 300 seconds.</td></tr>
</table>

Here is an example stanza that collects Config Rules data for just two rules.

```properties
[splunkapp2:us-east-1]
aws_account = splunkapp2
region = us-east-1
index = aws
polling_interval = 300
sourcetype = aws:config:rule
rule_names=required-tags,restricted-common-ports
```

### Configure a Config Rules input using configuration file

To configure the input using the configuration files, create `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/aws_config_rule_tasks.conf` using the following template.

```properties
[<name>]
account = <value>
region = <value>
rule_names = <value>
sourcetype = <value>
polling_interval = <value>
index = <value>
```

## Configure Inspector inputs for the Splunk Add-on for AWS

Configure Inspector inputs to collect Inspector data (source type: `aws:inspector`).

Configure an Inspector input for the Splunk Add-on for AWS on your data collection node through Splunk Web (recommended), or in local/aws_inspector_tasks.conf. This data source is only available in a subset of AWS regions. See the AWS documentation for a full list of supported regions: <http://docs.aws.amazon.com/general/latest/gr/rande.html#inspector_region>.

Choose a configuration option:

- [Configure an Inspector input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/Inspector#Configure_an_Inspector_input_using_Splunk_Web) (recommended)
- [Configure an Inspector input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/Inspector#Configure_an_Inspector_input_using_configuration_file)

### Configure an Inspector input using Splunk Web

To configure inputs using Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then click **Create New Input > Inspector**.

<table>
<tr><td>Argument in configuration file</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td>account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access your Inspector findings. In Splunk Web, select an account from the drop-down list. In aws_inspector_tasks.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.</td></tr>
<tr><td>regions</td><td>AWS Region</td><td>The AWS region that contains the data. In aws_inspector_tasks.conf, enter region IDs in a comma-separated list.</td></tr>
<tr><td>sourcetype</td><td>Source type</td><td>A source type for the events. Enter a value only if you want to override the default of aws:inspector. Event extraction relies on the default value of source type. If you change the default value, you must update props.conf as well.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform puts the Inspector findings. The default is main.</td></tr>
<tr><td>polling_interval</td><td>Pooling interval</td><td>The number of seconds to wait before the Splunk platform runs the command again. Default is 300 seconds.
Configure an Inspector input using configuration file</td></tr>
</table>

To configure the input using the configuration files, create `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/aws_inspector_tasks.conf` using the following template.

```properties
[<name>]
account = <value>
regions = <value>
index = <value>
polling_interval = <value>
sourcetype = <value>
```

Here is an example stanza that collects Inspector findings.

```properties
[splunkapp2:us-west-2]
account = splunkapp2
index = default
interval = 300
region = us-west-2
sourcetype = aws:inspector
```

## Configure CloudTrail inputs for the Splunk Add-on for AWS

> The CloudTrail input type supports the collection of CloudTrail data (source type: aws:cloudtrail). However, it is highly recommended that you configure SQS-based S3 inputs to collect this type of data.

Before you begin configuring your CloudTrail inputs, be aware of the following behaviors:

1. You should only have a single enabled CloudTrail modular input for each unique SQS > SNS > S3 bucket path. Multiple enabled modular inputs can cause conflicts when trying to delete SQS messages or S3 records that another modular input is attempting to access and parse. Be sure to disable or delete testing configs before going to production.

2. If you have multiple AWS regions from which you want to gather CloudTrail data, Amazon Web Services recommends that you configure a trail that applies to all regions in the AWS partition in which you are working. You then set up one CloudTrail input to collect data from the centralized S3 bucket where log files from all the regions are stored.

Configure a CloudTrail input on the data collection node using one of the following ways:

- [Configure a CloudTrail input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/CloudTrail#Configure_a_CloudTrail_input_using_Splunk_Web) (recommended)
- [Configure a CloudTrail input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/CloudTrail#Configure_a_CloudTrail_input_using_configuration_file)

### Configure a CloudTrail input using Splunk Web

To configure inputs in Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then click **Create New Input > CloudTrail**.

<table>
<tr><td>Argument in configuration file</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td>aws_account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access your CloudTrail data. In Splunk Web, select an account from the drop-down list. In inputs.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.</td></tr>
<tr><td>aws_region</td><td>AWS Region</td><td>The AWS region that contains the log notification SQS queue. In inputs.conf, enter the region ID. See the AWS documentation for more information.</td></tr>
<tr><td>sqs_queue</td><td>SQS queue name</td><td>The name of the queue to which AWS sends new CloudTrail log notifications. In Splunk Web, you can select a queue from the drop-down list, if your account permissions allow you to list queues, or enter the queue name manually. The queue name is the final segment of the full queue URL. For example, if your SQS queue URL is http://sqs.us-east-1.amazonaws.com/123456789012/testQueue, then your SQS queue name is testQueue.
<tr><td>remove_files_when_done</td><td>Remove logs when done</td><td>A boolean value indicating whether the Splunk platform should delete log files from the S3 bucket after indexing is complete. Default is false.</td></tr>
<tr><td>exclude_describe_events</td><td>Exclude events</td><td>A boolean value indicating whether or not to exclude certain events, such as read-only events that can produce a high volume of data. Default is true.</td></tr>
<tr><td>blacklist</td><td>Blacklist for exclusion</td><td>A PCRE regular expression that specifies event names to if exclude_describe_events is set to true. Leave blank to use the default regex, ^(?:Describe|List|Get).</td></tr>
<tr><td>excluded_events_index</td><td>Excluded events index</td><td>The name of the index in which the Splunk platform should put excluded events. Default is empty, which discards the events.</td></tr>
<tr><td>interval</td><td>Interval</td><td>The number of seconds to wait before the Splunk platform runs the command again. Default is 30 seconds.</td></tr>
<tr><td>sourcetype</td><td>Source type</td><td>A source type for the events. Enter a value only if you want to override the default of aws:cloudtrail. Event extraction relies on the default value of source type. If you change the default value, you must update props.conf as well.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform puts the CloudTrail data. The default is main.</td></tr>
</table>

### Configure a CloudTrail input using configuration file

To configure inputs manually in `inputs.conf`, create a stanza using the following template and add it to `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/inputs.conf`. If the file or path does not exist, create it.

```properties
[aws_cloudtrail://<name>]
aws_account = <value>
aws_region = <value>
sqs_queue = <value>
exclude_describe_events = <value>
remove_files_when_done = <value>
blacklist = <value>
excluded_events_index = <value>
interval = <value>
sourcetype = <value>
index = <value>
```

Some of these settings have default values that can be found in `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/default/inputs.conf`:

```properties
[aws_cloudtrail]
aws_account =
sourcetype = aws:cloudtrail
exclude_describe_events = true
remove_files_when_done = false
queueSize = 128KB
persistentQueueSize = 24MB
interval = 30
```

The values above correspond to the default values in Splunk Web as well as some internal values that are not exposed in Splunk Web for configuration. If you choose to copy this stanza to `/local` and use it as a starting point to configure your `inputs.conf` manually, change the stanza title from aws_cloudtrail to `aws_cloudtrail://<name>`.

### Switch from a CloudTrail input to an SQS-based S3 input

The SQS-based S3 input is a more fault-tolerant and higher-performing alternative to the CloudTrail input for collecting CloudTail data. If you are already collecting CloudTrail data using a CloudTrail input, you can configure an SQS-based S3 input and seamlessly switch to the new input for CloudTrail data collection with little disruption.

1. Disable the CloudTrail input you are using to collect CloudTrail data.
2. Set up a dead-letter queue and the SQS visibility timeout setting for the SQS queue from which you are collecting CloudTrail data. See [Configure SQS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWS#Configure_SQS).
3. Create an SQS-based S3 input, pointing to the SQS Queue you configured in the last step. Refer to [Configure SQS-based S3 inputs for the Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/SQS-basedS3) for the detailed configuration steps.

Once configured, the new SQS-based S3 input will replace the old CloudTrail input to collect CloudTrail data from the same SQS queue.

## Add a CloudWatch Logs input for the Splunk Add-on for AWS

> Splunk strongly recommends against using the CloudWatch Logs inputs to collect VPC Flow Logs data (source type: `aws:cloudwatchlogs:vpcflow`) since the input type will be deprecated in upcoming releases. Configure Kinesis inputs to collect VPC Flow Logs instead. The add-on includes index-time logic to perform the correct knowledge extraction for these events through the Kinesis input as well.

Configure a CloudWatch Logs input for the Splunk Add-on for Amazon Web Services on your data collection node through Splunk Web (recommended), or in `local/aws_cloudwatch_logs_tasks.conf`.

- [Configure a CloudWatch Logs input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/CloudWatchLogs#Configure_a_CloudWatch_Logs_input_using_Splunk_Web) (recommended)
- [Configure a CloudWatch Logs input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/CloudWatchLogs#Configure_a_CloudWatch_Logs_input_using_configuration_file)

### Configure a CloudWatch Logs input using Splunk Web

To configure inputs using Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then choose one of the following menu paths depending on the data type you want to collect:

- **Create New Input > VPC Flow Logs > CloudWatch Logs**.
- **Create New Input > Others > CloudWatch Logs**.

<table>
<tr><td>Argument in configuration file</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td>account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access your CloudWatch Logs data. In Splunk Web, select an account from the drop-down list. In aws_cloudwatch_logs_tasks.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.</td></tr>
<tr><td>region</td><td>AWS Region</td><td>The AWS region that contains the data. In aws_cloudwatch_logs_tasks.conf, enter the region ID.</td></tr>
<tr><td>groups</td><td>Log group</td><td>A comma-separated list of log group names.
Note: Wildcard is not supported for configuring log group names in the current release.
</td></tr>
<tr><td>only_after</td><td>Only After</td><td>GMT time string in '%Y-%m-%dT%H:%M:%S' format. If set, only events after this time are queried and indexed. Defaults to 1970-01-01T00:00:00.</td></tr>
<tr><td>stream_matcher</td><td>Stream Matching Regex</td><td>REGEX to strictly match stream names. Defaults to .*</td></tr>
<tr><td>interval</td><td>Interval</td><td>The number of seconds to wait before the Splunk platform runs the command again. Default is 600 seconds.</td></tr>
<tr><td>sourcetype</td><td>Source type</td><td>A source type for the events. Enter aws:cloudwatchlogs:vpcflow if you are indexing VPC Flow Log data. Enter aws:cloudwatchlogs if you are collecting any other CloudWatch Logs data.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform puts the CloudWatch Logs data. The default is main.</td></tr>
</table>

### Configure a CloudWatch Logs input using configuration file

To configure the input using configuration file, create `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/aws_cloudwatch_logs_tasks.conf` using the following template.

```properties
[<name>]
account = <value>
groups = <value>
index = <value>
interval = <value>
only_after = <value>
region = <value>
sourcetype = <value>
stream_matcher = <value>
```

Here is an example stanza that collects VPC Flow Log data from two log groups.

```properties
[splunkapp2:us-west-2]
account = splunkapp2
groups = SomeName/DefaultLogGroup, SomeOtherName/SomeOtherLogGroup
index = default
interval = 600
only_after = 1970-01-01T00:00:00
region = us-west-2
sourcetype = aws:cloudwatchlogs:vpcflow
stream_matcher = eni.*
```

## Configure CloudWatch inputs for the Splunk Add-on for AWS

Configure CloudWatch inputs to collect CloudWatch data (source type: `aws:cloudwatch`).

Configure a CloudWatch input on the data collection node using one of the following ways:

- [Configure a CloudWatch input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/CloudWatch#Configure_a_CloudWatch_input_using_Splunk_Web) (recommended)
- [Configure a CloudWatch input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/CloudWatch#Configure_a_CloudWatch_input_using_configuration_file)

> **Note** : As a best practice, configure separate CloudWatch inputs for each metric or set of metrics that have different minimum granularities, based on the sampling period that AWS allows for that metric. For example, CPUUtilization has a sampling period of 5 minutes, whereas Billing Estimated Charge has a sampling period of 4 hours. If you configure a granularity that is smaller than the minimum sampling period available in AWS, the input wastes API calls. For more information, see [Sizing, performance, and cost considerations for the Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/Sizingandcost#CloudWatch).

### Configure a CloudWatch input using Splunk Web

To configure inputs in Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then click **Create New Input > CloudWatch**.

<table>
<tr><td>Argument in configuration file</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td>aws_account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access your CloudWatch data. In Splunk Web, select an account from the drop-down list. In inputs.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.</td></tr>
<tr><td>aws_iam_role</td><td>Assume Role</td><td>The IAM role to assume, see Manage IAM roles</td></tr>
<tr><td>aws_region</td><td>AWS Regions</td><td>The AWS region name or names. In Splunk Web, select one or more regions from the drop-down list. In inputs.conf, enter one or more valid AWS region IDs, comma-separated. See the AWS documentation for more information.
Click Advanced to edit Metrics Configuration.</td></tr>
<tr><td>metric_namespace</td><td>Namespace</td><td>The metric namespace. For example, AWS/EBS. In Splunk Web, click + Add Namespace' and 'select a namespace from the drop-down list or manually enter it. If you manually enter a custom namespace, you will need to type in all your JSON manually for the remaining fields. In inputs.conf, enter a valid namespace for the region you specified. You can only specify one metric namespace per input.</td></tr>
<tr><td>metric_dimensions</td><td>Dimensions</td><td>CloudWatch metric dimensions as JSON array or object, with strings as keys and regular expressions as values. Splunk Web automatically populates correctly formatted JSON to collect all metric dimensions in the namespace you have selected. If you want, you can customize the JSON to limit the collection to just the dimensions you want to collect. For example, for the SQS namespace, you can collect only the metrics for Queue Names that start with "splunk" and end with "_current" by entering [{"QueueName": ["\"splunk.*_current\\\\s\""]}].

You can set multiple dimensions in one data input. If you use a JSON array, the dimension matched by any object in the array is matched. A JSON object has strings as keys and values that are either a regex or an array of regexes. A dimension is matched to the object if and only if:

- it has the same key set to the object;
- in the value of each key, there is one or more (in case the value is a list) elements matched by every regex in the value to the key in the JSON object.

For example, [{"key":["val.*", ".*lue"]}] will match {"key":"value"} and {"key":["value"]}, but not {"key":"value", "key2":"value2"}.
Exception: The BucketName dimension does not support wildcards or arrays with length greater than 1. Thus, when you collect metrics from the AWS/S3 namespace, configure separate CloudWatch inputs for each S3 bucket. Example: {"StorageType": ["StandardStorage"], "BucketName": ["my_favorite_bucket"]}.
</td></tr>
<tr><td>metric_names</td><td>Metrics</td><td>CloudWatch metric names in JSON array. For example: ["CPUUtilization","DiskReadOps","StatusCheckFailed_System"]. Splunk Web automatically populates correctly formatted JSON for all metric names in the namespace you have selected. Edit the JSON to remove any metrics you do not want to collect. Collecting metrics you do not need results in unnecessary API calls.</td></tr>
<tr><td>statistics</td><td>Metric statistics</td><td>The metric statistics you want to request. Defaults to Average, Sum, SampleCount, Maximum, Minimum. In inputs.conf, this list must be JSON encoded. For example: ["Average","Sum","SampleCount","Maximum","Minimum"].</td></tr>
<tr><td>sourcetype</td><td>Source type</td><td>A source type for the events. Enter a value if you want to override the default of aws:cloudwatch. Event extraction relies on the default value of source type. If you change the default value, you must update props.conf as well.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform puts the CloudWatch data. The default is main.</td></tr>
<tr><td>polling_interval</td><td>Polling interval</td><td>The polling interval in seconds. Must be a multiple of granularity period and no more than 21600 (6 hours). This value defaults to 3600. As a best practice, do not set this value to more than 12 times the value of the granularity period. Note that the add-on builds in a four minute delay after the granularity period ends before collecting data to allow for latency.</td></tr>
<tr><td>period</td><td>Period</td><td>The granularity, in seconds, of the returned data points. For metrics with regular resolution, a period can be as short as 60 seconds (1 minute) and must be a multiple of 60. Note that different AWS metrics may support different minimum granularity, based on the sampling period that AWS allows for that metric. For example, CPUUtilization has a sampling period of 5 minutes, whereas Billing Estimated Charge has a sampling period of 4 hours. Do not configure a granularity that is less than the allowed sampling period for the selected metric, or the reported granularity will reflect the sampling granularity but be labeled with your configured granularity, resulting in inconsistent data.
The smaller your granularity, the more precise your metrics data becomes. Configuring a small granularity is useful when you want to do precise analysis of metrics and you are not concerned about limiting your data volume. Configure a larger granularity when a broader view is acceptable or you want to limit the amount of data you collect from AWS.</td></tr>
</table>

### Configure a CloudWatch input using configuration file

To configure inputs manually in inputs.conf, create a stanza using the following template and add it to `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/inputs.conf`. If the file or path does not exist, create it.

```properties
[aws_cloudwatch://<name>]
aws_account = <value>
aws_iam_role=<value>
aws_region = <value>
metric_namespace = <value>
metric_names = <value>
metric_dimensions = <value>
statistics = <value>
period = <value>
polling_interval = <value>
sourcetype = <value>
index = <value>
```

Some of these settings have default values that can be found in `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/default/inputs.conf`:

```properties
[aws_cloudwatch]
aws_account =
sourcetype = aws:cloudwatch
queueSize = 128KB
persistentQueueSize = 24MB
interval = 30
```

The values above correspond to the default values in Splunk Web as well as some internal values that are not exposed in Splunk Web for configuration. If you choose to copy this stanza to `/local` and use it as a starting point to configure your `inputs.conf` manually, change the stanza title from aws_cloudwatch to `aws_cloudwatch://<name>`.

## Configure Description inputs for the Splunk Add-on for AWS

Configure Description inputs to collect Description data (source type: `aws:description`).

Configure a Description input on the data collection node using one of the following ways:

- [Configure a Description input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/DescriptionInput#Configure_a_Description_input_using_Splunk_Web) (recommended)
- [Configure a Description input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/DescriptionInput#Configure_a_Description_input_using_configuration_file)

> Note: If you are using the Splunk App for AWS, note that this input is called Metadata in the app.

### Configure a Description input using Splunk Web

To configure inputs in Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then click **Create New Input > Description**.

<table>
<tr><td>Argument in configuration file</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td>account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access your Description data. In Splunk Web, select an account from the drop-down list. In aws_description_tasks.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.</td></tr>
<tr><td>aws_iam_role</td><td>Assume Role</td><td>The IAM role to assume, see Manage IAM roles</td></tr>
<tr><td>regions</td><td>AWS Regions</td><td>The AWS regions for which you are collecting Description data. In Splunk Web, select one or more regions from the drop-down list. In aws_description_tasks.conf, enter one or more valid AWS region IDs, comma-separated. See the AWS documentation for more information.</td></tr>
<tr><td>apis</td><td>APIs/Interval (seconds)</td><td>APIs you want to collect data from, and intervals for each API, in the format of &lt;api name&gt;/&lt;api interval in seconds&gt;,&lt;api name&gt;/&lt;api interval in seconds&gt;. The default value in Splunk Web is
ec2_volumes/3600,ec2_instances/3600,ec2_reserved_instances/3600,ebs_snapshots/3600,elastic_load_balancers/3600,vpcs/3600,vpc_network_acls/3600,cloudfront_distributions/3600,vpc_subnets/3600,rds_instances/3600,ec2_key_pairs/3600,ec2_security_groups/3600
, which collects from all of the APIs supported in this release. Set your intervals to be 3600 seconds (one hour) or longer to avoid rate limiting errors.</td></tr>
<tr><td>sourcetype</td><td>Source type</td><td>A source type for the events. Enter aws:description.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform puts the Description data. The default is main.</td></tr>
</table>

### Configure a Description input using configuration file

To configure a Description input using the configuration files, create `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/aws_description_tasks.conf` using the following template.

```properties
[<name>]
account = <value>
aws_iam_role=<value>
apis = <value>
index = <value>
regions = <value>
sourcetype = <value>
```

Here is an example stanza that collects description data from all supported APIs.

```properties
[desc:splunkapp2]
account = splunkapp2
apis = ec2_volumes/3600,ec2_instances/3600,ec2_reserved_instances/3600,ebs_snapshots/3600,elastic_load_balancers/3600,vpcs/3600,vpc_network_acls/3600,cloudfront_distributions/3600,vpc_subnets/3600,rds_instances/3600,ec2_key_pairs/3600,ec2_security_groups/3600
index = default
regions = us-west-2
sourcetype = aws:description
```

## Configure Generic S3 inputs for the Splunk Add-on for AWS

> From version 4.3.0 onwards, the Splunk Add-on for AWS provides the SQS-based S3 input, which is a more scalable and higher-performing alternative to the generic S3 and incremental S3 input types for collecting various types of log files from S3 buckets. For new inputs for collecting an variety of pre-defined and custom data types, consider using the SQS-based S3 input instead.

The Generic S3 input lists all the objects in the bucket and examines each file's modified date every time it runs to pull uncollected data from an S3 bucket. When the number of objects in a bucket is large, this can be a very time-consuming process with low throughput.

Before you begin configuring your Generic S3 inputs, note the following expected behaviours.

1. You cannot edit the initial scan time parameter of an S3 input after you create it. If you need to adjust the start time of an S3 input, delete it and recreate it.
2. The S3 data input is not intended to read frequently modified files. If a file is modified after it has been indexed, the Splunk platform will index the file again, resulting in duplicated data. Use key/blacklist/whitelist options to instruct the add-on to index only those files that you know will not be modified later.
3. The S3 data input processes compressed files according to their suffixes. Use these suffixes only if the file is in the corresponding format, or data processing errors will occur. The data input supports the following compression types:
    - single file in zip, gzip, tar, or tar.gz formats
    - multiple files with or without folders in zip, tar, or tar.gz format
    > Expanding compressed files requires significant operating system resources.
4. The Splunk platform auto-detects the character set used in your files among these options: UTF-8 with/without BOM, UTF-16LE/BE with BOM, UTF-32BE/LE with BOM. If your S3 key uses a different character set, you can specify it in inputs.conf using the character_set parameter and separate out this collection job into its own input. Mixing non-autodetected character sets in a single input causes errors.
5. If your S3 bucket contains a very large number of files, you can configure multiple S3 inputs for a single S3 bucket to improve performance. The Splunk platform dedicates one process for each data input, so provided that your system has sufficient processing power, performance will improve with multiple inputs. See Performance reference for the S3 input in the Splunk Add-on for AWS for details.
    > To prevent indexing duplicate data, verify that multiple inputs do not collect the same S3 folder and file data.
6. As a best practice, archive your S3 bucket contents when you no longer need to actively collect them. AWS charges for list key API calls that the input uses to scan your buckets for new and changed files, so you can reduce costs and improve performance by archiving older S3 keys to another bucket or storage type.
7. After configuring an S3 input, you may need to wait for a few minutes before new events are ingested and can be searched. The wait time depends on the number of files in the S3 buckets from which you are collecting data – the larger the quantity, the longer the delay. Also, more verbose logging level causes longer data digestion time. Be warned that debug mode is extremely verbose and is not recommended on production systems.

Configure a Generic S3 input on the data collection node using one of the following ways:

- [Configure a Generic S3 input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/S3#Configure_a_Generic_S3_input_using_Splunk_Web) (recommended)
- [Configure a Generic S3 input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/S3#Configure_a_Generic_S3_input_using_configuration_file)

### Configure a Generic S3 input using Splunk Web

To configure inputs in Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then choose one of the following menu paths depending on which data type you want to collect:

- **Create New Input > CloudTrail > Generic S3**
- **Create New Input > CloudFront Access Log > Generic S3**
- **Create New Input > ELB Access Logs > Generic S3**
- **Create New Input > S3 Access Logs > Generic S3**
- **Create New Input > Others > Generic S3**

Make sure you choose the right menu path corresponding to the data type you want to collect. The system will automatically set the appropriate sourcetype and may display slightly different field settings in the subsequent configuration page based on the menu path.

<table>
<tr><td>Argument in configuration file</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td>aws_account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access the keys in your S3 buckets. In Splunk Web, select an account from the drop-down list. In inputs.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.
Note: If the region of the AWS account you select is GovCloud, you may encounter errors like Failed to load options for S3 Bucket. You need to manually add AWS GovCloud Endpoint in the S3 Host Name field. See http://docs.aws.amazon.com/govcloud-us/latest/UserGuide/using-govcloud-endpoints.html for more information.
</td></tr>
<tr><td>aws_iam_role</td><td>Assume Role</td><td>The IAM role to assume, see Manage IAM roles</td></tr>
<tr><td>bucket_name</td><td>S3 Bucket</td><td>AWS Bucket Name</td></tr>
<tr><td>log_file_prefix</td><td>Log File Prefix</td><td>Configure the prefix of the log file. This add-on will search the log files under this prefix.</td></tr>
<tr><td>log_start_date</td><td>Start Date/Time</td><td>The start date of the log.</td></tr>
<tr><td>log_end_date</td><td>End Date/Time</td><td>The end date of the log.</td></tr>
<tr><td>sourcetype</td><td>Source Type</td><td>A source type for the events. Specify only if you want to override the default of aws:s3. You can select a source type from the drop-down or type a custom source type yourself. To index access logs, enter aws:s3:accesslogs, aws:cloudfront:accesslogs, or aws:elb:accesslogs, depending on the log types in the bucket. To index CloudTrail events directly from an S3 bucket, change the source type to aws:cloudtrail.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform should to put the S3 data. The default is main.</td></tr>
<tr><td>ct_blacklist</td><td>CloudTrail Event Blacklist</td><td>Only valid if the source type is set to aws:cloudtrail. A PCRE regular expression that specifies event names to exclude. The default regex is ^$ to exclude events that can produce a high volume of data. Leave it blank if you want all data to be indexed.</td></tr>
<tr><td>blacklist</td><td>CloudTrail Event Blacklist</td><td>A regular expression to indicate the S3 paths that the Splunk platform should exclude from scanning. Regex should match the full path.</td></tr>
<tr><td>polling_interval</td><td>Polling Interval</td><td>The number of seconds to wait before the Splunk platform runs the command again. Default is 1800 seconds.</td></tr>
</table>

### Configure a Generic S3 input using configuration file

When you configure inputs manually in inputs.conf, create a stanza using the following template and add it to `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/inputs.conf`. If the file or path does not exist, create it.

```properties
[aws_s3://<name>]
is_secure = <whether use secure connection to AWS>
host_name = <the host name of the S3 service>
aws_account = <AWS account used to connect to AWS>
bucket_name = <S3 bucket name>
polling_interval = <Polling interval for statistics>
key_name = <S3 key prefix>
recursion_depth = <For folder keys, -1 == unconstrained>
initial_scan_datetime = <Splunk relative time>
terminal_scan_datetime = <Only S3 keys which have been modified before this datetime will be considered. Using datetime format: %Y-%m-%dT%H:%M:%S%z (for example, 2011-07-06T21:54:23-0700).>
max_items = <Max trackable items.>
max_retries = <Max number of retry attempts to stream incomplete items.>
whitelist = <Override regex for blacklist when using a folder key.>
blacklist = <Keys to ignore when using a folder key.>
character_set = <The encoding used in your S3 files. Default to 'auto' meaning that file encoding will be detected automatically amoung UTF-8, UTF8 without BOM, UTF-16BE, UTF-16LE, UTF32BE and UTF32LE. Notice that once one specified encoding is set, data input will only handle that encoding.>
ct_blacklist = <The blacklist to exclude cloudtrail events. Only valid when manually set sourcetype=aws:cloudtrail.>
ct_excluded_events_index = <name of index to put excluded events into. default is empty, which discards the events>
aws_iam_role = <AWS IAM role to be assumed>
```

> Note: Under one AWS account, to ingest logs in different prefixed locations in the bucket, you need to configure multiple AWS data inputs, one for each prefix name. Alternatively,
you can configure one data input but use different AWS accounts to ingest logs in different prefixed locations in the bucket.

Some of these settings have default values that can be found in `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/default/inputs.conf`:

```properties
[aws_s3]
aws_account =
sourcetype = aws:s3
initial_scan_datetime = default
max_items = 100000
max_retries = 3
polling_interval=
interval = 30
recursion_depth = -1
character_set = auto
is_secure = True
host_name = s3.amazonaws.com
ct_blacklist = ^(?:Describe|List|Get)
ct_excluded_events_index =
```

## Configure Incremental S3 inputs for the Splunk Add-on for AWS

> From version 4.3.0 onwards, the Splunk Add-on for AWS provides the SQS-based S3 input, which is a more scalable and higher-performing alternative to the generic S3 and incremental S3 input types for collecting various types of log files from S3 buckets. For new inputs for collecting an variety of pre-defined and custom data types, consider using the SQS-based S3 input instead.

The incremental S3 input only lists and retrieves objects that have not been ingested from a bucket by comparing datetime information included in filenames against checkpoint record, which significantly improves ingestion performance.

Configure an Incremental S3 input on the data collection node using one of the following ways:

- [Configure an Incremental S3 input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/IncrementalS3#Configure_an_Incremental_S3_input_using_Splunk_Web) (recommended)
- [Configure an Incremental S3 input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/IncrementalS3#Configure_an_Incremental_S3_input_using_configuration_file)

### Configure an Incremental S3 input using Splunk Web

To configure inputs in Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then choose one of the following menu paths depending on which data type you want to collect:

- **Create New Input > CloudTrail > Incremental S3**
- **Create New Input > CloudFront Access Log > Incremental S3**
- **Create New Input > ELB Access Logs > Incremental S3**
- **Create New Input > S3 Access Logs > Incremental S3**

Make sure you choose the right menu path corresponding to the data type you want to collect. The system will automatically set the appropriate sourcetype and may display slightly different field settings in the subsequent configuration page based on the menu path.

<table>
<tr><td>Argument in configuration file</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td>aws_account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access the keys in your S3 buckets. In Splunk Web, select an account from the drop-down list. In inputs.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.
Note: If the region of the AWS account you select is GovCloud, you may encounter errors like Failed to load options for S3 Bucket. You need to manually add AWS GovCloud Endpoint in the S3 Host Name field. See http://docs.aws.amazon.com/govcloud-us/latest/UserGuide/using-govcloud-endpoints.html for more information.
</td></tr>
<tr><td>aws_iam_role</td><td>Assume Role</td><td>The IAM role to assume, see Manage IAM roles</td></tr>
<tr><td>bucket_name</td><td>S3 Bucket</td><td>AWS Bucket Name</td></tr>
<tr><td>log_file_prefix</td><td>Log File Prefix</td><td>Configure the prefix of the log file, which along with other path elements, forms the URL under which the addon will search the log files.
The locations of the log files are different for each S3 incremental log type:

cloudtrail: This add-on will search for the cloudtrail logs under &lt;bucket_name&gt;/&lt;log_file_prefix&gt;/AWSLogs/&lt;Account ID&gt;/CloudTrail/&lt;Region ID&gt;/&lt;YYYY/MM/DD&gt;/&lt;file_name&gt;.json.gz.
elb: This add-on will search the elb access logs under &lt;bucket_name&gt;/&lt;log_file_prefix&gt;/AWSLogs/&lt;Account ID&gt;/elasticloadbalancing/&lt;Region ID&gt;/&lt;YYYY/MM/DD&gt;/&lt;file_name&gt;.log.gz.
S3: This add-on will search the S3 access logs under &lt;bucket_name&gt;/&lt;log_file_prefix&gt;&lt;YYYY-mm-DD-HH-MM-SS&gt;&lt;UniqueString&gt;.
cloudfront: This add-on will search the cloudfront access logs under &lt;bucket_name&gt;/&lt;log_file_prefix&gt;&lt;distributionID&gt;&lt;YYYY/MM/DD&gt;.&lt;UniqueID&gt;.gz
Note: Under one AWS account, to ingest logs in different prefixed locations in the bucket, you need to configure multiple AWS data inputs, one for each prefix name. Alternatively, you can configure one data input but use different AWS accounts to ingest logs in different prefixed locations in the bucket.
</td></tr>
<tr><td>log_type</td><td>Log Type</td><td>The type of logs to ingestion. This value is automatically set based on the menu path you chose to access this configuration page.</td></tr>
<tr><td>log_start_date</td><td>Log Start Date</td><td>The start date of the log.</td></tr>
<tr><td>distribution_id</td><td>Distribution ID</td><td>CloudFront distribution ID. This field is displayed only when you accessed the input configuration page through the Create New Input > CloudFront Access Log > Incremental S3 menu path.</td></tr>
<tr><td>sourcetype</td><td>Source Type</td><td>Source type for the events. This value is automatically set for the type of logs you want to collect based on the menu path you chose to access this configuration page.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform should to put the S3 data. The default is main.</td></tr>
<tr><td>interval</td><td>Interval</td><td>The number of seconds to wait before splunkd checks the health of the modular input so that it can trigger a restart if the input crashed. Default is 30 seconds.</td></tr>
</table>

### Configure an Incremental S3 input using configuration file

When you configure inputs manually in inputs.conf, create a stanza using the following template and add it to `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/inputs.conf`. If the file or path does not exist, create it.

```properties
[splunk_ta_aws_logs://<name>]
log_type =
aws_account =
host_name =
bucket_name =
bucket_region =
log_file_prefix =
log_start_date =
log_name_format =
aws_iam_role = AWS IAM role that to be assumed.
max_retries = @integer:[-1, 1000]. default is -1. -1 means retry until success.
max_fails = @integer: [0, 10000]. default is 10000. Stop discovering new keys if the number of failed files exceeded the max_fails.
max_number_of_process = @integer:[1, 64]. default is 2.
max_number_of_thread = @integer:[1, 64]. default is 4.
```

## Configure SQS-based S3 inputs for the Splunk Add-on for AWS

Configure SQS-based S3 inputs for the Splunk Add-on for AWS
SQS-based S3 is the recommended input type for collecting a variety of pre-defined data types: CloudFront Access Logs, Config, ELB Access logs, CloudTrail, S3 Access Logs, as well as other custom data types. Whenever possible, configure SQS-based S3 inputs to collect the data types that it supports.

Before you begin configuring your SQS-based S3 inputs, make sure you configure S3 to send notification to SQS via SNS to notify the add-on that new events were written to the S3 bucket. See [Configure SQS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWS#Configure_SQS) and [Configure SNS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWS#Configure_SNS) sections respectively in this manual.

Also, note the following expected behaviors:

1. The SQS-based S3 input only collects in near-real time newly created AWS service logs stored into S3 buckets with event notifications sent to SQS; any events that occurred in the past or events with no notifications sent through SNS to SQS will not be collected. If you want to collect historical logs stored into S3 buckets in the past, use the S3 input instead. The S3 input lets you set the initial scan time parameter (log start date) to collect data generated after a specified time in the past.
2. To collect logs of the same type from multiple S3 buckets, even across regions, instead of creating one SQS-based S3 input for each bucket, you can set up one input to collect data from all the buckets by configuring these buckets to send notifications to the same SQS queue the SQS-based S3 input polls messages from.
3. To achieve high throughput in ingesting data from an S3 bucket, you can configure multiple SQS-based S3 inputs for the S3 bucket to scale out data collection.
4. After configuring an SQS-based S3 input, you may need to wait for a few minutes before new events are ingested and can be searched. Also, more verbose logging level causes longer data digestion time. Be warned that debug mode is extremely verbose and is not recommended on production systems.
5. The SQS-based input allows you to ingest data from S3 buckets by optimizing the API calls made by the add-on and relying on SQS/SNS to collect events upon receipt of notification.
6. The SQS-based S3 input is stateless, which means that when multiple inputs are collecting data from the same bucket, if one input goes down, the other inputs will continue to collect data and take over the load from the failed input. This way, you can enhance fault tolerance by configuring multiple inputs to collect data from the same bucket.

The SQS-based S3 input supports these types of S3 logs: **CloudTrail Logs, Config, S3 Access Logs, CloudFront Access Logs and ELB Access Logs**. The locations of the log files are derived from the messages the input retrieves from SQS.

Configure a SQS-based S3 input on the data collection node using one of the following ways:

- [Configure an SQS-based S3 input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/SQS-basedS3#Configure_an_SQS-based_S3_input_using_Splunk_Web) (recommended)
- [Configure an SQS-based S3 input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/SQS-basedS3#Configure_an_SQS-based_S3_input_using_configuration_file)

### Configure an SQS-based S3 input using Splunk Web

To configure inputs in Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then choose one of the following menu paths depending on which data type you want to collect:

- **Create New Input > CloudTrail > SQS-based S3**
- **Create New Input > CloudFront Access Log > SQS-based S3**
- **Create New Input > Config > SQS-based S3**
- **Create New Input > ELB Access Logs > SQS-based S3**
- **Create New Input > S3 Access Logs > SQS-based S3**
- **Create New Input > Others > SQS-based S3**

Make sure you choose the right menu path corresponding to the data type you want to collect. The system will automatically set the appropriate sourcetype and may display slightly different field settings in the subsequent configuration page based on the menu path.

<table>
<tr><td>Argument</td><td>Corresponding Field in Splunk Web</td><td>Description</td></tr>
<tr><td>aws_account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access the keys in your S3 buckets. In Splunk Web, select an account from the drop-down list. In inputs.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.
Note: If the region of the AWS account you select is GovCloud, you may encounter errors like Failed to load options for S3 Bucket. You need to manually add AWS GovCloud Endpoint in the S3 Host Name field. See http://docs.aws.amazon.com/govcloud-us/latest/UserGuide/using-govcloud-endpoints.html for more information.
</td></tr>
<tr><td>aws_iam_role</td><td>Assume Role</td><td>The IAM role to assume, see Manage IAM roles</td></tr>
<tr><td>sqs_queue_region</td><td>AWS Region</td><td>AWS region that the SQS queue is in e.g. us-east-1.</td></tr>
<tr><td>sqs_queue_url</td><td>SQS Queue</td><td>The SQL queue URL.</td></tr>
<tr><td>sqs_batch_size</td><td>SQS Batch Size</td><td>The maximum number of messages to pull from the SQS queue in one batch. Enter an integer between 1 and 10 inclusive. Splunk recommends that you set a larger value for small files, and a smaller value for large files. The default SQS batch size is 10. If you are dealing with large files and your system memory is limited, set this to a smaller value.</td></tr>
<tr><td>s3_file_decoder</td><td>S3 File Decoder</td><td>The decoder to use to parse the corresponding log files. The decoder is set according to the Data Type you select. If you select a Custom Data Type, choose one from Cloudtrail, Config, ELB Access Logs, S3 Access Logs, CloudFront Access Logs.</td></tr>
<tr><td>sourcetype</td><td>Source Type</td><td>The source type for the events to collect, automatically filled in based on the decoder chosen for the input.</td></tr>
<tr><td>interval</td><td>Interval</td><td>The length of time in seconds between two data collection runs. The default is 300 seconds.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform should put the SQS-based S3 data. The default is main.</td></tr>
</table>

### Configure an SQS-based S3 input using configuration file

When you configure inputs manually in inputs.conf, create a stanza using the following template and add it to `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/inputs.conf`. If the file or path does not exist, create it.

You can configure the parameters below.

```properties
[aws_sqs_based_s3://<stanza_name>]
aws_account = <value>
interval = <value>
s3_file_decoder = <value>
sourcetype = <value>
sqs_batch_size = <value>
sqs_queue_region = <value>
sqs_queue_url = <value>
```

Valid values for `s3_file_decoder` are: CloudTrail, Config, S3 Access Logs, ELB Access Logs, CloudFront Access Logs, and CustomLogs.

If you want to ingest custom logs other the natively supported AWS log types, you must set `s3_file_decoder = CustomLogs`. This lets you ingest custom logs into Splunk but does not parse the data. To process custom logs into meaningful events, you need to perform additional configurations in `props.conf` and `transforms.conf` to parse the collected data to meet your specific requirements.

For more information on these settings, see `/README/inputs.conf.spec` under your add-on directory.

### Migrate from the Generic S3 input to the SQS-based S3 input

SQS-based S3 is the recommended input type for real-time data collection from S3 buckets because it is scalable and provides better ingestion performance than the other S3 input types.

If you are already using a generic S3 input to collect data, use the following steps to switch to the SQS-based S3 input.

1. Perform prerequisite configurations of AWS services:
    - Set up an SQS queue with a dead-letter queue and proper visibility timeout configured. See [Configure SQS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWS#Configure_SQS).
    - Set up the S3 bucket (with S3 key prefix, if specified) from which you are collecting data to send notifications to the SQS queue. See [Configure SNS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWS#Configure_SQS).
2. Add an SQS-based S3 input using the SQS queue you just configured. See [Configure and SQS-based S3 input](http://docs.splunk.com/Documentation/AddOns/released/AWS/SQS-basedS3#Configure_an_SQS-based_S3_input). After the setup, make sure the new input is enabled and starts collecting data from the bucket.
3. Edit your old generic S3 input and set the **End Date/Time** field to now (the current system time) to phase it out.
4. Wait until all the task executions of the old input are complete. As a best practice, wait at least double your polling frequency.
5. Disable the old generic S3 input.
6. Run the following searches to delete any duplicate events collected during the transition:

For CloudTrail events:

```sql
index=xxx sourcetype=aws:cloudtrail | streamstats count by source, eventID | search count > 1 | eval indexed_time=strftime(_indextime, "%+") | eval dup_id=source.eventID.indexed_time | table dup_id | outputcsv dupes.csv
```

```sql
index=xxx sourcetype=aws:cloudtrail | eval indexed_time=strftime(_indextime, "%+") | eval dup_id=source.eventID.indexed_time | search [|inputcsv dupes.csv | format "(" "" "" "" "OR" ")"] | delete
```

For S3 access logs:

```sql
index=xxx sourcetype=aws:s3:accesslogs | streamstats count by source, request_id | search count > 1 | eval indexed_time=strftime(_indextime, "%+") | eval dup_id=source.request_id.indexed_time | table dup_id | outputcsv dupes.csv
```

```sql
index=xxx sourcetype=aws:s3:accesslogs | eval indexed_time=strftime(_indextime, "%+") | eval dup_id=source.request_id.indexed_time | search [|inputcsv dupes.csv | format "(" "" "" "" "OR" ")"] | delete
```

For CloudFront access logs:

```sql
index=xxx sourcetype=aws:cloudfront:accesslogs | streamstats count by source, x_edge_request_id | search count > 1 | eval indexed_time=strftime(_indextime, "%+") | eval dup_id=source.x_edge_request_id.indexed_time | table dup_id | outputcsv dupes.csv
```

```sql
index=xxx sourcetype=aws:cloudfront:accesslogs | eval indexed_time=strftime(_indextime, "%+") | eval dup_id=source.x_edge_request_id.indexed_time | search [|inputcsv dupes.csv | format "(" "" "" "" "OR" ")"] | delete
```

For classic load balancer (elb) access logs:

Because events do not have unique IDs, use the hash function to remove duplication.

```sql
index=xxx sourcetype=aws:elb:accesslogs | eval hash=sha256(_raw) | streamstats count by source, hash | search count > 1 | eval indexed_time=strftime(_indextime, "%+") | eval dup_id=source.hash.indexed_time | table dup_id | outputcsv dupes.csv
```

```sql
index=xxx sourcetype=aws:elb:accesslogs
| eval hash=sha256(_raw)
| eval indexed_time=strftime(_indextime, "%+")
| eval dup_id=source.hash.indexed_time
| search [|inputcsv dupes.csv | format "(" "" "" "" "OR" ")"] | delete
```

Optionally, delete the old generic S3 input.

### Auto-scale data collection with SQS-based S3 inputs

With the SQS-based S3 input type, you can take full advantage of the auto-scaling capability of the AWS infrastructure to scale out data collection by configuring multiple inputs to ingest logs from the same S3 bucket without creating duplicate events. This is particularly useful if you are ingesting logs from a very large S3 bucket and hit a bottleneck in your data collection inputs.

1. Create an AWS auto scaling group for your heavy forwarder instances where the SQS-based S3 inputs will be running.
    To create an auto scaling group, you can either specify a launch configuration or create an AMI to provision new EC2 instances that host heavy forwarders, and use bootstrap script to install the Splunk Add-on for AWS and configure SQS-based S3 inputs. For detailed information about the auto scaling group and how to create it, refer to the AWS documentation: <http://docs.aws.amazon.com/autoscaling/latest/userguide/AutoScalingGroup.html>.
2. Set CloudWatch alarms for one of the following Amazon SQS metrics:
    - ApproximateNumberOfMessagesVisible (recommended): The number of messages available for retrieval from the queue.
    - ApproximateAgeOfOldestMessage: The approximate age (in seconds) of the oldest non-deleted message in the queue.
    For instructions on setting CloudWatch alarms for Amazon SQS metrics, refer to the AWS documentation: <http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/SQS_AlarmMetrics.html>.
3. Use the CloudWatch alarm as a trigger to provision new heavy forwarder instances with SQS-based S3 inputs configured to consume messages from the same SQS queue to improve ingestion performance.

## Configure Billing inputs for the Splunk Add-on for AWS

Configure Billing inputs to collect Billing data (source type: `aws:billing`).

Configure a Billing input on the data collection node using one of the following ways:

- [Configure a Billing input using Splunk Web] (recommended)
- [Configure a Billing input using configuration file]

> Note: If you want to collect both a Monthly report and a Detailed report, you should configure two billing inputs: one for the Monthly report and another for the Detailed report. This way, you can configure the `interval` and the `report_file_match_regex` for a specific report type rather than having the values you enter there apply to both report types.
> Note: After you have configured your billing inputs, see "Access billing data for the Splunk Add-on for AWS" for more information about data collection behavior and how to access the preconfigured reports included in the add-on.

### Configure a Billing input using Splunk Web

To configure inputs using Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then click Create New Input > Billing.

<table>
<tr><td>Argument in configuration file</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td>AWS Input Configuration</td></tr>
<tr><td>aws_account</td><td>AWS account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access your Billing data. In Splunk Web, select an account from the drop-down list. In inputs.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.</td></tr>
<tr><td>aws_iam_role</td><td>Assume Role</td><td>The IAM role to assume, see Manage IAM roles</td></tr>
<tr><td>bucket_name</td><td>S3 Bucket</td><td>The S3 bucket that is configured to hold billing reports.</td></tr>
<tr><td>monthly_report_type</td><td>Monthly report</td><td>The monthly report type that the Splunk platform collects from your AWS account. Enter one of the following values:

- None
- Monthly report
- Monthly cost allocation report

</td></tr>
<tr><td>detail_report_type</td><td>Detailed report</td><td>The detailed report type that the Splunk platform collects from your AWS account. Enter one of the following values:

- None
- Detailed billing report
- Detailed billing report with resource and tags

</td></tr>
<tr><td>Splunk-Related Configuration</td></tr>
<tr><td>initial_scan_datetime</td><td>Start Date/Time (UTC)</td><td>This add-on starts to collect data later than this time. If you leave this field empty, the default value is 90 days before the input is configured.

> Note: Once the input is created, this value cannot be changed.</td></tr>
sourcetype</td><td>Source type</td><td>A source type for the events. Specify a value if you want to override the default of aws:billing. Event extraction relies on the default value of source type. If you change the default value, you must update props.conf as well.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform puts the billing data. The default is main.
Advanced Settings</td></tr>
<tr><td>interval</td><td>Interval</td><td>Enter the number of seconds to wait before the Splunk platform runs the command again, or a valid cron schedule. Default is 86400 seconds (one day). Note that this interval applies differently for monthly report types and detailed report types. For monthly report types, the interval indicates how often to run the data collection for the current month's monthly report AND how often to check the previous month's monthly report's etag to determine if changes were made. If the etag does not match an already-downloaded version of the monthly report, it will download that report to get the latest data. For detailed report types, the interval indicates how often to check the previous month's detailed report etag to determine if changes were made. If the etag does not match a report already downloaded, it will download that report to get the latest data -- the present month is never collected until the month has ended.
Because AWS billing reports are usually not finalized until several days after the last day of the month, you can use the cron expression 0 0 8-31 * * to skip data collection for the first seven days of every month to avoid collecting multiple copies of not-yet-finalized reports for the just-finished month.</td></tr>
<tr><td>report_file_match_reg</td><td>Regex for report selection</td><td>A regular expression that the Splunk platform uses to match reports in AWS. This expression overrides values in the monthly_report_type and detail_report_type arguments. Thus, if you wish to collect both monthly and Detailed billing reports, but you want to use regex to specify the report collection period, you should configure two separate billing inputs so that the regex you specify here applies only to one of the report types that you want to collect.
Use this regex to limit the report collection to a certain time period to avoid collecting data that you do not need. This is particularly important for the first time that you enable the input. By default, the add-on collects all available reports for all previous months. If you collect Detailed reports, which are large in size, this can result in a very large amount of data collection. You may wish to limit how many months of past data that you collect. For example, you can use the expression \d+-aws-billing-detailed-line-items-201[56789]-\d+.csv.zip to collect only Detailed reports from January 2015 and later, or the expression \d+-aws-billing-detailed-line-items-with-resources-and-tags-2015-((0[4-9])|(10)|(11)|(12).csv.zip)|(\d+-aws-billing-detailed-line-items-with-resources-and-tags-201[6789]-\d+.csv.zip) to collect only the Detailed billing reports with resources and tags for April 2015 and later.</td></tr>
<tr><td>temp_folder</td><td>Temp Folder</td><td>Full path to a non-default folder with sufficient space for temporarily storing downloaded detailed billing report .zip files. Take into account the estimated size of uncompressed detailed billing report files, which can be much larger than that of zipped files. If you do not specify a temp folder, the add-on will use the system temp folder by default.</td></tr>
</table>

### Configure a Billing input using configuration file

To configure inputs in inputs.conf, create a stanza using the following template and add it to `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/inputs.conf`. If the file or path does not exist, create it.

```properties
[aws_billing://<name>]
aws_account = <value>
aws_iam_role=<value>
interval = <value>
initial_scan_datetime = <value>
bucket_name = <value>
detail_report_type = <value>
monthly_report_type = <value>
report_file_match_reg = <value>
sourcetype = <value>
index = <value>
host_name = s3.amazonaws.com
```

Some of these settings have default values that can be found in `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/default/inputs.conf` :

```properties
[aws_billing]
bucket_name =
aws_account =
monthly_report_type = Monthly cost allocation report
detail_report_type = Detailed billing report with resources and tags
report_file_match_reg =
interval = 86400
sourcetype = aws:billing
host_name = s3.amazonaws.com
```

The values above correspond to the default values in Splunk Web. If you choose to copy this stanza to /local and use it as a starting point to configure your inputs.conf manually, change the stanza title from aws_billing to aws_billing://\<name\>

## Configure Cost and Usage Report inputs for the Splunk Add-on for AWS

Configure Billing inputs to collect Cost and Usage Report data (source type: `aws:billing:cur`).

Configure a Cost and Usage Report input on the data collection node using one of the following ways:

- [Configure a Cost and Usage Report input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/BillingCostandUsage#Configure_a_Cost_and_Usage_Report_input_using_Splunk_Web) (recommended)
- [Configure a Cost and Usage Report input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/BillingCostandUsage#Configure_a_Cost_and_Usage_Report_input_using_configuration_file)

> Enable prefices so that AWS will deliver the reports into a folder (folder name will be the name of the prefix). Timestamps, report names can be used to filter results if you do not want to ingest all the reports.

After you have configured your Cost and Usage Report inputs, see [Access billing data for the Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/AccessBillingReportdata) for more information about data collection behavior and how to access the preconfigured reports included in the add-on.

See the Cost and Usage Report section of the AWS documentation for more information on AWS side configuration steps.

### Configure a Cost and Usage Report input using Splunk Web

To configure inputs using Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then click **Create New Input > Billing > Billing (Cost and Usage Report)**.

<table>
<tr><td>Argument in configuration file</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td colspan=3>AWS Input Configuration</td></tr>
<tr><td>aws_account</td><td>AWS account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access your Billing data. In Splunk Web, select an account from the drop-down list. In inputs.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.</td></tr>
<tr><td>aws_iam_role</td><td>Assume Role</td><td>The IAM role to assume, see Manage IAM roles</td></tr>
<tr><td>bucket_name</td><td>S3 Bucket</td><td>The S3 bucket that is configured to hold billing reports.</td></tr>
<tr><td>bucket_region</td><td>S3 Bucket</td><td>The region location where the S3 bucket the is configured to hold billing reports.</td></tr>
<tr><td>report_prefix</td><td>Report Prefix</td><td>Prefices used to allow AWS to deliver the reports into a specified folder.</td></tr>
<tr><td>report_names</td><td>Report Name Pattern</td><td>A regular expression used to filter reports by name</td></tr>
<tr><td colspan=3>Splunk-Related Configuration</td></tr>
<tr><td>start_date</td><td>Start Date</td><td>This add-on starts to collect data later than this time. If you leave this field empty, the default value is 90 days before the input is configured.
Note: Once the input is created, this value cannot be changed.</td></tr>
<tr><td>sourcetype</td><td>Source type</td><td>A source type for the events. Specify a value if you want to override the default of aws:billing. Event extraction relies on the default value of source type. If you change the default value, you must update props.conf as well.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform puts the billing data. The default is main.</td></tr>
<tr><td colspan=3>Advanced Settings</td></tr>
<tr><td>interval</td><td>Interval</td><td>Enter the number of seconds to wait before the Splunk platform runs the command again, or a valid cron schedule. Default is 86400 seconds (one day). Note that this interval applies differently for monthly report types and detailed report types. For monthly report types, the interval indicates how often to run the data collection for the current month's monthly report AND how often to check the previous month's monthly report's etag to determine if changes were made. If the etag does not match an already-downloaded version of the monthly report, it will download that report to get the latest data. For detailed report types, the interval indicates how often to check the previous month's detailed report etag to determine if changes were made. If the etag does not match a report already downloaded, it will download that report to get the latest data -- the present month is never collected until the month has ended.
Because AWS billing reports are usually not finalized until several days after the last day of the month, you can use the cron expression 0 0 8-31 * * to skip data collection for the first seven days of every month to avoid collecting multiple copies of not-yet-finalized reports for the just-finished month.</td></tr>
<tr><td>temp_folder</td><td>Temp Folder</td><td>Full path to a non-default folder with sufficient space for temporarily storing downloaded detailed billing report .zip files. Take into account the estimated size of uncompressed detailed billing report files, which can be much larger than that of zipped files. If you do not specify a temp folder, the add-on will use the system temp folder by default.</td></tr>
</table>

### Configure a Cost and Usage Report input using configuration file

To configure inputs in inputs.conf, create a stanza using the following template and add it to `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/inputs.conf`. If the file or path does not exist, create it.

```properties
[aws_billing_cur://<name>]
start_by_shell = true
aws_account = <value>
aws_iam_role = <value>
bucket_name = <value>
bucket_region = <value>
report_names = <value>
report_prefix = <value>
start_date = <value>
temp_folder = <value>
host_name = s3.amazonaws.com
```

Some of these settings have default values that can be found in `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/default/inputs.conf`:

```properties
[aws_billing_cur]
start_by_shell = false
aws_account = <value>
aws_iam_role = <value>
bucket_name = <value>
bucket_region = <value>
report_names = <value>
report_prefix = <value>
start_date = <value>
temp_folder = <value>
```

The values above correspond to the default values in Splunk Web. If you choose to copy this stanza to `/local` and use it as a starting point to configure your inputs.conf manually, change the stanza title from `aws_billing:cur` to `aws_billing:cur://\<name\>`.

## Configure Kinesis inputs for the Splunk Add-on for AWS

Kinesis is the recommended input type for collecting VPC Flow Logs (source type: `aws:cloudwatchlogs:vpcflow`). This input type also supports the collection of custom data types through Kinesis streams.

Configure an Kinesis input for the Splunk Add-on for AWS on your data collection node through Splunk Web (recommended), or in `local/aws_kinesis_tasks.conf`. This data source is only available in a subset of AWS regions. See the AWS documentation for a full list of supported regions: <http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region>.

> Note: The Kinesis data input only supports gzip compression or plaintext data. It cannot ingest data with other encodings, nor can it ingest data with a mix of gzip and plaintext in the same input. Create separate Kinesis inputs for gzip data and plaintext data.

Configure a Kinesis input on the data collection node using one of the following ways:

- [Configure a Kinesis input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/Kinesis#Configure_a_Kinesis_input_using_Splunk_Web) (recommended)
- [Configure a Kinesis input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/Kinesis#Configure_a_Kinesis_input_using_configuration_file)

### Configure a Kinesis input using Splunk Web

To configure inputs in Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then choose one of the following menu paths depending on which data type you want to collect:

- **Create New Input > VPC Flow Logs > Kinesis**
- **Create New Input > Others > Kinesis**

<table>

<tr><td>Argument</td><td>Corresponding Field in Splunk Web</td><td>Description</td></tr>
<tr><td>account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access your Kinesis data. In Splunk Web, select an account from the drop-down list. In aws_kinesis_tasks.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.</td></tr>
<tr><td>aws_iam_role</td><td>Assume Role</td><td>The IAM role to assume, see Manage IAM roles</td></tr>
<tr><td>region</td><td>AWS Region</td><td>The AWS region that contains the Kinesis streams. In aws_kinesis_tasks.conf, enter the region ID. See the AWS documentation for more information.</td></tr>
<tr><td>stream_names</td><td>Stream Names</td><td>Stream names in a comma-separated list. Leave blank to collect all streams.</td></tr>
<tr><td>encoding</td><td>Encoding with</td><td>The encoding of the stream data. Set to gzip or leave blank, which defaults to Base64. All stream data that you collect in a single input must have the same encoding. If you are collecting VPC Flow Logs data through this input, encoding is typically gzip.</td></tr>
<tr><td>init_stream_position</td><td>Initial Stream Position</td><td>LATEST or TRIM_HORIZON. LATEST starts data collection from the point the input is enabled. TRIM_HORIZON starts collecting with the oldest data record.</td></tr>
<tr><td>format</td><td>Record Format</td><td>CloudWatchLogs or none. If you choose CloudWatchLogs, this add-on will parse the data in CloudWatchLogs format.</td></tr>
<tr><td>sourcetype</td><td>Source type</td><td>A source type for the events.Enter aws:cloudwatchlogs:vpcflow if you are indexing VPC Flow Log data through Kinesis. Enter aws:kinesis if you are collecting any other Kinesis data.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform puts the Kinesis data. The default is main.</td></tr>
</table>

### Configure a Kinesis input using configuration file

To configure the input using the configuration files, create `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/aws_kinesis_tasks.conf` using the following template.

```properties
[<name>]
account = <value>
aws_iam_role=<value>
region = <value>
stream_names = <value>
encoding = <value>
init_stream_position = <value>
format = <value>
sourcetype = <value>
index = <value>
```

Here is an example stanza that collects Kinesis data for all streams available in the region.

```properties
[splunkapp2:us-east-1]
account = splunkapp2
region = us-east-1
encoding =
init_stream_position = LATEST
index = aws
format = CloudWatchLogs
sourcetype = aws:kinesis
```

## Configure SQS inputs for the Splunk Add-on for AWS

Configure an SQS input for the Splunk Add-on for AWS on your data collection node through Splunk Web (recommended), or in `local/aws_sqs_tasks.conf`. This data source is only available in a subset of AWS regions. See the AWS documentation for a full list of supported regions: <http://docs.aws.amazon.com/general/latest/gr/rande.html#inspector_region>.

Choose a configuration option:

- [Configure an SQS input using Splunk Web](http://docs.splunk.com/Documentation/AddOns/released/AWS/SQS#Configure_an_SQS_input_using_Splunk_Web) (recommended)
- [Configure an SQS input using configuration file](http://docs.splunk.com/Documentation/AddOns/released/AWS/SQS#Configure_an_SQS_input_using_configuration_file)

### Configure an SQS input using Splunk Web

To configure inputs using Splunk Web, click on Splunk Add-on for AWS in the left navigation bar on Splunk Web home, then click **Create New Input > Others > SQS**.

<table>
<tr><td>Argument in inputs.conf</td><td>Field in Splunk Web</td><td>Description</td></tr>
<tr><td>aws_account</td><td>AWS Account</td><td>The AWS account or EC2 IAM role the Splunk platform uses to access your SQS data. In Splunk Web, select an account from the drop-down list. In aws_sqs_tasks.conf, enter the friendly name of one of the AWS accounts that you configured on the Configuration page or the name of the autodiscovered EC2 IAM role.</td></tr>
<tr><td>aws_region</td><td>AWS Region</td><td>The AWS region that contains the log notification SQS queue. In aws_sqs_tasks.conf, enter the region ID. See the AWS documentation for more information.</td></tr>
<tr><td>sqs_queues</td><td>SQS queues</td><td>The name of the queue to which AWS sends new SQS log notifications. In Splunk Web, you can select a queue from the drop-down list, if your account permissions allow you to list queues, or enter the queue name manually. The queue name is the final segment of the full queue URL. For example, if your SQS queue URL is http://sqs.us-east-1.amazonaws.com/123456789012/testQueue, then your SQS queue name is testQueue.
You can add multiple queues separated by commas.</td></tr>
<tr><td>sourcetype</td><td>Source type</td><td>A source type for the events. Enter a value only if you want to override the default of aws:sqs. Event extraction relies on the default value of source type. If you change the default value, you must update props.conf as well.</td></tr>
<tr><td>index</td><td>Index</td><td>The index name where the Splunk platform puts the SQS data. The default is main.</td></tr>
<tr><td>interval</td><td>Interval</td><td>The number of seconds to wait before the Splunk platform runs the command again. The default is 30 seconds.</td></tr>
</table>

### Configure an SQS input using configuration file

To configure the input using the configuration files, create `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/local/aws_sqs_tasks.conf` using the following template.

```properties
[<name>]
aws_account = <value>
aws_region = <value>
sqs_queues = <value>
index = <value>
sourcetype = <value>
interval = <value>
```