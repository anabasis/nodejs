
Indexers in a distributed deployment

Important: To better understand this topic, you should be familiar with Splunk Enterprise distributed environments, covered in Distributed Deployment. 

The indexer is the Splunk Enterprise component that creates and manages indexes. The primary functions of an indexer are: 

• Indexing incoming data.


• Searching the indexed data.


In single-machine deployments consisting of just one Splunk Enterprise instance, the indexer also handles the data input and search management functions. 

For larger-scale needs, indexing is split out from the data input function and sometimes from the search management function as well. In these larger, distributed deployments, the indexer might reside on its own machine and handle only indexing, along with searching of its indexed data. In those cases, other Splunk Enterprise components take over the non-indexing roles. 

For instance, you might have a set of Windows and Linux machines generating events, which need to go to a central indexer for consolidation. Usually the best way to do this is to install a lightweight instance of Splunk Enterprise, known as a forwarder, on each of the event-generating machines. These forwarders handle data input and send the data across the network to the indexer residing on its own machine. 

Similarly, in cases where you have a large amount of indexed data and numerous concurrent users searching on it, it can make sense to split off the search management function from indexing. In this type of scenario, known as distributed search, one or more search heads distribute search requests across multiple indexers. The indexers still perform the actual searching of their own indexes, but the search heads manage the overall search process across all the indexers and present the consolidated search results to the user. 

Here is an example of a scaled-out deployment: 

Horizontal scaling new2 60.png 


 While the fundamental issues of indexing and event processing remain the same for distributed deployments, it is important to take into account deployment needs when planning your indexing strategy. 


Forward data to an indexer

To forward remote data to an indexer, you use forwarders, which are Splunk Enterprise instances that receive data inputs and then consolidate and send the data to a Splunk Enterprise indexer. Forwarders come in two flavors: 

•Universal forwarders. These maintain a small footprint on their host machine. They perform minimal processing on the incoming data streams before forwarding them on to an indexer, also known as the receiver.


•Heavy forwarders. These retain most of the functionality of a full Splunk Enterprise instance. They can parse data before forwarding it to the receiving indexer. (See How indexing works for the distinction between parsing and indexing.) They can store indexed data locally and also forward the parsed data to a receiver for final indexing on that machine as well. 


Both types of forwarders tag data with metadata such as host, source, and source type, before forwarding it on to the indexer. 

Forwarders allow you to use resources efficiently when processing large quantities or disparate types of data coming from remote sources. They also enable a number of interesting deployment topologies, by offering capabilities for load balancing, data filtering, and routing. 

For an extended discussion of forwarders, including configuration and detailed use cases, read Forwarding Data. 


Search across multiple indexers

In distributed search, search heads send search requests to indexers and then merge the results back to the user. This is useful for a number of purposes, including horizontal scaling, access control, and managing geo-dispersed data. 

For an extended discussion of distributed search and search heads, including configuration and detailed use cases, see Distributed Search. 

Indexer clusters also use search heads to coordinate searches across the cluster's peer nodes. See About indexer clusters and index replication. 


Deploy indexers in a distributed environment

To implement a distributed environment similar to the diagram earlier in this topic, you need to install and configure three types of components: 

• Indexers


• Forwarders (typically, universal forwarders)


• Search head(s)


Install and configure the indexers

By default, all full Splunk Enterprise instances serve as indexers. For horizontal scaling, you can install multiple indexers on separate machines. 

To learn how to install a Splunk Enterprise instance, read the Installation Manual. 

Then return to this manual for information on configuring each individual indexer to meet the needs of your specific deployment. 

Install and configure the forwarders

A typical distributed deployment has a large number of forwarders feeding data to a few indexers. For most forwarding purposes, the universal forwarder is the best choice. The universal forwarder is a separate downloadable from the full Splunk Enterprise instance. 

To learn how to install and configure forwarders, read Forwarding Data. 

Install and configure the search head(s)

You can install one or more search heads to handle your distributed search needs. Search heads are just full Splunk Enterprise instances that have been specially configured. 

To learn how to configure a search head, read Distributed Search. 

Other deployment tasks

You need to configure Splunk Enterprise licensing by designating a license master. See the chapter Configure Splunk Enterprise licenses in the Admin Manual for more information. 

You can use the Splunk Enterprise deployment server to simplify the job of updating the deployment components. For details on how to configure a deployment server, see Updating Splunk Enterprise Instances. 

Install a cluster of indexers

If data availability, data fidelity, and data recovery are key issues for your deployment, then you should consider deploying an indexer cluster, rather than a series of individual indexers. For further information, see About indexer clusters and index replication. 


Create custom indexes

You can create two types of indexes: 

• Events indexes


• Metrics indexes


Events indexes are the default index type. To create events indexes, see Create events indexes. 

To create metrics indexes, see Create metrics indexes. For general information on metrics indexes, see the Metrics manual, starting with Overview of metrics. 


Create events indexes

The main index, by default, holds all your events. It also serves as the default index for any inputs or search commands that don't specify an index, although you can change the default. 

With a Splunk Enterprise license, you can add an unlimited number of additional indexes. You can add indexes using Splunk Web, the CLI, or indexes.conf. 

This topic covers: 

• The reasons why you might want multiple indexes.


• How to create new indexes.


• How to send events to specific indexes.


• How to search specific indexes.


Why have multiple indexes?

There are several key reasons for having multiple indexes: 

• To control user access.


• To accommodate varying retention policies.


• To speed searches in certain situations.


The main reason you'd set up multiple indexes is to control user access to the data that's in them. When you assign users to roles, you can limit user searches to specific indexes based on the role they're in. 

In addition, if you have different policies for retention for different sets of data, you might want to send the data to different indexes and then set a different archive or retention policy for each index. 

Another reason to set up multiple indexes has to do with the way search works. If you have both a high-volume/high-noise data source and a low-volume data source feeding into the same index, and you search mostly for events from the low-volume data source, the search speed will be slower than necessary, because the indexer also has to search through all the data from the high-volume source. To mitigate this, you can create dedicated indexes for each data source and send data from each source to its dedicated index. Then, you can specify which index to search on. You'll probably notice an increase in search speed. 

Create events indexes

You can create events indexes with Splunk Web, the CLI, or by editing indexes.conf directly. 

Note: To add a new index to an indexer cluster, you must directly edit indexes.conf. You cannot add an index via Splunk Web or the CLI. For information on how to configure indexes.conf for clusters, see Configure the peer indexes in an indexer cluster. That topic includes an example of creating a new cluster index. 

Use Splunk Web 

1. In Splunk Web, navigate to Settings > Indexes and click New. 

2. To create a new index, enter: 

• A name for the index. User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. They cannot begin with an underscore or hyphen, or contain the word "kvstore". 


• The index data type. For event data, click Events. This is the default data type.


• The path locations for index data storage: 
• Home path. Leave blank for default $SPLUNK_DB/<index_name>/db


• Cold path. Leave blank for default $SPLUNK_DB/<index_name>/colddb


• Thawed path. Leave blank for default $SPLUNK_DB/<index_name>/thaweddb



• Enable/disable data integrity check.


• The maximum size of the entire index. Defaults to 500000MB.


• The maximum size of each index bucket. When setting the maximum size, use auto_high_volume for high volume indexes (such as the main index); otherwise, use auto.


• The frozen archive path. Set this field if you want to archive frozen buckets. For information on bucket archiving, see Archive indexed data.


• The app in which the index resides.


• The tsidx retention policy. See Reduce tsidx usage.


For more information on index settings, see Configure index storage. 

3. Click Save. 

You can edit an index by clicking on the index name in the Indexes section of the Settings menu in Splunk Web. Properties that you cannot change in Splunk Web are grayed out. To change those properties, edit indexes.conf, then restart the indexer. 

Note: Some index properties are configurable only by editing the indexes.conf file. Check the indexes.conf topic for a complete list of properties. 

Use the CLI 

Navigate to the $SPLUNK_HOME/bin/ directory and use the add index command. You do not need to stop the indexer first. 

To add a new index called "fflanda", enter the following command: 

splunk add index fflanda


Note: User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. They cannot begin with an underscore or hyphen, or contain the word "kvstore". 

If you do not want to use the default path for your new index, you can use parameters to specify a new location: 

splunk add index foo -homePath /your/path/foo/db -coldPath /your/path/foo/colddb
    -thawedPath /your/path/foo/thawedDb


You can also edit an index's properties from the CLI. For example, to edit an index called "fflanda" using the CLI, type: 

splunk edit index fflanda -<parameter> <value>


For detailed information on index settings, see Configure index storage. 

Edit indexes.conf 

To add a new index, add a stanza to indexes.conf in $SPLUNK_HOME/etc/system/local, identified by the name of the new index. For example: 

[newindex]
homePath=<path for hot and warm buckets>
coldPath=<path for cold buckets>
thawedPath=<path for thawed buckets>
...


For information on index settings, see Configure index storage and the indexes.conf spec file. 

Note: User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. They cannot begin with an underscore or hyphen, or contain the word "kvstore". 

You must restart the indexer after editing indexes.conf. 

For information on adding or editing index configurations on cluster nodes, see Configure the peer indexes in an indexer cluster. 

Send events to specific indexes

By default, all external events go to the index called main. However, you might want to send some events to other indexes. For example, you might want to route all data from a particular input to its own index. Or you might want to segment data or send event data from a noisy source to an index that is dedicated to receiving it. 

Important: To send events to a specific index, the index must already exist on the indexer. If you route any events to an index that doesn't exist, the indexer will drop those events. 

Send all events from a data input to a specific index

To send all events from a particular data input to a specific index, add the following line to the input's stanza in inputs.conf on the Splunk Enterprise component where the data is entering the system: either the indexer itself or a forwarder sending data to the indexer: 

index = <index_name> 

The following example inputs.conf stanza sends all data from /var/log to an index named fflanda: 

[monitor:///var/log]
disabled = false
index = fflanda


Route specific events to a different index

Just as you can route events to specific queues, you can also route specific events to specific indexes. You configure this on the indexer itself, not on the forwarder sending data to the indexer, if any. 

To route certain events to a specific index, edit props.conf and transforms.conf on the indexer: 

1. Identify a common attribute for the events that can be used to differentiate them. 

2. In props.conf, create a stanza for the source, source type, or host. This stanza specifies a transforms_name that corresponds to a regex-containing stanza you will create in transforms.conf. 

3. In transforms.conf, create an stanza named with the transforms_name you specified in step 2. This stanza: 

• Specifies a regular expression that matches the identified attribute from step 1.


• Specifies the alternate index that events matching the attribute should be routed to.


The sections below fill out the details for steps 2 and 3. 

Edit props.conf 

Add the following stanza to $SPLUNK_HOME/etc/system/local/props.conf: 

[<spec>]
TRANSFORMS-<class_name> = <transforms_name>


Note the following: 

• <spec> is one of the following: 
• <sourcetype>, the sourcetype of an event


• host::<host>, where <host> is the host for an event


• source::<source>, where <source> is the source for an event



• <class_name> is any unique identifier.


• <transforms_name> is whatever unique identifier you want to give to your transform in transforms.conf.


Edit transforms.conf 

Add the following stanza to $SPLUNK_HOME/etc/system/local/transforms.conf: 

[<transforms_name>]
REGEX = <your_custom_regex>
DEST_KEY = _MetaData:Index
FORMAT = <alternate_index_name>


Note the following: 

• <transforms_name> must match the <transforms_name> identifier you specified in props.conf. 


• <your_custom_regex> must provide a match for the attribute you identified earlier, in step 1.


• DEST_KEY must be set to the index attribute _MetaData:Index. 


• <alternate_index_name> specifies the alternate index that the events will route to.


Example

This examples routes events of windows_snare_log source type to the appropriate index based on their log types. "Application" logs will go to an alternate index, while all other log types, such as "Security", will go to the default index. 

To make this determination, it uses props.conf to direct events of windows_snare_log source type through the transforms.conf stanza named "AppRedirect", where a regex then looks for the log type, "Application". Any event with a match on "Application" in the appropriate location is routed to the alternate index, "applogindex". All other events go to the default index. 

1. Identify an attribute 

The events in this example look like this: 

web1.example.com	MSWinEventLog	1	Application	721	Wed Sep 06 17:05:31 2006
4156	MSDTC	Unknown User	N/A	Information	WEB1	Printers		String
message: Session idle timeout over, tearing down the session.	179

web1.example.com	MSWinEventLog	1	Security	722	Wed Sep 06 17:59:08 2006
576	Security	SYSTEM	User	Success Audit	WEB1	Privilege Use
Special privileges assigned to new logon:     User Name:      Domain:      Logon
ID: (0x0,0x4F3C5880)     Assigned: SeBackupPrivilege   SeRestorePrivilege
SeDebugPrivilege   SeChangeNotifyPrivilege   SeAssignPrimaryTokenPrivilege 525


Some events contain the value "Application", while others contain the value "Security" in the same location. 

2. Edit props.conf  

Add this stanza to $SPLUNK_HOME/etc/system/local/props.conf: 

[windows_snare_syslog]
TRANSFORMS-index = AppRedirect


This directs events of windows_snare_syslog sourcetype to the AppRedirect stanza in transforms.conf. 

3. Edit transforms.conf  

Add this stanza to $SPLUNK_HOME/etc/system/local/transforms.conf: 

[AppRedirect]
REGEX = MSWinEventLog\s+\d+\s+Application
DEST_KEY = _MetaData:Index
FORMAT = applogindex


This stanza processes the events directed here by props.conf. Events that match the regex (because they contain the string "Application" in the specified location) get routed to the alternate index, "applogindex". All other events route as usual to the default index. 

Search a specific index

When the indexer searches, it targets the default index (by default, main), unless the search explicitly specifies an index. For example, this search command searches in the hatch index: 


index=hatch userid=henry.gale

You can also specify an alternate default index for a given role to search when you create or edit that role. 


Create metrics indexes

You can create metrics indexes with Splunk Web, the CLI, the REST API, or by editing indexes.conf directly. For more about metrics, see Overview of metrics in the Metrics manual. 

Use Splunk Web

1. In Splunk Web, navigate to Settings > Indexes and click New.


2. For Index Name, type a name for the index. User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. Index names cannot begin with an underscore or hyphen, or contain the word "kvstore". 


3. For Index Data Type, click Metrics.


4. Enter the remaining properties of the index as needed. For details, see Create events indexes. 


5. Click Save.


Use the command line interface (CLI)

1. Open a command prompt.


2. Navigate to the $SPLUNK_HOME/bin/ directory.


3. Use the add index command to create an index. User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. Index names cannot begin with an underscore or hyphen, or contain the word "kvstore".


For example, to create an index called mymetricsindex, enter the following command: 

splunk add index mymetricsindex -datatype metric

To list all metrics indexes, enter the following command: 

splunk list index -datatype metric

To list all indexes, including events indexes, enter the following command: 

splunk list index -datatype all

Use the REST API

Create an index using the /data/indexes endpoint with the "datatype=metric" parameter. For details, see /data/indexes in the REST API Reference Manual. 

For example, to create a metrics index called mymetricsindex, enter the following command: 

curl -k -u admin:pass https://localhost:8089/services/data/indexes  \
    -d name=mymetricsindex                                          \
    -d datatype=metric

To list all metrics indexes using the REST API, enter the following command: 

curl -k -u admin:pass https://localhost:8089/services/data/indexes?datatype=metric

To list all indexes, including events indexes, enter the following command: 

curl -k -u admin:pass https://localhost:8089/services/data/indexes?datatype=all

Edit indexes.conf

To create a new metrics index, add a stanza to indexes.conf in $SPLUNK_HOME/etc/system/local, identified by the name of the new index. Change the datatype parameter to datatype = metric. 

For example, to create a metrics index called “mymetricsindex”, add the following stanza: 

[mymetricsindex]
homePath=<path for hot and warm buckets>
coldPath=<path for cold buckets>
thawedPath=<path for thawed buckets>
datatype = metric
...


For information on index settings, see Configure index storage and the indexes.conf spec file. 

User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. They cannot begin with an underscore or hyphen, or contain the word "kvstore".

You must restart the indexer after editing indexes.conf. 

For information on adding or editing index configurations on cluster nodes, see Configure the peer indexes in an indexer cluster. 



Remove indexes and indexed data

You can remove indexed data or even entire indexes from the indexer. These are the main options: 

• Delete events from subsequent searches.


• Remove all data from one or more indexes. 


• Remove or disable an entire index.


• Delete older data, based on a retirement policy.


Caution: Removing data is irreversible. If you want to get your data back once you've removed data using any of the techniques described in this topic, you must re-index the applicable data sources. 


Delete events from subsequent searches

The Splunk search language provides the special command delete to delete event data from subsequent searches. Before using delete , read this section carefully. 

The delete command is available only with events indexes. You cannot use it with metrics indexes 

Note: You cannot run the delete command during a real-time search; you cannot delete events as they come in. If you try to use delete during a real-time search, Splunk Enterprise will display an error. 

Who can delete?

The delete command can only be run by a user with the "delete_by_keyword" capability. By default, Splunk Enterprise ships with a special role, "can_delete" that has this capability (and no others). The admin role does not have this capability by default. It's recommended that you create a special user that you log into when you intend to delete index data. 

For more information, refer to Add and edit roles in Securing Splunk Enterprise. 

How to delete

First run a search that returns the events you want deleted. Make sure that this search returns only the events you want to delete, and no other events. Once you're certain of that, you can pipe the results of the search to the delete command. 

For example, if you want to remove the events you've indexed from a source called /fflanda/incoming/cheese.log so that they no longer appear in searches, do the following: 

1. Disable or remove that source so that it no longer gets indexed. 

2. Search for events from that source in your index: 


source="/fflanda/incoming/cheese.log"

3. Look at the results to confirm that this is the data you want to delete. 

4. Once you've confirmed that this is the data you want to delete, pipe the search to delete: 


source="/fflanda/incoming/cheese.log" | delete

See the page about the delete command in the Search Reference Manual for more examples. 

Note: When running Splunk on Windows, substitute the forward slashes (/) in the examples with backslashes (\). 

Piping a search to the delete command marks all the events returned by that search so that subsequent searches do not return them. No user (even with admin permissions) will be able to see this data when searching. 

Note: Piping to delete does not reclaim disk space. The data is not actually removed from the index; it is just invisible to searches. 

The delete command does not update the metadata of the events, so any metadata searches will still include the events although they are not searchable. The main All indexed data dashboard will still show event counts for the deleted sources, hosts, or sourcetypes. 

The delete operation and indexer clusters

In the normal course of index replication, the effects of a delete operation get quickly propagated across all bucket copies in the cluster, typically within a few seconds or minutes, depending on the cluster load and amount of data and buckets affected by the delete operation. During this propagation interval, a search can return results that have already been deleted. 

Also, if a peer that had primary bucket copies at the time of the delete operation goes down before all the results have been propagated, some of the deletes will be lost. In that case, you must rerun the operation after the primary copies from the downed peer have been reassigned. 


Remove data from one or all indexes

To delete indexed data permanently from your disk, use the CLI clean command. This command completely deletes the data in one or all indexes, depending on whether you provide an <index_name> argument. Typically, you run clean before re-indexing all your data. 

Note: The clean command does not work on clustered indexes. 

How to use the clean command

Here are the main ways to use the clean command: 

• To access the help page for clean, type:


splunk help clean


• To permanently remove data from all indexes, type:


splunk clean eventdata


• To permanently remove data from a single index, type:


splunk clean eventdata -index <index_name>


where <index_name> is the name of the targeted index. 

• Add the -f parameter to force clean to skip its confirmation prompts.


Important: You must stop the indexer before you run the clean command. 

Note: In pre-5.0 versions of Splunk Enterprise, running the clean command caused the indexer to reset the next bucket ID value for the index to 0. Starting with version 5.0, this is no longer the case. So, if the latest bucket ID was 3, after you run clean, the next bucket ID will be 4, not 0. For more information on bucket naming conventions and the bucket ID, see What the index directories look like. 

Examples

This example removes data from all indexes: 

splunk stop
splunk clean eventdata 


This example removes data from the _internal index and forces Splunk to skip the confirmation prompt: 

splunk stop
splunk clean eventdata -index _internal -f



Remove an index entirely

To remove an index entirely (and not just the data contained in it), use the CLI command remove index: 

splunk remove index <index_name>


This command deletes the index's data directories and removes the index's stanza from indexes.conf. 

Before running the command, look through all inputs.conf files (on your indexer and on any forwarders sending data to the indexer) and make sure that none of the stanzas are directing data to the index you plan to delete. In other words, if you want to delete an index called "nogood", make sure the following attribute/value pair does not appear in any of your input stanzas: index=nogood. Once the index has been deleted, the indexer will discard any data still being sent to that index. 

When you run remove index, it first warns you if any of the inputs on the indexer (but not on any forwarders) are still configured to send data to the specified index. You'll see a message like this: 

03-28-2012 23:59:22.973 -0700 WARN  IndexAdminHandler - Events from the following 3 inputs will now be discarded, since they had targeted index=zzz:
03-28-2012 23:59:22.973 -0700 WARN  IndexAdminHandler - type: monitor, id: /home/v/syslog-avg-1000-lines
03-28-2012 23:59:22.973 -0700 WARN  IndexAdminHandler - type: monitor, id: /mnt/kickstart/internal/fermi
03-28-2012 23:59:22.973 -0700 WARN  IndexAdminHandler - type: monitor, id: /mnt/kickstart/internal/flights


You can run remove index while splunkd is running. You do not need to restart splunkd after the command completes. 

The index deletion process is ordinarily fast, but the duration depends on several factors: 

• The amount of data being deleted. 


• Whether you are currently performing heavy writes to other indexes on the same disk. 


• Whether you have a large number of small .tsidx files in the index you're deleting.



Disable an index without deleting it

Use the disable index CLI command to disable an index without deleting it: 

splunk disable index <index_name>


Unlike the remove index command, disable index does not delete index data, and it is reversible (with the enable index command). However, once an index is disabled, splunkd will no longer accept data targeted at it. 

You can also disable an index in Splunk Web. To do this, navigate to Settings > Indexes and click Disable to the right of the index you want to disable. 


Delete older data based on retirement policy

When data in an index reaches a specified age or when the index grows to a specified size, it rolls to the "frozen" state, at which point the indexer deletes it from the index. Just before deleting the data, the indexer can move it to an archive, depending on how you configure your retirement policy. 

For more information, see Set a retirement and archiving policy. 


Manage pipeline sets for index parallelization

Index parallelization is a feature that allows an indexer to maintain multiple pipeline sets. A pipeline set handles the processing of data from ingestion of raw data, through event processing, to writing the events to disk. A pipeline set is one instance of the processing pipeline described in How indexing works. 

By default, an indexer runs just a single pipeline set. However, if the underlying machine is under-utilized, in terms of available cores and I/O both, you can configure the indexer to run two pipeline sets. By running two pipeline sets, you potentially double the indexer's indexing throughput capacity. 

Note: The actual amount of increased throughput on your indexer depends on the nature of your data inputs and other factors. 

In addition, if the indexer is having difficulty handling bursts of data, index parallelization can help it to accommodate the bursts, assuming again that the machine has the available capacity. 

To summarize, these are some typical use cases for index parallelization, dependent on available machine resources: 

• Scale indexer throughput.


• Handle bursts of data.


For a better understanding of the use cases and to determine whether your deployment can benefit from multiple pipeline sets, see Parallelization settings in the Capacity Planning Manual. 

Note: You cannot use index parallelization with multiple pipeline sets for metrics data that is received from a UDP data input. If your system uses multiple pipeline sets, use a TCP or HTTP Event Collector data input for metrics data. For more about metrics, see the Metrics manual. 


Configure the number of pipeline sets

Caution: Before you increase the number of pipeline sets from the default of one, be sure that your indexer can support multiple pipeline sets. Read Parallelization settings in the Capacity Planning Manual. In addition, consult with Professional Services, particularly if you want to increase the number of pipeline sets beyond two. 

To set the number of pipeline sets to two, change the parallelIngestionPipelines attribute in the [general] stanza of server.conf: 

 parallelIngestionPipelines = 2


You must restart the indexer for the change to take effect. 

Unless Professional Services advises otherwise, limit the number of pipeline sets to a maximum of 2. 


How the indexer handles multiple pipeline sets

When you implement two pipeline sets, you have two complete processing pipelines, from the point of data ingestion to the point of writing events to disk. The pipeline sets operate independently of each other, with no knowledge of each other's activities. The effect is essentially the same as if each pipeline set was running on its own, separate indexer. 

Each data input goes to a single pipeline. For example, if you are directly ingesting a file, the entire file will get processed through a single pipeline. The pipelines do not share the file's data. 

When a data input enters the indexer, it can enter either of the pipeline sets. The indexer uses round-robin load balancing to allocate new inputs across its pipeline sets. 

Each pipeline writes to its own set of hot buckets. 


The effect of multiple pipeline sets on indexing settings

Some indexing settings are scoped to pipeline sets. These include any settings that are related to a pipeline, processor or queue. Examples of these include max_fd and maxKBps in limits.conf and maxHotBuckets in indexes.conf. 

If you have multiple pipeline sets, these limits apply to each pipeline set individually, not to the indexer as a whole. For example, each pipeline set is separately subject to the maxHotBuckets limit. If you set maxHotBuckets to 4, each pipeline set is allowed a maximum of four hot buckets at a time, for a total of eight on an indexer with two pipeline sets. 


Forwarders and multiple pipeline sets

You can also configure forwarders to run multiple pipeline sets. Multiple pipeline sets increase forwarder throughput and allow the forwarder to process multiple inputs simultaneously. 

This can be of particular value, for example, when a forwarder needs to process a large file that would occupy the pipeline for a long period of time. With just a single pipeline, no other files can be processed until the forwarder finishes the large file. With two pipeline sets, the second pipeline can ingest and forward smaller files quickly, while the first pipeline continues to process the large file. 

Assuming that the forwarder has sufficient resources and depending on the nature of the incoming data, a forwarder with two pipelines can potentially forward twice as much data as a forwarder with one pipeline. 

How forwarders use multiple pipeline sets

When you enable multiple pipeline sets on a forwarder, each pipeline handles both data input and output. In the case of a heavy forwarder, each pipeline also handles parsing. 

The forwarder uses round-robin load balancing to allocate new inputs across its pipeline sets. 

The forwarder forwards the output streams independently of each other. If the forwarder is configured for load balancing, it load balances each output stream separately. The receiving indexer handles each stream coming from the forwarder separately, as if each stream were coming from a different forwarder. 

Note: The pipeline sets on forwarders and indexers are entirely independent of each other. For example, a forwarder with multiple pipeline sets can forward to any indexer, no matter whether the indexer has one pipeline set or two. The forwarder does not know the pipeline configuration on the indexer, and it does not need to know it. Similarly, an indexer with multiple pipeline sets can receive data from any forwarder, no matter how many pipeline sets the forwarder has. 

Configure pipeline sets on a forwarder

You configure the number of pipeline sets for forwarders in the same way as for indexers, with the parallelIngestionPipelines attribute in the [general] stanza of server.conf. 

For heavy forwarders, the indexer guidelines apply: The underlying machine must be significantly under-utilized. You should generally limit the number of pipeline sets to two and consult with Professional Services. See Parallelization settings in the Capacity Planning Manual. 

For universal forwarders, a single pipeline set uses, on average, around 0.5 of a core, but utilization can reach a maximum of 1.5 cores. Therefore, two pipeline sets will use between 1.0 and 3.0 cores. If you want to configure more than two pipeline sets on a universal forwarder, consult with Professional Services first. 



Optimize indexes

While the indexer is indexing data, one or more instances of the splunk-optimize process will run intermittently, merging index files together to optimize performance when searching the data. The splunk-optimize process can use a significant amount of cpu but only briefly. You can reduce the number of concurrent instances of splunk-optimize by changing the value of maxConcurrentOptimizes in indexes.conf, but this is not typically necessary. 

If splunk-optimize does not run frequently enough, searching will be less efficient. 

splunk-optimize runs only on hot buckets. You can run it on warm buckets manually, if you find one with a larger number of index (.tsidx) files; typically, more than 25. To run splunk-optimize, go to $SPLUNKHOME/bin and type: 

 splunk-optimize -d|--directory <bucket_directory>


splunk-optimize accepts a number of optional parameters. To see a list of available parameters, type: 

splunk-optimize


To enable verbose logging from splunk-optimize to splunkd.log, you can set category.SplunkOptimize in log.cfg to INFO or DEBUG. The recommended way to do this is through the CLI: 

 splunk set log-level SplunkOptimize -level DEBUG -auth admin:passwd


For more information on buckets, see How Splunk stores indexes. 



Use the monitoring console to view indexing performance

You can use the monitoring console to monitor most aspects of your deployment. This topic discusses the console dashboards that provide insight into indexing performance. 

The primary documentation for the monitoring console is located in Monitoring Splunk Enterprise. 

There are two performance dashboards under the Indexing menu: 

• Indexing Performance: Instance


• Indexing Performance: Deployment


As their names suggest, they provide similar information, scoped either to a single instance or to the entire deployment. 

The performance dashboards provide a wide variety of information on indexing processes, such as: 

• Indexing rate


• Queue fill ratios


• CPU information for various parts of the data pipeline


View the dashboards themselves for more information. In addition, see Indexing performance dashboards in Monitoring Splunk Enterprise.

