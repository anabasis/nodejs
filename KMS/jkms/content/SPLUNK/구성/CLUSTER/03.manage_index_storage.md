
How the indexer stores indexes

As the indexer indexes your data, it creates a number of files: 

• The raw data in compressed form (the rawdata journal)


• Indexes that point to the raw data (tsidx files)


• Some other metadata files 


Together, these files constitute the Splunk Enterprise index. The files reside in sets of directories organized by age. Some directories contain newly indexed data; others contain previously indexed data. The number of such directories can grow quite large, depending on how much data you're indexing. 


Why you might care

You might not care, actually. The indexer handles indexed data by default in a way that gracefully ages the data through several states. After a long period of time, typically several years, the indexer removes old data from your system. You might well be fine with the default scheme it uses. 

However, if you're indexing large amounts of data, have specific data retention requirements, or otherwise need to carefully plan your aging policy, you've got to read this topic. Also, to back up your data, it helps to know where to find it. So, read on.... 


How data ages

Each of the index directories is known as a bucket. To summarize so far: 

• An "index" contains compressed raw data and associated index files.


• An index resides across many age-designated index directories.


• An index directory is called a bucket.


A bucket moves through several states as it ages: 

• hot


• warm


• cold


• frozen 


• thawed


As buckets age, they "roll" from one state to the next. As data is indexed, it goes into a hot bucket. Hot buckets are both searchable and actively being written to. An index can have several hot buckets open at a time. 

When certain conditions occur (for example, the hot bucket reaches a certain size or splunkd gets restarted), the hot bucket becomes a warm bucket ("rolls to warm"), and a new hot bucket is created in its place. Warm buckets are searchable, but are not actively written to. There are many warm buckets. 

Once further conditions are met (for example, the index reaches some maximum number of warm buckets), the indexer begins to roll the warm buckets to cold, based on their age. It always selects the oldest warm bucket to roll to cold. Buckets continue to roll to cold as they age in this manner. After a set period of time, cold buckets roll to frozen, at which point they are either archived or deleted. By editing attributes in indexes.conf, you can specify the bucket aging policy, which determines when a bucket moves from one state to the next. 

If the frozen data has been archived, it can later be thawed. Thawed data is available for searches. 

Here are the states that buckets age through: 


Bucket state 

Description 

Searchable? 

Hot  Contains newly indexed data. Open for writing. One or more hot buckets for each index.  Yes  
Warm  Data rolled from hot. There are many warm buckets. Data is not actively written to warm buckets.  Yes  
Cold  Data rolled from warm. There are many cold buckets.  Yes  
Frozen  Data rolled from cold. The indexer deletes frozen data by default, but you can choose to archive it instead. Archived data can later be thawed.  No  
Thawed  Data restored from an archive. If you archive frozen data, you can later return it to the index by thawing it.  Yes  

The collection of buckets in a particular state is sometimes referred to as a database or "db": the "hot db", the "warm db", the "cold db", etc. 


What the index directories look like

Each index occupies its own directory under $SPLUNK_HOME/var/lib/splunk. The name of the directory is the same as the index name. Under the index directory are a series of subdirectories that categorize the buckets by state (hot/warm, cold, or thawed). 

The buckets themselves are subdirectories within those directories. The bucket directory names are based on the age of the data. 

Here is the directory structure for the default index (defaultdb): 


Bucket state 

Default location 

Notes 

Hot  $SPLUNK_HOME/var/lib/splunk/defaultdb/db/*  There can be multiple hot subdirectories, one for each hot bucket. See Bucket naming conventions.  
Warm  $SPLUNK_HOME/var/lib/splunk/defaultdb/db/*  There are separate subdirectories for each warm bucket. See Bucket naming conventions.  
Cold  $SPLUNK_HOME/var/lib/splunk/defaultdb/colddb/*  There are multiple cold subdirectories. When warm buckets roll to cold, they get moved into this directory, but are not renamed.  
Frozen  N/A: Frozen data gets deleted or archived into a directory location you specify.  Deletion is the default; see Archive indexed data for information on how to archive the data instead.  
Thawed  $SPLUNK_HOME/var/lib/splunk/defaultdb/thaweddb/*  Location for data that has been archived and later thawed. See Restore archived data for information on restoring archived data to a thawed state.  

The paths for the hot/warm, cold, and thawed directories are configurable, so, for example, you can store cold buckets in a separate location from hot/warm buckets. See Configure index storage and Use multiple partitions for index data. 

All index locations must be writable. 

Note: In pre-6.0 versions of Splunk Enterprise, replicated copies of indexer cluster buckets always resided in the colddb directory, even if they were hot or warm buckets. Starting with 6.0, hot and warm replicated copies reside in the db directory, the same as for non-replicated copies. 

Bucket states and SmartStore

For indexes eanbled with the SmartStore feature, which places data on a remote store such as S3, the cold state does not ordinarily exist. See Bucket states and SmartStore. 


Bucket naming conventions

Bucket names depend on: 

• The state of the bucket: hot or warm/cold/thawed


• The type of bucket directory: non-clustered, clustered originating, or clustered replicated


Important: Bucket naming conventions are subject to change. 

Non-clustered buckets

A standalone indexer creates non-clustered buckets. These use one type of naming convention. 

Clustered buckets

An indexer that is part of an indexer cluster creates clustered buckets. A clustered bucket has multiple exact copies. The naming convention for clustered buckets distinguishes the types of copies, originating or replicated. 

Briefly, a bucket in an indexer cluster has multiple copies, according to its replication factor. When the data enters the cluster, the receiving indexer writes the data to a hot bucket. This receiving indexer is known as the source cluster peer, and the bucket where the data gets written is called the originating copy of the bucket. 

As data is written to the hot copy, the source peer streams copies of the hot data, in blocks, to other indexers in the cluster. These indexers are referred to as the target peers for the bucket. The copies of the streamed data on the target peers are known as replicated copies of the bucket. 

When the source peer rolls its originating hot bucket to warm, the target peers roll their replicated copies of that bucket. The warm copies are exact replicas of each other. 

For an introduction to indexer cluster architecture and replicated data streaming, read Basic indexer cluster architecture. 

Bucket names

These are the naming conventions: 


Bucket type 

Hot bucket 

Warm/cold/thawed bucket 

Non-clustered  hot_v1_<localid>  db_<newest_time>_<oldest_time>_<localid>  
Clustered originating  hot_v1_<localid>  db_<newest_time>_<oldest_time>_<localid>_<guid>  
Clustered replicated  <localid>_<guid>  rb_<newest_time>_<oldest_time>_<localid>_<guid>  

Note: 

• <newest_time> and <oldest_time> are timestamps indicating the age of the data in the bucket. The timestamps are expressed in UTC epoch time (in seconds). For example: db_1223658000_1223654401_2835 is a warm, non-clustered bucket containing data from October 10, 2008, covering the exact period of 9am-10am.


• <localid> is an ID for the bucket. For a clustered bucket, the originating and replicated copies of the bucket have the same <localid>.


• <guid> is the guid of the source peer node. The guid is located in the peer's $SPLUNK_HOME/etc/instance.cfg file.


In an indexer cluster, the originating warm bucket and its replicated copies have identical names, except for the prefix (db for the originating bucket; rb for the replicated copies). 

Note: In an indexer cluster, when data is streamed from the source peer to a target peer, the data first goes into a temporary directory on the target peer, identified by the hot bucket convention of <localid>_<guid>. This is true for any replicated bucket copy, whether or not the streaming bucket is a hot bucket. For example, during bucket fix-up activities, a peer might stream a warm bucket to other peers. When the replication of that bucket has completed, the <localid>_<guid> directory is rolled into a warm bucket directory, identified by the rb_ prefix. 


Buckets and Splunk Enterprise administration

When you are administering Splunk Enterprise, it helps to understand how the indexer stores indexes across buckets. In particular, several admin activities require a good understanding of buckets: 

• For information on setting a retirement and archiving policy, see Set a retirement and archiving policy. You can base the retirement policy on either the size or the age of data.


• For information on how to archive your indexed data, see Archive indexed data. To learn how to restore data from archive, read Restore archived data.


• To learn how to back up your data, read Back up indexed data. That topic also discusses how to manually roll hot buckets to warm, so that you can then back them up.


• For information on setting limits on disk usage, see Set limits on disk usage.


• For a list of configurable bucket settings, see Configure index storage.


• For information on configuring index size, see Configure index size.


• For information on partitioning index data, see Use multiple partitions for index data.


• For information on how buckets function in indexer clusters, see Buckets and indexer clusters.


• For information on buckets and SmartStore, see Bucket states and SmartStore.


In addition, see indexes.conf in the Admin Manual. 



Configure index storage

You configure indexes in indexes.conf. How you edit indexes.conf depends on whether you're using index replication, also known as indexer clustering: 

• For non-clustered indexes, edit the version of indexes.conf in $SPLUNK_HOME/etc/system/local/, or create one if it does not already exist there. Do not edit the copy in $SPLUNK_HOME/etc/system/default. For information on configuration files and directory locations, see About configuration files. 


• For clustered indexes, create or edit a version of indexes.conf on the cluster master node and then distribute it to all the peer nodes, as described in Configure the peer indexes in an indexer cluster.


For non-clustered indexes only, you can optionally use Splunk Web to configure the path to your indexes. Go to Settings > Server settings > General settings. Under the section Index settings, set the field Path to indexes. After doing this, you must restart the indexer from the CLI, not from within Splunk Web. Most other settings, however, require direct editing of indexes.conf. 


Attributes that affect index buckets

This table lists the key indexes.conf attributes affecting buckets and what they configure. It also provides links to other topics that show how to use these attributes. For the most detailed information on these attributes, as well as others, always refer to the indexes.conf spec file. 

Note: This list is specific to non-SmartStore indexes. The set of attributes that control SmartStore indexes is significantly different. See Configure SmartStore. 


Attribute 

What it configures 

Default 

For more information, see ... 

homePath  The path that contains the hot and warm buckets. (Required.) 
This location must be writable. 
 $SPLUNK_HOME/var/lib/splunk/ defaultdb/db/ (for the default index only)  Configure index path attributes  
coldPath  The path that contains the cold buckets. (Required.) 
This location must be writable. 
 $SPLUNK_HOME/var/lib/splunk/ defaultdb/colddb/ (for the default index only)  Configure index path attributes  
thawedPath  The path that contains any thawed buckets. (Required.) 
This location must be writable. 
 $SPLUNK_HOME/var/lib/splunk/ defaultdb/thaweddb/ (for the default index only)  Configure index path attributes  
repFactor  Determines whether the index gets replicated to other cluster peers. (Required for indexes on cluster peer nodes.)  0 (which means that the index will not get replicated to other peers; the correct behavior for non-clustered indexes). For clustered indexes, you must set repFactor to auto, which causes the index to get replicated.  Configure the peer indexes in an indexer cluster  
maxHotBuckets  The maximum number of hot buckets. This value should be at least 2, to deal with any archival data. The main default index, for example, has this value set to 10.  3, for new, custom indexes.  How data ages
 
maxDataSize  Determines rolling behavior, hot to warm. The maximum size for a hot bucket. When a hot bucket reaches this size, it rolls to warm. This attribute also determines the approximate size for all buckets.  Depends; see indexes.conf.  Set a retirement and archiving policy  
maxWarmDBCount  Determines rolling behavior, warm to cold. The maximum number of warm buckets. When the maximum is reached, warm buckets begin rolling to cold.  300  Use multiple partitions for index data  
maxTotalDataSizeMB  Determines rolling behavior, cold to frozen. The maximum size of an index. When this limit is reached, cold buckets begin rolling to frozen.  500000 (MB)  Set a retirement and archiving policy  
frozenTimePeriodInSecs  Determines rolling behavior, cold to frozen. Maximum age for a bucket, after which it rolls to frozen.  188697600 (in seconds; approx. 6 years)  Set a retirement and archiving policy  
coldToFrozenDir  Location for archived data. Determines behavior when a bucket rolls from cold to frozen. If set, the indexer will archive frozen buckets into this directory just before deleting them from the index.  If you don't set either this attribute or coldToFrozenScript, the indexer will just log the bucket's directory name and then delete it once it rolls to frozen.  Archive indexed data  
coldToFrozenScript  Script to run just before a cold bucket rolls to frozen. If you set both this attribute and coldToFrozenDir, the indexer will use coldToFrozenDir and ignore this attribute.  If you don't set either this attribute or coldToFrozenDir, the indexer will just log the bucket's directory name and then delete it once it rolls to frozen.  Archive indexed data  
homePath.maxDataSizeMB 
coldPath.maxDataSizeMB 
 Maximum size for homePath (hot/warm bucket storage) or coldPath (cold bucket storage). If either attribute is missing or set to 0, its path is not individually constrained in size.  None  Configure index size according to bucket type  
maxVolumeDataSizeMB  Maximum size for a volume. If the attribute is missing, the individual volume is not constrained in size.  None  Configure index size with volumes  


Configure index path attributes

When creating a new index, you configure several index path attributes, for example, homePath and coldPath. When you configure path attributes, follow these restrictions and recommendations: 

• The path must be writable. In the case of homePath, the parent path must also be writable.


• Do not use environment variables in index paths. The only exception to this is SPLUNK_DB.


• The path cannot be a root directory, such as homePath=/myindex or homePath=C:\myindex. 


• It is recommended that you specify the path using $_index_name as placeholder for the index name. For example:



homePath = $SPLUNK_DB/$_index_name/dbAt run time, the indexer expands $_index_name to the name of the index. For example, if the index name is "newindex", homePath becomes $SPLUNK_DB/newindex/db.
The set of index path attributes includes: 

• homePath


• coldPath


• thawedPath


• bloomHomePath


• summaryHomePath


• tstatsHomePath


For more information on path attributes, see the indexes.conf spec file. 

For information on using multiple partitions to hold your index data, see Use multiple partitions for index data . 


Index size and indexer clusters

Note: This section pertains to non-SmartStore indexes only. Clusters handle sizing of SmartStore indexes differently. See Configure data retention for SmartStore indexes. 

The attributes that control the size of a non-SmartStore index and its number of buckets operate on each peer node individually. They do not operate across the cluster. 

For example, take the maxTotalDataSizeMB attribute. This attribute specifies the maximum size of the index. Its value is applied on a per-peer basis to limit the size of the index on each peer. When an index reaches its maximum size on a particular peer node, the peer freezes the oldest bucket in its copy of the index. 

This means that the size of an index on a peer node is determined by the total size of all bucket copies for that index on that peer node. It doesn't matter whether the copies are primary copies, searchable copies, non-searchable copies, or excess copies. They all count toward the index size on that peer. 

Because a cluster usually does not distribute bucket copies perfectly evenly across the set of peer nodes, an index will typically be a different size on each of the peer nodes. This means that the index might reach its maximum size on one peer while still having room to grow on the other peers. 

To handle this situation, each peer tells the master when it freezes a copy of a bucket. At that point, the master no longer initiates fix-up activities for the frozen bucket. The master does not, however, instruct the other peers to freeze their copies of that bucket. Each peer will subsequently freeze its copy of the bucket, if any, when its copy of the index reaches the maximum size limit. See How the cluster handles frozen buckets. 

Note: Although these attributes operate separately on each peer, you should set them to the same values across all peers in the cluster. See Configure the peer indexes in an indexer cluster. 

For help in sizing your cluster disk space needs, see Storage considerations. 



Move the index database

You can move the index database from one location to another. You do this by changing the path definition of SPLUNK_DB through the command-line interface of your operating system. 

The procedures in this topic assume that the index database is in the default location, created during installation. 

If you move individual indexes or parts of an index to separate locations, the procedures in this topic are not valid. For information on the structure of Splunk Enterprise indexes, see How the indexer stores indexes. For information on how to change the location for a single index, see Configure index storage. 

Note: Although you can use Splunk Web to change the locations of individual indexes or index volumes, you cannot use it to change the default storage location of indexes, SPLUNK_DB. 


For *nix users

Prerequisties 

Make sure the target file system has at least 1.2 times the size of the total amount of raw data that you plan to index. 

Steps 

1. Create the target directory with write permissions for the user that Splunk Enterprise runs as. For example, if Splunk Enterprise runs as user "splunk", give it ownership of the directory: 

mkdir /foo/bar
chown splunk /foo/bar/


For information on setting the user that Splunk Enterprise runs as, see Run Splunk Enterprise as a different or non-root user in the Installation Manual. 

2. Stop the indexer: 

splunk stop


3. Copy the index file system to the target directory: 

cp -rp $SPLUNK_DB/* /foo/bar/


4. Unset the SPLUNK_DB environment variable: 

unset SPLUNK_DB


5. Change the SPLUNK_DB attribute in $SPLUNK_HOME/etc/splunk-launch.conf to specify the new index directory: 

SPLUNK_DB=/foo/bar


6. Start the indexer: 

splunk start


The indexer picks up where it left off, reading from, and writing to, the new copy of the index. 

7. You can delete the old index database after verifying that the indexer can read and write to the new location. 


For Windows users

Prerequisties 

Make sure the target drive or directory has at least 1.2 times the size of the total amount of raw data that you plan to index. 

Caution: Do not use mapped network drives for index stores. 

Steps 

1. From a command prompt, make sure that the target directory has permissions that allow the splunkd process to write to that directory: 

C:\Program Files\Splunk> D:
D:\> mkdir \new\path\for\index
D:\> cacls D:\new\path\for\index /T /E /G <the user Splunk Enterprise runs as>:F


For more information about determining the user Splunk Enterprise runs as, see Install on Windows in the Installation Manual. 

2. Stop the indexer: 

splunk stop


You can also use the Services control panel to stop the splunkd and splunkweb services. 

3. Copy the existing index file system to the target directory: 

xcopy "C:\Program Files\Splunk\var\lib\splunk\*.*" D:\new\path\for\index /s /e /v /o /k


4. Unset the SPLUNK_DB environment variable: 

set SPLUNK_DB=


5. Edit the SPLUNK_DB attribute in %SPLUNK_HOME%\etc\splunk-launch.conf to specify the new index directory: 

SPLUNK_DB=D:\new\path\for\index


If the line in the configuration file that contains the SPLUNK_DB attribute has a pound sign (#) as its first character, remove the #. 

6. Start the indexer: 

splunk start


The indexer picks up where it left off, reading from, and writing to, the new copy of the index. 

7. You can delete the old index database after verifying that the indexer can read and write to the new location. 



Use multiple partitions for index data

Note: This topic is not relevant to SmartStore indexes. See About SmartStore. 

The indexer can use multiple disks and partitions for its index data. It's possible to configure the indexer to use many disks/partitions/filesystems on the basis of multiple indexes and bucket types, so long as you mount them correctly and point to them properly from indexes.conf. However, we recommend that you use a single high performance file system to hold your index data for the best experience. 

If you do use multiple partitions, the most common way to arrange the index data is to keep the hot/warm buckets on the local machine, and to put the cold bucket on a separate array of disks (for longer term storage). You'll want to run your hot/warm buckets on a machine with with fast read/write partitions, since most searching will happen there. Cold buckets should be located on a reliable array of disks. 


Configure multiple partitions

To configure multiple partitions: 

1. Set up partitions just as you'd normally set them up in any operating system. 

2. Mount the disks/partitions. 

3. Edit indexes.conf to point to the correct paths for the partitions. You set paths on a per-index basis, so you can also set separate partitions for different indexes. Each index has its own [<index>] stanza, where <index> is the name of the index. These are the settable path attributes: 

• homePath = <path on server> 
• This is the path that contains the hot and warm databases for the index. 


• See Configure index path attributes for guidelines on defining index paths.



• coldPath = <path on server> 
• This is the path that contains the cold databases for the index. 


• See Configure index path attributes for guidelines on defining index paths.



•thawedPath = <path on server> 
• This is the path that contains any thawed databases for the index.


Configure maximum index size

Note: This topic is not relevant to SmartStore indexes. See Configure data retention for SmartStore indexes. 

You can configure maximum index size in a number of ways: 

• On a per-index basis


• For hot/warm and cold buckets separately


• Across indexes, using volumes


To configure index storage size, you set attributes in indexes.conf. For more information on the attributes mentioned in this topic, read "Configure index storage". 

Caution: While processing indexes, the indexer might occasionally exceed the configured maximums for short periods of time. When setting limits, be sure to factor in some buffer space. Also, note that certain systems, such as most Unix systems, maintain a configurable reserve space on their partitions. You must take that reserve space, if any, into account when determining how large your indexes can grow. 


Configure index size for each index

To set the maximum index size on a per-index basis, use the maxTotalDataSizeMB attribute. When this limit is reached, buckets begin rolling to frozen. 


Configure index size according to bucket type

To set the maximum size for homePath (hot/warm bucket storage) or coldPath (cold bucket storage), use the maxDataSizeMB attributes: 

# set hot/warm storage to 10,000MB
homePath.maxDataSizeMB = 10000 
# set cold storage to 5,000MB
coldPath.maxDataSizeMB = 5000


The maxDataSizeMB attributes can be set globally or for each index. An index-level setting will override a global setting. To control bucket storage across groups of indexes, use the maxVolumeDataSizeMB attribute, described below. 


Configure index size with volumes

You can manage disk usage across multiple indexes by creating volumes and specifying maximum data size for them. A volume represents a directory on the file system where indexed data resides. 

Volumes can store data from multiple indexes. You would typically use separate volumes for hot/warm and cold buckets. For instance, you can set up one volume to contain the hot/warm buckets for all your indexes, and another volume to contain the cold buckets. 

You can use volumes to define homePath and coldPath. You cannot use them to define thawedPath. 

In addition, you must use volumes if you explicitly define bloomHomePath. For information on bloomHomePath, see the topic "Configure bloom filters" in this manual. 

Configure a volume

To set up a volume, use this syntax: 

[volume:<volume_name>]
path = <pathname_for_volume>


You can also optionally include a maxVolumeDataSizeMB attribute, which specfies the maximum size for the volume. 

For example: 

[volume:hot1]
path = /mnt/fast_disk
maxVolumeDataSizeMB = 100000


The example defines a volume called "hot1", located at /mnt/fast_disk, with a maximum size of 100,000MB. 

Similarly, this stanza defines a volume called "cold1" that uses a maximum of 150,000MB: 

[volume:cold1]
path = /mnt/big_disk
maxVolumeDataSizeMB = 150000


Use a volume

Once you configure volumes, you can use them to define an index's homePath and coldPath. For example, using the volumes configured above, you can define two indexes: 

[idx1]
homePath = volume:hot1/idx1
coldPath = volume:cold1/idx1

[idx2]
homePath = volume:hot1/idx2
coldPath = volume:cold1/idx2


You can use volumes to manage index storage space in any way that makes sense to you. Usually, however, volumes correlate to hot/warm and cold buckets, because of the different storage requirements typical when dealing with different bucket types. So, you will probably use some volumes exclusively for designating homePath (hot/warm buckets) and others for coldPath (cold buckets). 

When a volume containing warm buckets reaches its maxVolumeDataSizeMB, it starts rolling buckets to cold. When a volume containing cold buckets reaches its maxVolumeDataSizeMB, it starts rolling buckets to frozen. If a volume contains both warm and cold buckets (which will happen if an index's homePath and coldPath are both set to the same volume), the oldest bucket will be rolled to frozen. 


Put it all together

This example shows how to use the per-index homePath.maxDataSizeMB and coldPath.maxDataSizeMB attributes in combination with volumes to maintain fine-grained control over index storage. In particular, it shows how to use those attributes to prevent bursts of data into one index from triggering massive bucket moves from other indexes. You can use these per-index settings to ensure that no index will ever occupy more than a specified size, thereby alleviating that concern. 

# global settings

# Inheritable by all indexes: no hot/warm bucket can exceed 1 TB.
# Individual indexes can override this setting.
homePath.maxDataSizeMB = 1000000

# volumes

[volume:caliente]
path = /mnt/fast_disk
maxVolumeDataSizeMB = 100000

[volume:frio]
path = /mnt/big_disk
maxVolumeDataSizeMB = 1000000

# indexes

[i1]
homePath = volume:caliente/i1
# homePath.maxDataSizeMB is inherited from the global setting
coldPath = volume:frio/i1
# coldPath.maxDataSizeMB not specified anywhere: 
# This results in no size limit - old-style behavior

[i2]
homePath = volume:caliente/i2
homePath.maxDataSizeMB = 1000  
# overrides the global default
coldPath = volume:frio/i2
coldPath.maxDataSizeMB = 10000  
# limits the size of cold buckets

[i3]
homePath = /old/style/path
homePath.maxDataSizeMB = 1000
coldPath = volume:frio/i3
coldPath.maxDataSizeMB = 10000


Set limits on disk usage

Note: This topic is not relevant to SmartStore indexes. See Initiate eviction based on occupancy of the cache's disk partition for information on how SmartStore controls local disk usage. 

Splunk Enterprise uses several methods to control disk space. Indexes consume most of the disk space. If you run out of disk space, the indexer stops indexing. You can set a minimum free space limit to control how low free disk space falls before indexing stops. Indexing resumes once space exceeds the minimum. 

Note: To determine how much space you need for your indexes, see "Estimate your storage requirements" in the Capacity Planning Manual. 


Set minimum free disk space

You can set a minimum amount of free disk space for the disk where indexed data is stored. If the limit is reached, the indexer stops operating. Both indexing and searching are affected: 

• Periodically, the indexer checks space on all partitions that contain indexes. If the free disk space limit has been reached on any of those partitions, the indexer stops indexing data until more space is available. A UI banner and splunkd warning are posted to indicate the need to clear more disk space.


• Before attempting to launch a search, the indexer requires that the specified amount of free space be available on the file system where the dispatch directory is stored, $SPLUNK_HOME/var/run/splunk/dispatch


The default minimum free disk space is 5000MB. 

Note: 

• The indexer does not clear any of its disk space with this method. It simply pauses until more space becomes available.


• Incoming data can be lost while indexing is suspended.


You can set minimium free disk space through Splunk Web, the CLI, or the server.conf configuration file. 

In Splunk Web

To specify minimum disk usage in Splunk Web: 

1. Click Settings in the upper right portion of Splunk Web. 

2. Click Server settings. 

3. Click General settings. 

4. Under the Index settings section, set the field Pause indexing if free disk space (in MB) falls below to the desired minimum free disk space in megabytes. 

5. Click Save. 

6. Restart the indexer for your changes to take effect. 

From the command line interface (CLI)

You can set the minimum free disk space with the CLI. This example sets the minimum free disk space to 20,000MB (20GB): 

splunk set minfreemb 20000
splunk restart


For information on using the CLI, see "About the CLI" in the Admin manual. 

In server.conf

You can also set the minimum free disk space in the server.conf file. The relevant stanza/attribute is this: 

[diskUsage]
minFreeSpace = <num>


Note that <num> represents megabytes. The default is 5000. 


Control index storage

The indexes.conf file contains index configuration settings. You can control disk storage usage by specifying maximum index size or maximum age of data. When one of these limits is reached, the oldest indexed data will be deleted (the default) or archived. You can archive the data by using a predefined archive script or creating your own. 

For detailed instructions on how to use indexes.conf to set maximum index size or age, see "Set a retirement and archiving policy". 

For detailed information on index storage, see "How the indexer stores indexes". 



Reduce tsidx disk usage

The tsidx retention policy determines how long the indexer retains the tsidx files that it uses to search efficiently and quickly across its data. By default, the indexer retains the tsidx files for all its indexed data for as long as it retains the data itself. By adjusting the policy to remove tsidx files associated with older data, you can set the optimal trade-off between storage costs and search performance. 

The indexer stores tsidx files in buckets alongside the rawdata files. The tsidx files are vital for efficient searching across large amounts of data. They also occupy substantial amounts of storage. 

For data that you are regularly running searches across, you absolutely need the tsidx files. However, if you have data that requires only infrequent searching as it ages, you can adjust the tsidx retention policy to reduce the tsidx files once they reach a specified age. This allows you to reduce the disk space that your indexed data occupies. 

The tsidx reduction process eliminates the full-size tsidx files and replaces them with mini versions of those files that contain essential metadata. The rawdata files and some other metadata files remain untouched. You can continue to search across the aged data, if necessary, but such searches will exhibit significantly worse performance. Rare term searches, in particular, will run slowly. 

To summarize, the main use case for tsidx reduction is for environments where most searches run against recent data. In that case, fast access to older data might not be worth the cost of storing the tsidx files. By reducing tsidx files for older data, you incur little performance hit for most searches while gaining large savings in disk usage. 

Note: You cannot reduce tsidx files for buckets in SmartStore indexes However, you can still search buckets that were tsidx-reduced before migration to SmartStore. See About SmartStore. 


Estimate the storage savings

Tsidx reduction replaces a bucket's full-size tsidx files with smaller versions of those files, known as mini-tsidx files. It also eliminates the bucket's merged_lexicon.lex file. 

The full-size tsidx files usually constitute a large portion of the overall bucket size. The exact amount depends on the type of data. Data with many unique terms requires larger tsidx files. As a general guideline, the tsidx reduction process decreases bucket size by approximately one-third to two-thirds. For example, a 1GB bucket decreases in size to somewhere between 350MB and 700MB. 

To make a rough estimate of a bucket's reduction potential, look at the size of its merged_lexicon.lex file. The merged_lexicon.lex file is an indicator of the number of unique terms in a bucket's data. Buckets with larger merged_lexicon.lex files have tsidx files that reduce to a greater degree, because of the greater number of unique terms. 

The size of a mini-tsidx file is generally about 5% to 10% of the size of the corresponding original, full-size file. As mentioned earlier, however, the overall reduction in bucket size is less than that - typically, one-third to two-thirds. This is because, in addition to the mini-tsidx files, the reduced bucket retains the rawdata file and a number of metadata files. 


How tsidx reduction works

When you enable tsidx reduction, you specify a reduction age, on a per-index basis. When buckets in that index reach the specified age, the indexer reduces their tsidx files. 

The reduction process

The tsidx reduction process runs, by default, every ten minutes. It checks each bucket in the index and reduces the tsidx files in any bucket whose most recent event is at least the specified reduction age. 

The reduction process runs on only a single bucket at a time. If multiple buckets are ready for reduction, the process handles them sequentially. 

The reduction process is fast. For example, when running on a 1GB bucket, it typically completes in just a few seconds. 

Once a tsidx file is reduced, it stays reduced. If you disable the tsidx reduction setting or increase the reduction age, the change affects only buckets that are not already reduced. If necessary, however, there is a way to convert reduced buckets back into buckets with full tsidx files. See Restore reduced buckets to their orginal state. 

Effect of reduction on bucket files

The tsidx reduction process eliminates the full-size tsidx files from each targeted bucket after replacing them with mini versions that contain only essential metadata. The mini-tsidx file consists of the header of the original tsidx file, which contains metadata about each event. In addition, tsidx reduction eliminates the bucket's merged_lexicon.lex file. 

The bucket retains its rawdata file, along with the mini-tsidx files and certain other metadata files, including the bloomfilter file. 

Full size tsidx files have a .tsidx filename extension. Mini-tsidx files use the .mini.tsidx extension. 

Note: The full-size version of the tsidx file gets deleted only after the mini version has been created. This means that the bucket will briefly contain both versions of the file, with the commensurate increase in disk usage. 

Effect of reduction on in-progress searches

If a search is in progress on a particular bucket that qualifies for tsidx reduction, the reduction for that bucket will be delayed until the search on the bucket completes. The mini-tsidx files will be created but deletion of the full-size files will await the search completion. 

Note: If the indexer is performing a search that ranges across multiple buckets, including one that is ready for reduction, reduction of the bucket might complete before the search reaches it. As expected, when the search does reach the reduced bucket, it will run slowly on that bucket. 


Searches across reduced buckets

Once a bucket has undergone tsidx reduction, you can run searches across the bucket, but they will take much longer to complete. Since the indexer searches the most recent buckets first, it will return results from all non-reduced buckets before it reaches the reduced buckets. 

When the search hits the reduced buckets, a message appears in Splunk Web to warn users of a potential delay in search completion: "Search on most recent data has completed. Expect slower search speeds as we search the minified buckets." 

A few search commands do not work with reduced buckets. These include tstats and typeahead. A warning is added to search.log if such a search touches a reduced bucket: "The full buckets will return results and the reduced buckets will return 0 results." In addition, for the tstats command only, the following message appears in Splunk Web: "Reduced buckets were found in index={index}. Tstats searches are not supported on reduced buckets. Search results will be incorrect." 

Note: Tsidx reduction does not touch tsidx files for accelerated data models, which are maintained in their own directories, separate from the index buckets. Therefore, tstats commands that are restricted to an accelerated data model will continue to function normally and are not affected by this feature. 


Configure the tsidx retention policy

By default, the indexer retains all tsidx files for the life of the buckets. To change the policy, you must enable tsidx reduction. 

You can also change the tsidx retention period from its default of seven days. A bucket gets reduced only when all events in the bucket exceed the retention period. 

Configure through Splunk Web

To enable tsidx reduction on an index, edit the index: 

1. Navigate to Settings > Indexes. 

2. Click the name of the index that you want to edit. 

3. Go to the Storage Optimization section of the Edit screen. 

4. In the Tsidx Retention Policy field, click Enable Reduction. 

5. To modify the default retention period, edit the "Reduce tsidx files older than" field. 

6. Click Save. 

Configure in indexes.conf

You can enable tsidx reduction by directly editing indexes.conf. You can enable reduction for one or more indexes individually or for all indexes globally. 

To enable tsidx reduction for a single index, place the relevant attributes under the index's stanza in indexes.conf. For example, to enable reduction for the "newone" index and to set the retention period to 10 days: 

[newone]
enableTsidxReduction = true
timePeriodInSecBeforeTsidxReduction = 864000


To enable tsidx reduction for all indexes, place the settings under the [default] stanza. 

You must restart the indexer for the settings to take effect. 

Configure through the CLI

To enable tsidx reduction, with a 10 day retention period, on an index called "newone": 

splunk edit index newone -enableTsidxReduction true -timePeriodInSecBeforeTsidxReduction 864000


You do not need to restart the indexer after running this command. 


Performance impact when you first enable tsidx reduction

Once you enable tsidx reduction, the indexer begins to look for buckets to reduce. It reduces all buckets that exceed the specified retention period. The indexer reduces only one bucket at a time, so performance impact should be minimal. 


Determine whether a bucket is reduced

Run the dbinspect search command: 

| dbinspect index=_internal


The tsidxState field in the results specifies "full" or "mini" for each bucket. 


Tsidx reduction and indexer clusters

An indexer cluster runs tsidx reduction in the same way, and according to the same rules and settings, as a standalone indexer. However, since only searchable bucket copies have tsidx files to begin with, reduction only occurs on searchable copies. With tsidx reduction enabled, a searchable bucket copy can contain either a full-size or a mini tsidx file, depending on the age of the bucket. 

You must push changes to the tsidx reduction settings by means of the configuration bundle method. This ensures that all peer nodes use the same settings. Tsidx reduction then occurs at approximately the same time for all searchable copies of a reduction-ready bucket, no matter what peers they reside in. 

If, post-reduction, the cluster must convert a non-searchable copy of a reduced bucket to searchable to meet the search factor, there are two ways that the conversion can proceed: 

• If another searchable copy of the bucket exists in the cluster, the cluster will stream that copy's mini-tsidx files to the non-searchable copy. When streaming is complete, the copy is considered searchable.


• If no other searchable copy of the bucket exists, the cluster has no mini-tsidx files available for streaming to the non-searchable copy. In that case, the cluster must first build full-size tsidx files from the non-searchable copy's rawdata file and then reduce the full-size files. There is no way to create mini-tsidx files directly from a rawdata file.


For more information on how an indexer cluster makes non-searchable copies of a bucket searchable, see Bucket-fixing scenarios. 


Restore reduced buckets to their original state

You cannot restore reduced buckets to their original state merely by increasing the age setting for tsidx reduction. That setting does not affect buckets that have already been reduced. 

Instead, to revert a bucket with mini-tsidx files to full-size tsidx files: 

1. Stop the indexer. 

2. In indexes.conf, either disable tsidx reduction or increase the age setting for tsidx reduction beyond the age of the buckets that you want to restore. Otherwise, the bucket will be reduced for a second time soon after you revert it. 

3. Run the splunk rebuild command on the bucket: 

splunk rebuild <bucket directory>


See "Rebuild a single bucket." 

4. Restart the indexer. 



Configure bloom filters

This topic talks about bloom filters and how Splunk Enterprise uses them to improve search performance, particularly for rare term searches. 

Before you continue reading this topic, you should be familiar with how the indexer stores data and how the data ages after it has been indexed. Basically, indexed data resides in database directories consisting of subdirectories called buckets. Each index has its own set of databases. As data ages, it moves through several types of buckets (hot, warm, cold, and frozen). Read more about "How the indexer stores data" and "How data ages". 


Why bloom filters?

A Bloom filter is a data structure that is used to test whether an element is a member of a set. Our implementation stores the bloom filter as a file on disk in each bucket. When you run a search, especially when you are searching for rare terms, using bloom filters significantly decreases the time it takes to retrieve events from an index. 

As the indexer indexes your time-series data, it creates a compressed file that contains the raw data broken into events based on timestamps and a set of time-series index (tsidx) files. The tsidx files are lexicon files that act as a dictionary of all the keywords in your data (error codes, response times, etc.) and contain references to the location of events in the raw data. When you run a search, the indexer searches the tsidx files for the keywords and retrieves the events from the referenced raw data file. 

Bloom filters work at the bucket level and use a separate file, bloomfilter, which is basically a hash table that can tell you that a keyword definitely does not exist in a bucket. Then, when you run a search, the indexer only need search the tsidx files in the buckets that the bloom filters do not rule out. The execution cost of retrieving events from disk grows with the size and number of tsidx files. Because they decrease the number of tsidx files that the indexer need search, bloom filters decrease the time it takes to search each bucket. 

Instead of storing all of the unique keywords found in a bucket's tsidx files, the bloom filter computes a hash for each keyword. Multiple keywords can result in the same hash, which means that you can have false positives but never false negatives. Because of this, bloom filters can quickly rule out terms that definitely do not exist in a particular bucket and the indexer moves on to searching the next bucket. If the bloom filter cannot rule out a bucket (the keyword may or may not actually exist in the bucket), the indexer searches the bucket normally. 


Configure bloom filters

Note: This section is not relevant to SmartStore indexes. For SmartStore indexes, bloomfilters must be enabled and they must use the default path. See About SmartStore. 

Bloom filters are created when buckets roll from hot to warm. By default, they are deleted when the buckets roll to frozen, unless you have configured a different retention behavior. This section talks about the configuration file parameters you can use to configure and manage your bloomfilter files. 

To specify whether or not you want to use bloom filters, use the use_bloomfilter parameter in limits.conf: 

[search]
use_bloomfilter = true|false
* Control whether to use bloom filters to rule out buckets.
* Defaults to True.


To create a bloom filter for a specific index, edit the following Per Index options in indexes.conf: 

bloomHomePath = <path on indexer>
    * The location where the bloom filter files for the index are stored.
    * If specified, it must be defined in terms of a volume definition.
    * If not specified, bloom filter files for the index will be stored inside the bucket directories.
    * The path must be writable.
    * You must restart splunkd after changing this parameter.

createBloomfilter = true|false
* Determines whether to create bloom filter files for the index.
* Defaults to "true".


Determine which indexes.conf changes require restart

Some changes to indexes.conf require that you restart the indexer for the changes to take effect: 

• Changing any of these attributes: rawChunkSizeBytes, minRawFileSyncSecs, syncMeta, maxConcurrentOptimizes, coldToFrozenDir, coldtoFrozenScript


• Changing any of these attributes for existing indexes: repFactor, homePath, coldPath, thawedPath, bloomHomePath, summaryHomePath, tstatsHomePath


• Adding or removing a volume


• Enabling an index that contains data


• Removing an index 


You do not need to restart the indexer if you only make these changes: 

• Adding new index stanzas.


• Changing any attributes not listed as requiring restart


• Enabling or disabling an index that contains no data


The configuration changes that cause the peer nodes in an indexer cluster to undergo a rolling restart are a superset of those listed here. See Restart or reload after configuration bundle changes? 

Note: For information on other configuration changes, outside of indexes.conf, that require a restart, see When to restart Splunk Enterprise after a configuration file change in the Admin Manual. 



Use the monitoring console to view index and volume status

You can use the monitoring console to monitor most aspects of your deployment. This topic discusses the console dashboards that provide insight into indexing performance. 

The primary documentation for the monitoring console is located in Monitoring Splunk Enterprise. 

There are several dashboards that monitor the status of indexes and volumes. The dashboards are scoped either to a single instance or to the entire deployment. They are located under the Indexing menu: 

• Indexes and Volumes: Instance


• Indexes and Volumes: Deployment


• Index Detail: Instance


• Index Detail: Deployment


• Volume Detail: Instance


• Volume Detail: Deployment


These dashboards provide a wealth of information about your indexes and volumes, such as: 

• Disk usage by index


• Volume usage


• Index and volume size over time


• Data age


• Statistics for bucket types


• Bucket settings


View the dashboards themselves for more information. In addition, see "Indexing: Indexes and volumes" in Monitoring Splunk Enterprise. 
