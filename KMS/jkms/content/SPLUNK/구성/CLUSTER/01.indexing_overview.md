
Indexes, indexers, and indexer clusters

This manual discusses Splunk Enterprise data repositories and the Splunk Enterprise components that create and manage them. 

The index is the repository for Splunk Enterprise data. Splunk Enterprise transforms incoming data into events, which it stores in indexes. 

An indexer is a Splunk Enterprise instance that indexes data. For small deployments, a single instance might perform other Splunk Enterprise functions as well, such as data input and search management. In a larger, distributed deployment, however, the functions of data input and search management are allocated to other Splunk Enterprise components. This manual focuses exclusively on the indexing function, in the context of either a single-instance or a distributed deployment. 

An indexer cluster is a group of indexers configured to replicate each others' data, so that the system keeps multiple copies of all data. This process is known as index replication, or indexer clustering. By maintaining multiple, identical copies of data, clusters prevent data loss while promoting data availability for searching. 


Indexes

As Splunk Enterprise indexes your data, it creates a number of files. These files fall into two main categories: 

• The raw data in compressed form (rawdata)


• Indexes that point to the raw data (index files, also referred to as tsidx files), plus some metadata files 


Together, these files constitute the Splunk Enterprise index. The files reside in sets of directories organized by age. These directories are called buckets. See How Splunk Enterprise stores indexes. 

Splunk Enterprise manages its indexes to facilitate flexible searching and fast data retrieval, eventually archiving them according to a user-configurable schedule. Splunk Enterprise handles everything with flat files; it doesn't require any third-party database software running in the background. 

To start indexing, you simply specify the data inputs that you want Splunk Enterprise to index. You can add more inputs at any time, and Splunk Enterprise will begin indexing them as well. See What Splunk Enterprise can index in Getting Data In to learn how to add data inputs. 

Splunk Enterprise, by default, puts all user data into a single, preconfigured index. It also employs several other indexes for internal purposes. You can add new indexes and manage existing ones to meet your data requirements. See Manage indexes. 

Event processing

During indexing, Splunk Enterprise performs event processing. It processes incoming data to enable fast search and analysis, storing the results in the index as events. While indexing, Splunk Enterprise enhances the data in various ways, including by: 

• Separating the datastream into individual, searchable events.


• Creating or identifying timestamps.


• Extracting fields such as host, source, and sourcetype.


• Performing user-defined actions on the incoming data, such as identifying custom fields, masking sensitive data, writing new or modified keys, applying breaking rules for multi-line events, filtering unwanted events, and routing events to specified indexes or servers.


Getting Data In describes how to configure event processing to meet the needs of your data. See Overview of event processing. 

Index types

Splunk Enterprise supports two types of indexes: 

• Events indexes. Events indexes impose minimal structure and can accommodate any type of data, including metrics data. Events indexes are the default index type.


• Metrics indexes. Metrics indexes use a highly structured format to handle the higher volume and lower latency demands associated with metrics data. Putting metrics data into metrics indexes results in faster performance and less use of index storage, compared to putting the same data into events indexes. For information on the metrics format, see the Metrics manual.


There are minimal differences in how indexers process and manage the two index types. Despite its name, event processing occurs in the same sequence for both events and metrics indexes. Metrics data is really just a highly structured kind of event data. 


Indexers

The indexer is the Splunk Enterprise component that creates and manages indexes. The primary functions of an indexer are: 

• Indexing incoming data.


• Searching the indexed data.


In single-machine deployments consisting of just one Splunk Enterprise instance, the indexer also handles the data input and search management functions. This type of small deployment might handle the needs of a single department in an organization. 

For larger-scale needs, indexing is split out from the data input function and sometimes from the search management function as well. In these larger, distributed deployments, the Splunk Enterprise indexer might reside on its own machine and handle only indexing, along with searching of its indexed data. In those cases, other Splunk Enterprise components take over the non-indexing roles. Forwarders consume the data, indexers index and search the data, and search heads coordinate searches across the set of indexers. Here's an example of a scaled-out deployment: 


Horizontal scaling new2 60.png 

For more information on using indexers in a distributed deployment, see Indexers in a distributed deployment. 

A Splunk indexer is simply a Splunk Enterprise instance. To learn how to install a Splunk Enterprise instance, read the Installation Manual. 


Indexer clusters

An indexer cluster is a group of Splunk Enterprise nodes that, working in concert, provide a redundant indexing and searching capability. There are three types of nodes in a cluster: 

• A single master node to manage the cluster. The master node is a specialized type of indexer.


• Several peer nodes that handle the indexing function for the cluster, indexing and maintaining multiple copies of the data and running searches across the data.


• One or more search heads to coordinate searches across all the peer nodes.


Indexer clusters feature automatic failover from one peer node to the next. This means that, if one or more peers fail, incoming data continues to get indexed and indexed data continues to be searchable. 

The first part of this manual contains configuration and management information relevant for all indexers, independent of whether they are part of a cluster. The second part of this manual, starting with the topic About indexer clusters and index replication, is relevant only for clusters. 



How indexing works

Splunk Enterprise can index any type of time-series data (data with timestamps). When Splunk Enterprise indexes data, it breaks it into events, based on the timestamps. 

The indexing process follows the same sequence of steps for both events indexes and metrics indexes.


Event processing and the data pipeline

Data enters the indexer and proceeds through a pipeline where event processing occurs. Finally, the processed data it is written to disk. This pipeline consists of several shorter pipelines that are strung together. A single instance of this end-to-end data pipeline is called a pipeline set. 

Event processing occurs in two main stages, parsing and indexing. All data that comes into Splunk Enterprise enters through the parsing pipeline as large (10,000 bytes) chunks. During parsing, Splunk Enterprise breaks these chunks into events which it hands off to the indexing pipeline, where final processing occurs. 

While parsing, Splunk Enterprise performs a number of actions, including: 

• Extracting a set of default fields for each event, including host, source, and sourcetype.


• Configuring character set encoding.


• Identifying line termination using linebreaking rules. While many events are short and only take up a line or two, others can be long. 


• Identifying timestamps or creating them if they don't exist. At the same time that it processes timestamps, Splunk identifies event boundaries.


• Splunk can be set up to mask sensitive event data (such as credit card or social security numbers) at this stage. It can also be configured to apply custom metadata to incoming events.


In the indexing pipeline, Splunk Enterprise performs additional processing, including: 

• Breaking all events into segments that can then be searched upon. You can determine the level of segmentation, which affects indexing and searching speed, search capability, and efficiency of disk compression.


• Building the index data structures.


• Writing the raw data and index files to disk, where post-indexing compression occurs.


Note: The term "indexing" is also used in a more general sense to refer to the entirety of event processing, encompassing both the parsing pipeline and the indexing pipeline. The differentation between the parsing and indexing pipelines is of relevance mainly when deploying heavy forwarders. 

Heavy forwarders can run raw data through the parsing pipeline and then forward the parsed data on to indexers for final indexing. Universal forwarders do not parse data in this way. Instead, universal forwarders forward the raw data to the indexer, which then processes it through both pipelines. 

Note that both types of forwarders do perform a type of parsing on certain structured data. See Extract data from files with headers in Getting Data In. 

For more information about events and how the indexer transforms data into events, see the chapter Configure event processing in Getting Data In. 

This diagram shows the main processes inherent in indexing: 

Datapipeline 60.png 

Note: This diagram represents a simplified view of the indexing architecture. It provides a functional view of the architecture and does not fully describe Splunk Enterprise internals. In particular, the parsing pipeline actually consists of three pipelines: parsing, merging, and typing, which together handle the parsing function. The distinction can matter during troubleshooting, but does not generally affect how you configure or deploy Splunk Enterprise. 

For a more detailed discussion of the data pipeline and how it affects deployment decisions, see How data moves through Splunk Enterprise: the data pipeline in Distributed Deployment. 


What's in an index?

Splunk Enterprise stores all of the data it processes in indexes. An index is a collection of databases, which are subdirectories located in $SPLUNK_HOME/var/lib/splunk. Indexes consist of two types of files: rawdata files and index files. See How Splunk Enterprise stores indexes. 


Default set of indexes

Splunk Enterprise comes with a number of preconfigured indexes, including: 

• main: This is the default Splunk Enterprise index. All processed data is stored here unless otherwise specified.


• _internal: Stores Splunk Enterprise internal logs and processing metrics.


• _audit: Contains events related to the file system change monitor, auditing, and all user search history.


A Splunk Enterprise administrator can create new indexes, edit index properties, remove unwanted indexes, and relocate existing indexes. Splunk Enterprise administrators manage indexes through Splunk Web, the CLI, and configuration files such as indexes.conf. See Managing indexes. 


Index time versus search time

Splunk Enterprise documentation contains references to the terms "index time" and "search time". These terms distinguish between the types of processing that occur during indexing, and the types that occur when a search is run. 

It is important to consider this distinction when administering Splunk Enterprise. For example, say that you want to use custom source types and hosts. You should define those custom source types and hosts before you start indexing, so that the indexing process can tag events with them. After indexing, you cannot change the host or source type assignments. 

If you neglect to create the custom source types and hosts until after you have begun to index data, your choice is either to re-index the data, in order to apply the custom source types and hosts to the existing data, as well as to new data, or, alternatively, to manage the issue at search time by tagging the events with alternate values. 

Conversely, as a general rule, it is better to perform most knowledge-building activities, such as field extraction, at search time. Index-time custom field extraction can degrade performance at both index time and search time. When you add to the number of fields extracted during indexing, the indexing process slows. Later, searches on the index are also slower, because the index has been enlarged by the additional fields, and a search on a larger index takes longer. 

You can avoid such performance issues by instead relying on search-time field extraction. For details on search-time field extraction, see About fields and When Splunk Enterprise extracts fields in the Knowledge Manager Manual. 


At index time

Index-time processes take place between the point when the data is consumed and the point when it is written to disk. 

The following processes occur during index time: 

• Default field extraction (such as host, source, sourcetype, and timestamp)


• Static or dynamic host assignment for specific inputs


• Default host assignment overrides


• Source type customization


• Custom index-time field extraction


• Structured data field extraction


• Event timestamping


• Event linebreaking


• Event segmentation (also happens at search time)



At search time

Search-time processes take place while a search is run, as events are collected by the search. The following processes occur at search time: 

• Event segmentation (also happens at index time)


• Event type matching


• Search-time field extraction (automatic and custom field extractions, including multivalue fields and calculated fields)


• Field aliasing


• Addition of fields from lookups


• Source type renaming


• Tagging



The data pipeline

The data pipeline provides a more detailed way to think about the progression of data through the system. The data pipeline is particularly useful for understanding how to assign configurations and work across a distributed deployment. See How data moves through Splunk: the data pipeline in Distributed Deployment. 



Install an indexer

All full Splunk Enterprise instances serve as indexers by default. To learn how to install a Splunk Enterprise instance, read the Installation Manual. Then return to this manual to learn how to configure the indexer. 

If you plan to deploy an indexer in a distributed deployment, next read the topic, Indexers in a distributed environment. 



Indexers in a distributed deployment

Important: To better understand this topic, you should be familiar with Splunk Enterprise distributed environments, covered in Distributed Deployment. 

The indexer is the Splunk Enterprise component that creates and manages indexes. The primary functions of an indexer are: 

• Indexing incoming data.


• Searching the indexed data.


In single-machine deployments consisting of just one Splunk Enterprise instance, the indexer also handles the data input and search management functions. 

For larger-scale needs, indexing is split out from the data input function and sometimes from the search management function as well. In these larger, distributed deployments, the indexer might reside on its own machine and handle only indexing, along with searching of its indexed data. In those cases, other Splunk Enterprise components take over the non-indexing roles. 

For instance, you might have a set of Windows and Linux machines generating events, which need to go to a central indexer for consolidation. Usually the best way to do this is to install a lightweight instance of Splunk Enterprise, known as a forwarder, on each of the event-generating machines. These forwarders handle data input and send the data across the network to the indexer residing on its own machine. 

Similarly, in cases where you have a large amount of indexed data and numerous concurrent users searching on it, it can make sense to split off the search management function from indexing. In this type of scenario, known as distributed search, one or more search heads distribute search requests across multiple indexers. The indexers still perform the actual searching of their own indexes, but the search heads manage the overall search process across all the indexers and present the consolidated search results to the user. 

Here is an example of a scaled-out deployment: 

Horizontal scaling new2 60.png 


 While the fundamental issues of indexing and event processing remain the same for distributed deployments, it is important to take into account deployment needs when planning your indexing strategy. 


Forward data to an indexer

To forward remote data to an indexer, you use forwarders, which are Splunk Enterprise instances that receive data inputs and then consolidate and send the data to a Splunk Enterprise indexer. Forwarders come in two flavors: 

•Universal forwarders. These maintain a small footprint on their host machine. They perform minimal processing on the incoming data streams before forwarding them on to an indexer, also known as the receiver.


•Heavy forwarders. These retain most of the functionality of a full Splunk Enterprise instance. They can parse data before forwarding it to the receiving indexer. (See How indexing works for the distinction between parsing and indexing.) They can store indexed data locally and also forward the parsed data to a receiver for final indexing on that machine as well. 


Both types of forwarders tag data with metadata such as host, source, and source type, before forwarding it on to the indexer. 

Forwarders allow you to use resources efficiently when processing large quantities or disparate types of data coming from remote sources. They also enable a number of interesting deployment topologies, by offering capabilities for load balancing, data filtering, and routing. 

For an extended discussion of forwarders, including configuration and detailed use cases, read Forwarding Data. 


Search across multiple indexers

In distributed search, search heads send search requests to indexers and then merge the results back to the user. This is useful for a number of purposes, including horizontal scaling, access control, and managing geo-dispersed data. 

For an extended discussion of distributed search and search heads, including configuration and detailed use cases, see Distributed Search. 

Indexer clusters also use search heads to coordinate searches across the cluster's peer nodes. See About indexer clusters and index replication. 


Deploy indexers in a distributed environment

To implement a distributed environment similar to the diagram earlier in this topic, you need to install and configure three types of components: 

• Indexers


• Forwarders (typically, universal forwarders)


• Search head(s)


Install and configure the indexers

By default, all full Splunk Enterprise instances serve as indexers. For horizontal scaling, you can install multiple indexers on separate machines. 

To learn how to install a Splunk Enterprise instance, read the Installation Manual. 

Then return to this manual for information on configuring each individual indexer to meet the needs of your specific deployment. 

Install and configure the forwarders

A typical distributed deployment has a large number of forwarders feeding data to a few indexers. For most forwarding purposes, the universal forwarder is the best choice. The universal forwarder is a separate downloadable from the full Splunk Enterprise instance. 

To learn how to install and configure forwarders, read Forwarding Data. 

Install and configure the search head(s)

You can install one or more search heads to handle your distributed search needs. Search heads are just full Splunk Enterprise instances that have been specially configured. 

To learn how to configure a search head, read Distributed Search. 

Other deployment tasks

You need to configure Splunk Enterprise licensing by designating a license master. See the chapter Configure Splunk Enterprise licenses in the Admin Manual for more information. 

You can use the Splunk Enterprise deployment server to simplify the job of updating the deployment components. For details on how to configure a deployment server, see Updating Splunk Enterprise Instances. 

Install a cluster of indexers

If data availability, data fidelity, and data recovery are key issues for your deployment, then you should consider deploying an indexer cluster, rather than a series of individual indexers. For further information, see About indexer clusters and index replication. 