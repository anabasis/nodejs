# Get metrics data in

## Get metrics in from StatsD

StatsD is a network daemon that runs on the Node.js platform, sending metrics over UDP or TCP. For an overview of StatsD, see Measure Anything, Measure Everything on the Code as Craft website.

StatsD는 Node.js 플랫폼에서 실행되는 네트워크 데몬으로 UDP 또는 TCP를 통해 메트릭을 보냅니다. StatsD에 대한 개요는 Code as Craft 웹 사이트에서 무엇이든 측정, 모두 측정을 참조하십시오.

StatsD has several implementations, some of which encode dimensions in different ways. The Splunk platform supports the following formats natively:

StatsD에는 여러 가지 구현이 있으며 그 중 일부는 다른 방식으로 차원을 인코딩합니다. Splunk 플랫폼은 기본적으로 다음 형식을 지원합니다.

- Basic StatsD data line metric protocol, which includes metric_name, _value and metric_type.
- Expanded StatsD data line metric protocol, which adds sample rate and dimensions.

Splunk supports two metric_type values for StatsD metric data points: g, for gauge metrics, and c, for counter metrics.

- stat_name, _value 및 metric_type을 포함하는 기본 StatsD 데이터 라인 메트릭 프로토콜.
- 샘플링 속도 및 차원을 추가하는 확장 된 StatsD 데이터 라인 메트릭 프로토콜.

Splunk는 StatsD 메트릭 데이터 포인트에 대해 g(게이지 메트릭) 및 c(카운터 메트릭)의 두 가지 metric_type 값을 지원합니다.

### Basic StatsD metric protocol

The basic StatsD data line metric protocol just has three fields: the metric_name, the metric _value, and the metric_type.

기본 StatsD 데이터 라인 메트릭 프로토콜에는 metric_name, metric _value 및 metric_type의 세 가지 필드 만 있습니다.

Syntax

`<metric_name>:<_value>|<metric_type>`

Example metric

`performance.os.disk:1099511627776|g`

### Expanded StatsD metric protocol

The expanded StatsD data line metric protocol supports dimensions and a sample rate. Sample rates only apply to counter metrics, meaning they have a metric_type of c. For

확장 된 StatsD 데이터 라인 메트릭 프로토콜은 차원 및 샘플 속도를 지원합니다. 샘플 속도는 카운터 메트릭에만 적용됩니다. 즉, metric_type이 c입니다. 에 대한

For more about formats for metric names and dimensions, see Best practices for metrics.

메트릭 이름 및 차원의 형식에 대한 자세한 내용은 메트릭 모범 사례를 참조하십시오.

#### Syntax

`<metric_name>:<_value>|<metric_type>|@<sample_rate>|#dim1:valueX,dim2:valueY`

##### Example gauge metric

A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. For example, you can use a gauge to represent the number of currently running search jobs, or the temperature in your server room.

게이지는 임의로 위아래로 이동할 수있는 단일 숫자 값을 나타내는 메트릭입니다. 예를 들어 게이지를 사용하여 현재 실행중인 검색 작업 수 또는 서버 실의 온도를 나타낼 수 있습니다.

`performance.os.disk:1099511627776|g|#region:us-west-1,datacenter:us-west-1a,rack:63,os:Ubuntu16.10,arch:x64,team:LON,service:6,service_version:0,service_environment:test, path:/dev/sdal,fstype:ext3`

#### Example counter metric, after processing by the Splunk platform

A counter metric counts occurrences of an event. Its value can only increase or be reset to zero. For example, you can use a counter to represent a number of requests served, tasks completed, or errors. For more information about counter metrics, see Investigate counter metrics.

카운터 메트릭은 이벤트 발생을 계산합니다. 이 값은 0으로 만 증가 시키거나 재설정 될 수 있습니다. 예를 들어 카운터를 사용하여 처리 된 많은 요청, 완료된 작업 또는 오류를 나타낼 수 있습니다. 카운터 메트릭에 대한 자세한 내용은 카운터 메트릭 조사를 참조하십시오.

Here is an example of an counter metric that has been processed by the Splunk platform.

다음은 Splunk 플랫폼에서 처리 한 카운터 메트릭의 예입니다.

`event.login:6|c|@0.5|#region:west,dc:west-1,ip:10.1.1.1,host:valis1.buttercupgames.com,app:zoolu`

>> Note that this counter metric has a sample rate of 0.5. This means that this counter metric is sampled only 50% of the time by the StatsD client. The Splunk platform adjusts for this by multiplying the metric value by 1/0.5, or 2. This means that the original metric sent from the StatsD client looked like this:
>> 이 카운터 메트릭의 샘플 속도는 0.5입니다. 이는이 카운터 메트릭이 StatsD 클라이언트에 의해 시간의 50 % 만 샘플링됨을 의미합니다. Splunk 플랫폼은 메트릭 값에 1 / 0.5 또는 2를 곱하여이를 조정합니다. 이는 StatsD 클라이언트에서 전송 된 원래 메트릭이 다음과 같음을 나타냅니다.

`event.login:3|c|@0.5|#region:west,dc:west-1,ip:10.1.1.1,host:valis1.buttercupgames.com,app:zoolu`

>> Note that the original metric event had a numeric value of 3.
원래 메트릭 이벤트의 숫자 값은 3입니다.

#### About the sample rate

When large numbers of data points are being produced for a particular counter metric, it can be expensive for the Splunk platform to aggregate them. The StatsD client manages this by implementing a sample rate to reduce the network traffic that it sends to the Splunk platform.

특정 카운터 메트릭에 대해 많은 수의 데이터 포인트가 생성되는 경우 Splunk 플랫폼이 집계하는 데 비용이 많이들 수 있습니다. StatsD 클라이언트는 Splunk 플랫폼으로 보내는 네트워크 트래픽을 줄이기 위해 샘플 속도를 구현하여이를 관리합니다.

The StatsD client puts the sample_rate value in the counter metric data point to indicate to the Splunk platform the actual downsampling percentage that it employed. The Splunk platform responds to this by multiplying the value of a downsampled counter metric by 1/<sample_rate>.

StatsD 클라이언트는 sample_rate 값을 카운터 메트릭 데이터 포인트에 넣어 Splunk 플랫폼에 실제 다운 샘플링 백분율을 표시합니다. Splunk 플랫폼은 다운 샘플링 된 카운터 메트릭의 값에 1/<sample_rate>를 곱하여 이에 응답합니다.

For example, say you have a counter metric named event.login with a sample_rate of 0.1. This means that only 10% of the event.login data points are passed from the StatsD client to your Splunk platform implementation. The Splunk platform multiplies the event.login values by 1/0.1, or 10, to adjust for the missed data points. So if your Splunk platform implementation receives a event.login data point with a value of 2, it will change that value to 20.

예를 들어, sample_rate가 0.1 인 event.login이라는 카운터 메트릭이 있다고 가정하십시오. 이는 event.login 데이터 포인트의 10% 만 StatsD 클라이언트에서 Splunk 플랫폼 구현으로 전달됨을 의미합니다. Splunk 플랫폼은 event.login 값에 1/0.1 또는 10을 곱하여 누락 된 데이터 포인트를 조정합니다. 따라서 Splunk 플랫폼 구현에서 값이 2 인 event.login 데이터 포인트를 수신하면 해당 값이 20으로 변경됩니다.

The Splunk platform passes an warning message for sample_rate values that are not within 0 and 1. The default setting for sample_rate is 1.

Splunk 플랫폼은 0과 1이 아닌 sample_rate 값에 대한 경고 메시지를 전달합니다. sample_rate의 기본 설정은 1입니다.

### Using other StatsD formats

If you use a StatsD implementation that uses a different format for dimensions from the ones that the Splunk platform supports natively, for example, one that embeds dimensions within the metric name, you can still use metrics in the Splunk platform. However, you'll need to customize Splunk configuration files to specify how to extract dimensions from your format.

Splunk 플랫폼이 기본적으로 지원하는 것과 다른 형식 (예:메트릭 이름 내에 차원을 포함하는 형식)을 사용하는 StatsD 구현을 사용하는 경우에도 Splunk 플랫폼에서 메트릭을 사용할 수 있습니다. 그러나 Splunk 구성 파일을 사용자 지정하여 형식에서 차원을 추출하는 방법을 지정해야합니다.

Another option is to use StatsD to gather metrics, but use collectd to send the data to the Splunk platform over HTTP. The benefit of this method is that collectd normalizes the dimension format in the metrics data. For more, see Get metrics in from collectd.

또 다른 옵션은 StatsD를 사용하여 메트릭을 수집하지만 collectd를 사용하여 HTTP를 통해 Splunk 플랫폼으로 데이터를 보내는 것입니다. 이 방법의 장점은 수집하면 메트릭 데이터에서 차원 형식을 정규화 한다는 것입니다. 자세한 내용은 collectd 지표 가져 오기를 참조하십시오.

### Set up a data input for StatsD data

After you configure your data source to send data in the StatsD protocol, create a UDP or TCP data input in the Splunk platform to listen for StatsD data on an open port.

StatsD 프로토콜로 데이터를 보내도록 데이터 소스를 구성한 후 Splunk 플랫폼에서 UDP 또는 TCP 데이터 입력을 만들어 열린 포트에서 StatsD 데이터를 수신합니다.

1. In Splunk Web, go to Settings > Data inputs.
2. Under Local inputs, click Add new next to UDP or TCP, depending on the type of input you want to create.
When using UDP ports to ingest metric data, you cannot use parallel ingestion or the multiple pipeline sets feature.

3. For Port, enter the number of the port you are using for StatsD.
4. Click Next.
5. Click Select Source Type, then select Metrics > statsd.
6. For Index, select an existing metrics index. Or, click Create a new index to create one.
If you choose to create an index, in the New Index dialog box:
Enter an Index Name. User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. Index names cannot begin with an underscore or hyphen.
For Index Data Type, click Metrics.
Configure additional index properties as needed.
Click Save.
7. Click Review, then click Submit.

1. Splunk Web에서 설정> 데이터 입력으로 이동합니다.
2. 로컬 입력에서 만들려는 입력 유형에 따라 UDP 또는 TCP 옆에있는 새로 추가를 클릭합니다.
UDP 포트를 사용하여 메트릭 데이터를 수집하는 경우 병렬 수집 또는 다중 파이프 라인 세트 기능을 사용할 수 없습니다.

3. 포트에 StatsD에 사용중인 포트 번호를 입력하십시오.
4. 다음을 클릭하십시오.
5. 소스 유형 선택을 클릭 한 후 메트릭> statsd를 선택하십시오.
6. 색인에서 기존 메트릭 색인을 선택하십시오. 또는 새 색인 작성을 클릭하여 색인을 작성하십시오.
색인 작성을 선택한 경우 새 색인 대화 상자에서 다음을 수행하십시오.
색인 이름을 입력하십시오. 사용자 정의 색인 이름은 숫자, 소문자, 밑줄 및 하이픈으로 만 구성되어야합니다. 색인 이름은 밑줄이나 하이픈으로 시작할 수 없습니다.
인덱스 데이터 유형의 경우 메트릭을 클릭하십시오.
필요에 따라 추가 인덱스 속성을 구성하십시오.
저장을 클릭하십시오.
7. 검토를 클릭 한 후 제출을 클릭하십시오.

## Extract dimensions for unsupported StatsD formats

Many StatsD clients embed dimension names in the metric name. For example, let's say your StatsD client uses the following line metric protocol format, which is not supported natively by the Splunk platform:

많은 StatsD 클라이언트는 메트릭 이름에 차원 이름을 포함시킵니다. 예를 들어 StatsD 클라이언트가 Splunk 플랫폼에서 기본적으로 지원하지 않는 다음 라인 메트릭 프로토콜 형식을 사용한다고 가정합니다.

`<dimension>.<metric_name>:<value>|<metric_type>`

Here's an example of a metric returned using this unsupported format:

이 지원되지 않는 형식을 사용하여 반환 된 측정 항목의 예는 다음과 같습니다.

`10.1.1.198.cpu.percent:75|g`

The extracted measurement should be:

`metric_name:cpu.percent=75`

The extracted dimension should be:

`ip=10.1.1.198`

To create the correct results, you must edit Splunk configuration files or use the REST API to create a custom source type that specifies how to extract dimensions from this metrics data.

올바른 결과를 만들려면 Splunk 구성 파일을 편집하거나 REST API를 사용하여이 메트릭 데이터에서 차원을 추출하는 방법을 지정하는 사용자 지정 소스 유형을 만들어야합니다.

### Configure dimension extraction by editing configuration files

1. Define a custom source type for your StatsD metrics data.
In a text editor, open the props.conf configuration file from the local directory for the location you want to use, such as the Search & Reporting app ($SPLUNK_HOME/etc/apps/search/local/) or from the system ($SPLUNK_HOME/etc/system/local). If a props.conf file does not exist in this location, create a text file and save it to that location.
Append a stanza to the props.conf file as follows:

1. StatsD 메트릭 데이터에 대한 사용자 정의 소스 유형을 정의하십시오.
텍스트 편집기에서 사용하려는 위치의 로컬 디렉토리 (예 : Search & Reporting 앱 ($ SPLUNK_HOME / etc / apps / search / local /) 또는 시스템 ($))에서 props.conf 구성 파일을여십시오. SPLUNK_HOME / etc / system / local). props.conf 파일이이 위치에 없으면 텍스트 파일을 만들어 해당 위치에 저장하십시오.
다음과 같이 props.conf 파일에 스탠자를 추가하십시오.

```properties
# props.conf
[<metrics_sourcetype_name>]
METRICS_PROTOCOL = statsd
STATSD-DIM-TRANSFORMS = <statsd_dim_stanza_name1>,<statsd_dim_stanza_name2>...
```

metrics_sourcetype_name: The name of your custom metrics source type.
statsd_dim_stanza_name: A comma-separated list of transforms stanza names that specify how to extract dimensions. If only one stanza is used for the source type, and if the transforms stanza name is same as the metrics_sourcetype_name, this STATSD-DIM-TRANSFORMS setting can be omitted.

metrics_sourcetype_name : 맞춤 측정 항목 소스 유형의 이름입니다.
statsd_dim_stanza_name : 차원을 추출하는 방법을 지정하는 쉼표로 구분 된 변환 스탠자 이름 목록입니다. 소스 유형에 스탠자를 하나만 사용하고 변환 스탠자 이름이 metrics_sourcetype_name과 동일한 경우이 STATSD-DIM-TRANSFORMS 설정을 생략 할 수 있습니다.

2. Define one or more regular expressions to extract the dimensions from metric_name.
In a text editor, open the transforms.conf configuration file from the local directory for the location you want to use, such as the Search & Reporting app ($SPLUNK_HOME/etc/apps/search/local/) or from the system ($SPLUNK_HOME/etc/system/local). If a transforms.conf file does not exist in this location, create a text file and save it to that location.
Append a stanza for each regular expression as follows:

2. metric_name에서 차원을 추출 할 하나 이상의 정규식을 정의하십시오.
텍스트 편집기에서 사용하려는 위치의 로컬 디렉토리 (예 : Search & Reporting 앱 ($ SPLUNK_HOME / etc / apps / search / local /) 또는 시스템 ($))에서 transforms.conf 구성 파일을여십시오. SPLUNK_HOME / etc / system / local). 이 위치에 transforms.conf 파일이 없으면 텍스트 파일을 만들어 해당 위치에 저장하십시오.
다음과 같이 각 정규식에 대한 스탠자를 추가하십시오.

```properties
# transforms.conf
[statsd-dims:<unique_transforms_stanza_name>]
REGEX = <regular expression>
REMOVE_DIMS_FROM_METRIC_NAME = <Boolean>
```

`unique_transforms_stanza_name`: A unique name for this stanza.
`REGEX = <regular expression>`: A regular expression that defines how to match and extract dimensions from StatsD metrics data. The Splunk platform supports a named capturing-group extraction format (?<dim1>group)(?<dim2>group)... to provide dimension names for the corresponding values that are extracted.
`REMOVE_DIMS_FROM_METRIC_NAME = <Boolean>`: Specifies whether unmatched segments of the StatsD dotted name segment are used as the metric_name.
When true, dimension values are removed from the measurement and the unmatched portion becomes the metric_name. The default value is true.

unique_transforms_stanza_name :이 스탠자의 고유 이름입니다.
REGEX = <일반 표현식> : StatsD 메트릭 데이터에서 차원을 일치시키고 추출하는 방법을 정의하는 정규 표현식입니다. Splunk 플랫폼은 명명 된 캡처 그룹 추출 형식 (? <dim1> group) (? <dim2> group) ...을 지원하여 추출되는 해당 값의 차원 이름을 제공합니다.
REMOVE_DIMS_FROM_METRIC_NAME = <Boolean> : StatsD 점으로 구분 된 이름 세그먼트의 일치하지 않는 세그먼트가 metric_name으로 사용되는지 여부를 지정합니다.
true 인 경우 측정에서 치수 값이 제거되고 일치하지 않는 부분이 metric_name이됩니다. 기본값은 true입니다.

When false, extracted dimension values are included in the metric_name.
false 인 경우 추출 된 차원 값이 metric_name에 포함됩니다.

For example, a metric measurement name is "x.y.z". The regular expression matches "y" and "z". When REMOVE_DIMS_FROM_METRIC_NAME is true, metric_name is "x". When false, metric_name is "x.y.z".

예를 들어 메트릭 측정 이름은 "x.y.z"입니다. 정규식은 "y"및 "z"와 일치합니다. REMOVE_DIMS_FROM_METRIC_NAME이 true 인 경우 metric_name은 "x"입니다. false 인 경우 metric_name은 "x.y.z"입니다.

3. Create a data input for this source type as described in Set up a data input for StatsD data, and select your custom source type.

3. StatsD 데이터에 대한 데이터 입력 설정에 설명 된대로이 소스 유형에 대한 데이터 입력을 작성하고 사용자 정의 소스 유형을 선택하십시오.

For more about editing these configuration files, see About configuration files, props.conf, and transforms.conf in the Admin Manual.

이러한 구성 파일 편집에 대한 자세한 내용은 관리자 매뉴얼의 구성 파일, props.conf 및 transforms.conf를 참조하십시오.

### Examples of configuring dimension extraction

Let's say you have StatsD metrics data such as:

다음과 같은 StatsD 메트릭 데이터가 있다고 가정합니다.

data=mem.percent.used.10.2.3.4.windows:33|g
You need to extract the "ipv4" and "os" dimensions.

If you defined two regular expressions, one for "ipv4" and one for "os", you would append the following stanzas to your configuration files:
"ipv4"와 "os"에 대해 두 개의 정규식을 정의한 경우 다음 스탠자를 구성 파일에 추가합니다.

```properties
# props.conf.example
[my_custom_metrics_sourcetype]
METRICS_PROTOCOL = statsd
STATSD-DIM-TRANSFORMS = regex_stanza1, regex_stanza2
```

```properties
# transforms.conf.example
[statsd-dims:regex_stanza1]
REGEX = (?<ipv4>\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3})
REMOVE_DIMS_FROM_METRIC_NAME = true
[statsd-dims:regex_stanza2]
REGEX = \S+\.(?<os>\w+):
REMOVE_DIMS_FROM_METRIC_NAME = true
```

Now let's say you can accomplish this same extraction using a single regular expression. In this case, you would append the following stanzas to your configuration files:

이제 단일 정규식을 사용하여 동일한 추출을 수행 할 수 있다고 가정하겠습니다. 이 경우 다음 스탠자를 구성 파일에 추가합니다.

```properties
# props.conf.example
[my_custom_metrics_sourcetype]
METRICS_PROTOCOL = statsd
```

```properties
# transforms.conf.example
[statsd-dims:my_custom_metrics_sourcetype]
REGEX = (?<ipv4>\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3})\.(?<os>\w+):
REMOVE_DIMS_FROM_METRIC_NAME = true
```

Notice that the STATSD-DIM-TRANSFORMS setting in the props.conf configuration file is not needed when only a single regular expression is used for a source type.

props.conf 구성 파일의 STATSD-DIM-TRANSFORMS 설정은 소스 유형에 단일 정규식 만 사용되는 경우 필요하지 않습니다.

### Configure dimension extraction for StatsD by using the REST API

1. Define a custom source type for your StatsD metrics data by using the /services/saved/sourcetypes REST endpoint:

1. / services / saved / sourcetypes REST 엔드 포인트를 사용하여 StatsD 메트릭 데이터에 대한 사용자 정의 소스 유형을 정의하십시오.

```bash
https://<host>:<mPort>/services/saved/sourcetypes   \
-d "name=<metrics_sourcetype_name>&METRICS_PROTOCOL=statsd&STATSD-DIM-TRANSFORMS=<statsd_dim_stanza_name>&SHOULD_LINEMERGE=false&ANNOTATE_PUNCT=false&ADD_EXTRA_TIME_FIELDS=false&DATETIME_CONFIG=CURRENT&pulldown_type=true&category=Metrics"
```

- metrics_sourcetype_name: The name of your custom metrics source type.
- statsd_dim_stanza_name: A list of transforms stanza names that specify how to extract dimensions. If only one stanza is used for the source type, and if the transforms stanza name is same as the metrics_sourcetype_name, this STATSD-DIM-TRANSFORMS setting can be omitted.

- metrics_sourcetype_name : 맞춤 측정 항목 소스 유형의 이름입니다.
- statsd_dim_stanza_name : 차원을 추출하는 방법을 지정하는 변환 스탠자 이름의 목록입니다. 소스 유형에 스탠자를 하나만 사용하고 변환 스탠자 이름이 metrics_sourcetype_name과 동일한 경우이 STATSD-DIM-TRANSFORMS 설정을 생략 할 수 있습니다.

For example, enter the following command:

```bash
curl -k -u admin:changeme https://localhost:8089/services/saved/sourcetypes   \
-d "name=statsd_custom&METRICS_PROTOCOL=statsd&STATSD-DIM-TRANSFORMS=statsd-ex&SHOULD_LINEMERGE=false&ANNOTATE_PUNCT=false&ADD_EXTRA_TIME_FIELDS=false&DATETIME_CONFIG=CURRENT&pulldown_type=true&category=Metrics"
```

2. Create one or more regular expressions to extract the dimensions from metric_name by using the /data/transforms/statsdextractions REST endpoint:

2. / data / transforms / statsdextractions REST 엔드 포인트를 사용하여 하나 이상의 정규식을 작성하여 metric_name에서 차원을 추출하십시오.

```properties
https://<host>:<mPort>/services/data/transforms/statsdextractions \
-d "name=<unique_transforms_stanza_name>&REGEX=<regular expression>&REMOVE_DIMS_FROM_METRIC_NAME=<Boolean>"
```

- unique_tran-sforms_stanza_name: A unique name for this stanza.
- REGEX = <regular expression>: A regular expression that defines how to match and extract dimensions from StatsD metrics data. The Splunk platform supports a named capturing-group extraction format (?<dim1>group)(?<dim2>group)... to provide dimension names for the corresponding values that are extracted.
- REMOVE_DIMS_FROM_METRIC_NAME = <Boolean>: Specifies whether unmatched segments of the StatsD dotted name segment are used as the metric_name.
When true, dimension values are be removed from the measurement and the unmatched portion becomes the metric_name. The default value is true.

- unique_tran-sforms_stanza_name :이 스탠자의 고유 이름입니다.
- REGEX = <정규 표현식> : StatsD 메트릭 데이터에서 차원을 일치시키고 추출하는 방법을 정의하는 정규 표현식입니다. Splunk 플랫폼은 명명 된 캡처 그룹 추출 형식 (? <dim1> group) (? <dim2> group) ...을 지원하여 추출되는 해당 값의 차원 이름을 제공합니다.
- REMOVE_DIMS_FROM_METRIC_NAME = <Boolean> : StatsD 점으로 구분 된 이름 세그먼트의 일치하지 않는 세그먼트를 metric_name으로 사용할지 여부를 지정합니다.
true 인 경우 측정에서 치수 값이 제거되고 일치하지 않는 부분이 metric_name이됩니다. 기본값은 true입니다.

When false, extracted dimension values are included in the metric_name.
false 인 경우 추출 된 차원 값이 metric_name에 포함됩니다.

For example, a metric measurement name is "x.y.z". The regular expression matches "y" and "z". When REMOVE_DIMS_FROM_METRIC_NAME is true, metric_name is "x". When false, metric_name is "x.y.z".

예를 들어 메트릭 측정 이름은 "x.y.z"입니다. 정규식은 "y"및 "z"와 일치합니다. REMOVE_DIMS_FROM_METRIC_NAME이 true 인 경우 metric_name은 "x"입니다. false 인 경우 metric_name은 "x.y.z"입니다.

For example, enter the following command:

```properties
curl -k -u admin:changeme https://localhost:8089/services/data/transforms/statsdextractions \
-d "name=statsd-ex&REGEX=\.(?<hostname>\S%2B?)\.(?<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})&REMOVE_DIMS_FROM_METRIC_NAME=true"
```

3. Reload the metrics processor to load the configuration changes by using the /admin/metrics-reload/_reload REST endpoint:

3. / admin / metrics-reload / _reload REST 엔드 포인트를 사용하여 구성 변경 사항을로드하려면 메트릭 프로세서를 다시로드하십시오.

```properties
https://<host>:<mPort>/services/admin/metrics-reload/_reload
```

For example, enter the following command:

```properties
curl -k -u admin:changeme \
https://localhost:8089/services/admin/metrics-reload/_reload
```

4. Create a data input for this source type as described in Set up a data input for StatsD data, and select your custom source type.

4. StatsD 데이터에 대한 데이터 입력 설정에 설명된대로 이 소스 유형에 대한 데이터 입력을 작성하고 사용자 정의 소스 유형을 선택하십시오.

For more about using the Splunk REST API, see Using the REST API reference, /data/transforms/statsdextractions, and /admin/metrics-reload/_reload in the REST API Reference Manual.

Splunk REST API 사용에 대한 자세한 내용은 REST API 참조 설명서의 REST API 참조 사용, /data/transforms/statsdextractions 및 /admin/metrics-reload/_reload를 참조하십시오.

## Get metrics in from collectd

Collectd is an open source daemon that collects performance metrics from a variety of sources. Using the collectd write_http plugin, collectd sends metrics data to a data input in the Splunk platform using the HTTP Event Collector (HEC).

Collectd는 다양한 소스에서 성능 지표를 수집하는 오픈 소스 데몬입니다. collectd write_http 플러그인을 사용하여 collectd는 HTTP 이벤트 콜렉터 (HEC)를 사용하여 Splunk 플랫폼의 데이터 입력으로 메트릭 데이터를 보냅니다.

To send metrics using collectd, do the following:

collectd 것을 사용하여 메트릭을 보내려면 다음을 수행하십시오.

1. Configure the HTTP Event Collector (HEC) data input.
2. Install collectd.
3. Configure collectd.
4. Start collectd.

1. HTTP 이벤트 콜렉터 (HEC) 데이터 입력을 구성하십시오.
2. collectd 설치.
3. 수집을 구성하십시오.
4. 수집을 시작하십시오.

### Configure the HTTP Event Collector (HEC) data input

The HTTP Event Collector (HEC) is an endpoint that lets you send application events to your deployment of the Splunk platform using the HTTP or Secure HTTP (HTTPS) protocols. Configure this data input before setting up collectd because you'll need to use data input details for the collectd configuration.

HEC (HTTP Event Collector)는 HTTP 또는 HTTP (Secure HTTP) 프로토콜을 사용하여 Splunk 플랫폼 배포에 응용 프로그램 이벤트를 보낼 수있는 엔드 포인트입니다. collectd 구성에 데이터 입력 세부 사항을 사용해야하므로 수집을 설정하기 전에이 데이터 입력을 구성하십시오.

1. In Splunk Web, click Settings > Data Inputs.
2. Under Local Inputs, click HTTP Event Collector.
3. Verify that HEC is enabled.
a Click Global Settings.
b For All Tokens, click Enabled if this button is not already selected.
c Note the value for HTTP Port Number, which you'll need to configure collectd.
d Click Save.
4. Configure an HEC token for sending data by clicking New Token.
5. On the Select Source page, for Name, enter a token name, for example "collectd token".
6. Leave the other options blank or unselected.
7. Click Next.
8. On the Input Settings page, for Source type, click Select.
9. Click Select Source Type, then select Metrics > collectd_http.
10. Next to Default Index, select your metrics index, or click Create a new index to create one.
If you choose to create an index, in the New Index dialog box:
Enter an Index Name. User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. Index names cannot begin with an underscore or hyphen.
For Index Data Type, click Metrics.
Configure additional index properties as needed.
Click Save.
11. Click Review, and then click Submit.
12. Copy the Token Value that is displayed, which you'll need to configure collectd.

1. Splunk Web에서 설정> 데이터 입력을 클릭하십시오.
2. 로컬 입력에서 HTTP 이벤트 콜렉터를 클릭하십시오.
3. HEC가 사용 가능한지 확인하십시오.
a 전역 설정을 클릭합니다.
b 모든 토큰에 대해이 버튼을 아직 선택하지 않은 경우 사용을 클릭합니다.
c 수집 한 HTTP 포트 번호의 값을 기록해 두십시오.
d 저장을 클릭합니다.
4. 새 토큰을 클릭하여 데이터를 보낼 HEC 토큰을 구성하십시오.
5. 소스 선택 페이지에서 이름에 토큰 이름 (예 : "collectd 토큰")을 입력하십시오.
6. 다른 옵션은 비워 두거나 선택하지 마십시오.
7. 다음을 클릭하십시오.
8. 입력 설정 페이지에서 소스 유형에 대해 선택을 클릭하십시오.
9. 소스 유형 선택을 클릭 한 후 메트릭> collectd_http를 선택하십시오.
10. 기본 색인 옆에서 메트릭 색인을 선택하거나 새 색인 작성을 클릭하여 작성하십시오.
색인 작성을 선택한 경우 새 색인 대화 상자에서 다음을 수행하십시오.
색인 이름을 입력하십시오. 사용자 정의 색인 이름은 숫자, 소문자, 밑줄 및 하이픈으로 만 구성되어야합니다. 색인 이름은 밑줄이나 하이픈으로 시작할 수 없습니다.
인덱스 데이터 유형의 경우 메트릭을 클릭하십시오.
필요에 따라 추가 인덱스 속성을 구성하십시오.
저장을 클릭하십시오.
11. 검토를 클릭 한 후 제출을 클릭하십시오.
12. 표시되는 토큰 값을 복사하십시오. collectd 토큰을 구성해야합니다.

### Add collectd events directly to a metrics index

To test your data input, you can send collectd events directly to your metrics index using the /collector/raw REST API endpoint, which accepts data in the collectd JSON format. Your metrics index is assigned to an HEC data input that has its unique HEC token, and "collectd_http" as its source type.

데이터 입력을 테스트하기 위해 collectd 이벤트를 collectd JSON 형식의 데이터를 허용하는 / collector/raw REST API 엔드 포인트를 사용하여 메트릭 인덱스로 직접 보낼 수 있습니다. 측정 항목 색인은 고유 한 HEC 토큰과 소스 유형으로 "collectd_http"가있는 HEC 데이터 입력에 지정됩니다.

The following example shows a curl command that sends a collectd event to the index associated with your HEC token:

다음 예제는 collectd 이벤트를 HEC 토큰과 연관된 인덱스로 보내는 curl 명령을 보여줍니다.

```properties
curl -k https://localhost:8088/services/collector/raw?sourcetype=collectd_http   \
-H "Authorization: Splunk <HEC_token>"                                      \
-d '[{"values":[164.9196798931339196],"dstypes":["derive"],"dsnames":["value"],"time":1505356687.894,"interval":10.000,"host":"collectd","plugin":"protocols","plugin_instance":"IpExt","type":"protocol_counter","type_instance":"InOctets"}]'
```

You can verify the HEC data input is working by running a search using mcatalog to list all metric names, with the time range set to "All Time", for example:

mcatalog를 사용하여 검색을 실행하여 시간 범위가 "All Time"으로 설정된 모든 메트릭 이름을 나열하여 HEC 데이터 입력이 작동 중인지 확인할 수 있습니다.

```spl
| mcatalog values(metric_name) WHERE index=<your_metrics_index> AND metric_name=protocols.protocol_counter.InOctets.value
```

Or, use the Metrics Catalog REST endpoint to list metric names:
또는 Metrics Catalog REST 엔드 포인트를 사용하여 메트릭 이름을 나열하십시오.

```properties
curl -k -u <admin:passwd> "https://localhost:8089/services/catalog/metricstore/metrics?earliest=0"
```

For more information about using HEC, see the following topics in Getting Data In:
HEC 사용에 대한 자세한 정보는 데이터 가져 오기에서 다음 주제를 참조하십시오.

Set up and use HTTP Event Collector in Splunk Web
Format events for the HTTP Event Collector
Send metrics to a metrics index
See mstats and mcatalog in the Search Reference manual.
Splunk Web에서 HTTP 이벤트 콜렉터 설정 및 사용
HTTP 이벤트 콜렉터의 이벤트 형식
메트릭 색인으로 메트릭 보내기
검색 참조 매뉴얼의 mstats 및 mcatalog를 참조하십시오.

See the following topics in the REST API Reference Manual:

- Metrics Catalog endpoint descriptions
- /collector
- /collector/raw

### Install collectd

Install the collectd agent on the computers in your system from which you want to collect metrics.

메트릭을 수집하려는 시스템의 컴퓨터에 collectd 에이전트를 설치하십시오.

1. Go to the First steps page on the collectd website.
2. Install collectd version 5.6 or higher, following the installation instructions for your operating system.

1. 수집 한 웹 사이트의 첫 번째 단계 페이지로 이동하십시오.
2. 운영 체제의 설치 지시 사항에 따라 collectd 버전 5.6 이상을 설치하십시오.
meteulig-eul sujibhalyeoneun siseutem-ui keo

### Configure collectd

The collectd server is an optional daemon that can be used to aggregate metrics from different inputs and one-to-many collectd clients.

collectd 서버는 다른 입력 및 일대 다 collectd 클라이언트에서 메트릭을 집계하는 데 사용할 수있는 선택적 데몬입니다.

Configure the collectd client to collect data by configuring plugins in the collectd.conf configuration file. The location of the collectd.conf file depends on your operating system. For details, see "Configuration" on the First steps page on the collectd website.

collectd.conf 구성 파일에서 플러그인을 구성하여 데이터를 수집하도록 collectd 클라이언트를 구성하십시오. collectd.conf 파일의 위치는 운영 체제에 따라 다릅니다. 자세한 내용은 collectd 웹 사이트의 첫 번째 단계 페이지에서 "구성"을 참조하십시오.

#### The write_http plugin

The write_http plugin requires the following fields from your HEC data input:
write_http 플러그인에는 HEC 데이터 입력의 다음 필드가 필요합니다.

<table>
<tr><td>Field name</td><td>Description</td><td>Syntax</td><td>Example</td></tr>
<tr><td>URL</td><td>URL to which the values are submitted. This URL includes your Splunk host machine (IP address, host name, or load balancer name), and the HTTP port number.</td><td>URL "https://<Splunk_host>:<HTTP_port>/services/collector/raw"</td><td>URL "https://10.66.104.127:8088/services/collector/raw"</td></tr>
<tr><td>Header</td><td>An HTTP header to add to the request.</td><td>Header "Authorization: Splunk <HEC_token>"</td><td>Header "Authorization: Splunk b0221cd8-c4b4-465a-9a3c-273e3a75aa29"</td></tr>
<tr><td>Format</td><td>The format of the data.</td><td>Format "JSON"</td><td>Format "JSON"</td></tr>
</table>

### Enable and configure plugins

Enable each plugin below by uncommenting the plugin's LoadPlugin statement, then configure the plugin as described. Most of these plugins are for gathering basic OS-level metrics. The logfile plugin is needed for debugging purposes. You can configure additional plugins according to your requirements.

플러그인의 LoadPlugin 문을 주석 해제하여 아래의 각 플러그인을 활성화 한 다음 설명에 따라 플러그인을 구성하십시오. 이 플러그인의 대부분은 기본 OS 레벨 메트릭을 수집하기위한 것입니다. 로그 파일 플러그인은 디버깅 목적으로 필요합니다. 요구 사항에 따라 추가 플러그인을 구성 할 수 있습니다.

>> You might need to install some plugins separately, depending on your installation method and operating system. For details, see the collectd website.
>> 설치 방법 및 운영 체제에 따라 일부 플러그인을 별도로 설치해야 할 수도 있습니다. 자세한 내용은 collectd 웹 사이트를 참조하십시오.

#### cpu

```xml
LoadPlugin cpu
<Plugin cpu>
  ReportByCpu true
</Plugin>
```

#### interface

```xml
LoadPlugin interface
Use the default configuration.
```

#### load

```xml
LoadPlugin load
<Plugin load>
    ReportRelative true
</Plugin>
```

#### logfile

```xml
LoadPlugin logfile
<Plugin logfile>
    LogLevel info
    File STDOUT
    Timestamp true
    PrintSeverity false
</Plugin>
```

#### memory

```xml
LoadPlugin memory
<Plugin memory>
    ValuesAbsolute true
    ValuesPercentage true
</Plugin>
```

#### network

```xml
LoadPlugin network
```

Enable this plugin only if the collectd client is not on the same machine as the connectd server, then use the default configuration.

collectd 클라이언트가 연결된 서버와 동일한 시스템에 없는 경우에만 이 플러그인을 사용으로 설정 한 후 기본 구성을 사용하십시오.

#### syslog

```xml
LoadPlugin syslog
```

Use the default configuration.

#### write_http

You need the values from your HEC data input to configure this plugin.

```xml
LoadPlugin write_http
<Plugin write_http>
    <Node "node1">
        URL "https://<Splunk_host>:<HTTP_port>/services/collector/raw"
        Header "Authorization: Splunk <HEC_token>"
        Format "JSON"
        VerifyPeer false
        VerifyHost false
        Metrics true
        StoreRates true
    </Node>
</Plugin>
```

### Start collectd

To start collectd, follow the instructions under "Starting the daemon" on the First steps page on the collectd website.

수집을 시작하려면 collectd 웹 사이트의 첫 번째 단계 페이지에서 "데몬 시작"에있는 지시 사항을 따르십시오.

Modules for all of the enabled plugins in your collectd.conf file must be installed. Errors are displayed for any modules that are missing. For more about the available collectd plugins, see Table of Plugins on the collectd Wiki website.

collectd.conf 파일에서 활성화 된 모든 플러그인에 대한 모듈이 설치되어 있어야합니다. 누락 된 모듈에 대한 오류가 표시됩니다. 사용 가능한 수집 플러그인에 대한 자세한 내용은 collectd Wiki 웹 사이트에서 플러그인 테이블을 참조하십시오.

Install modules according to your operating system. For example, on Linux you must install collectd-write_http.x86_64 to use the write_http plugin.

운영 체제에 따라 모듈을 설치하십시오. 예를 들어 Linux에서는 write_http 플러그인을 사용하려면 collectd-write_http.x86_64를 설치해야합니다.

Tips:

- For troubleshooting, refer to the collectd log file enabled by the logfile plugin for details.
- Use the File setting in the logfile plugin to write to a specified file rather than to standard output. For example:

- 문제점 해결은 세부 사항은 로그 파일 플러그인에서 사용 가능한 collectd 로그 파일을 참조하십시오.
- 로그 파일 플러그인의 파일 설정을 사용하여 표준 출력이 아닌 지정된 파일에 씁니다. 예를 들면 다음과 같습니다.

```xml
<Plugin logfile>
    LogLevel info
    File "/var/log/collectd.log"
    Timestamp true
    PrintSeverity false
</Plugin>
```

- If you are installing collectd on Linux, you can use yum to list available modules. For example, use this CLI command:
- Linux에 collectd 것을 설치하는 경우 yum을 사용하여 사용 가능한 모듈을 나열 할 수 있습니다. 예를 들어,이 CLI 명령을 사용하십시오.

```bash
yum list | grep collectd
```

- In the collectd.conf file, set the FQDNLookup setting to false to render a friendly name for the domain name.
- collectd.conf 파일에서 FQDNLookup 설정을 false로 설정하여 도메인네임의 이름을 알기 쉽게 표시하십시오.

## Get metrics in from other sources

If you are gathering metrics from a source that is not natively supported, you can still add this metrics data to a metrics index.

기본적으로 지원되지 않는 소스에서 메트릭을 수집하는 경우이 메트릭 데이터를 메트릭 인덱스에 계속 추가 할 수 있습니다.

### Get metrics in from files in CSV format

There are two accepted formats for CSV files when you use them as inputs for metrics data. The format you use depends on how you want the Splunk software to index the information in the CSV file. Should it index so that each data point has multiple measurements, or so that each data point has only one measurement?

CSV 파일을 메트릭 데이터의 입력으로 사용할 경우 CSV 파일에 허용되는 두 가지 형식이 있습니다. 사용하는 형식은 Splunk 소프트웨어가 CSV 파일의 정보를 인덱싱하는 방법에 따라 다릅니다. 각 데이터 포인트에 여러 측정 값이 있거나 각 데이터 포인트에 측정 값이 하나만 있도록 색인을 작성해야합니까?

It is more efficient to use metric data points that can contain multiple measurements. When you index metrics data this way you reduce your data storage costs and can benefit from improved search performance.

여러 측정을 포함 할 수있는 메트릭 데이터 포인트를 사용하는 것이 더 효율적입니다. 이 방법으로 메트릭 데이터를 인덱싱하면 데이터 스토리지 비용이 절감되고 검색 성능이 향상됩니다.

#### Set metrics CSV source types and data inputs

If your metrics data is in CSV format, use the metrics_csv pre-trained source type. It can handle both CSV metrics formats.

메트릭 데이터가 CSV 형식 인 경우 metrics_csv 사전 훈련 된 소스 유형을 사용하십시오. CSV 메트릭 형식을 모두 처리 할 수 있습니다.

Create a data input to add your CSV data to a metrics index. The input uses the pretrained metrics_csv source type. The data input should have:

CSV 입력을 지표 색인에 추가 할 데이터 입력을 작성하십시오. 입력은 사전 훈련 된 metrics_csv 소스 유형을 사용합니다. 데이터 입력에는 다음이 있어야합니다.

- Source type: Metrics > metrics_csv
- Index: a metrics index

After you set up your metrics_csv input, you should have the following inputs.conf configuration on your universal forwarder. It monitors the CSV data and sends it to the metrics indexer.
metrics_csv 입력을 설정 한 후 유니버설 포워더에 다음 inputs.conf 구성이 있어야합니다. CSV 데이터를 모니터링하여 메트릭 인덱서로 보냅니다.

```properties
#inputs.conf
[monitor:///opt/metrics_data]
index = metrics
sourcetype = metrics_csv
```

You should also have the following indexes.conf configuration on the metrics indexer:

또한 지표 인덱서에 다음과 같은 indexes.conf 구성이 있어야합니다.

```properties
#indexes.conf
[metrics]
homePath = $SPLUNK_DB/metrics/db
coldPath = $SPLUNK_DB/metrics/colddb
thawedPath = $SPLUNK_DB/metrics/thaweddb
datatype = metric
maxTotalDataSizeMB = 512000
```

See Monitor files and directories in the Getting Data In manual, and Create metrics indexes in the Managing Indexers and Clusters of Indexers manual.

데이터 가져 오기 매뉴얼의 파일 및 디렉토리 모니터 및 인덱서 및 인덱서 클러스터 관리 매뉴얼의 메트릭 색인 작성을 참조하십시오.

#### Format a CSV file for multiple-measurement metric data points

When you format a CSV file for multiple-measurement metric data points, the first column header is _time, the metric timestamp. It is a required field.

다중 측정 메트릭 데이터 포인트에 대한 CSV 파일을 형식화 할 때 첫 번째 열 헤더는 메트릭 타임 스탬프 인 _time입니다. 필수 필드입니다.

This is followed by one or more column headers for each metric measurement. Each measurement column header follows this syntax: metric_name:<metric_name>.

각 측정 단위마다 하나 이상의 열 머리글이옵니다. 각 측정 열 헤더는 metric_name : <metric_name> 구문을 따릅니다.

The Splunk software considers additional columns that are not a timestamp or a measurement to be dimensions.

Splunk 소프트웨어는 타임 스탬프 또는 측정이 아닌 추가 열을 차원으로 간주합니다.

Each row of the CSV table is a separate metric data point.
CSV 테이블의 각 행은 별도의 메트릭 데이터 포인트입니다.

<table>
<tr><td>Field name</td><td>Required</td><td>Description</td><td>Example</td></tr>
<tr><td>_time</td><td>Yes</td><td>The metric timestamp. The format is epoch time (elapsed time since 1/1/1970), in milliseconds.
메트릭 타임 스탬프 형식은 에포크 시간 (970 년 1 월 1 일 이후 경과 시간)입니다 (밀리 초).
</td><td>1504907933.000</td></tr>
<tr><td>metric_name:&lt;metric_name&gt;</td><td>Yes</td><td>A measurement for a specific metric, as specified by <metric_name>, such as metric_name:os.cpu.idle or metric_name:max.size.kpbs. Their values are always numeric.
metric_name : os.cpu.idle 또는 metric_name : max.size.kpbs와 같이로 지정된 특정 메트릭에 대한 측정입니다. 그들의 값은 항상 숫자입니다.
</td><td>13.34</td></tr>
<tr><td>dimensions</td><td>No</td><td>All other fields are treated as dimensions.다른 모든 필드는 차원으로 처리됩니다.</td><td>For a dimension named ip, a value of 192.0.2.1.</td></tr>
</table>

Here is an example of a CSV file that is formatted for multiple-measurement metric data points. The first column is _time, the metric timestamp. The middle three columns are measurements. The last two columns are dimensions.

다음은 다중 측정 지표 데이터 포인트에 맞게 형식화 된 CSV 파일의 예입니다. 첫 번째 열은 메트릭 타임 스탬프 인 _time입니다. 가운데 세 열은 측정입니다. 마지막 두 열은 치수입니다.

```csv
"_time","metric_name:cpu.usr","metric_name:cpu.sys","metric_name:cpu.idle","dc","host"
"1562020701",11.12,12.23,13.34,"east","east.splunk.com"
"1562020702",21.12,22.33,23.34,"west","west.splunk.com"
```

This CSV file example contains the same information as the example CSV file for single-measurement metric data points in the following section. However, because it uses two data points for this information instead of six, it will take up less space on disk when it is indexed.

이 CSV 파일 예제에는 다음 섹션의 단일 측정 메트릭 데이터 포인트에 대한 예제 CSV 파일과 동일한 정보가 포함되어 있습니다. 그러나이 정보에 6 개가 아닌 2 개의 데이터 포인트를 사용하기 때문에 인덱싱 될 때 디스크 공간을 덜 차지합니다.

#### Format a CSV file for single-measurement metric data points

When you format a CSV file for single-measurement metric data points, the first three columns are fields that are required for single-measurement metric data points:

단일 측정 메트릭 데이터 포인트에 대한 CSV 파일을 형식화 할 때 처음 세 열은 단일 측정 메트릭 데이터 포인트에 필요한 필드입니다.

- metric_timestamp
- metric_name
- _value.

All additional columns are considered to be dimensions.
모든 추가 열은 차원으로 간주됩니다.

During the ingestion and indexing process the metric_name and _value measurements will merged into the metric_name:<metric_name>=<numeric_value> format.
수집 및 인덱싱 프로세스 중에 metric_name 및 _value 측정 값이 metric_name : <metric_name> = <numeric_value> 형식으로 병합됩니다.

<table>
<tr><td>Field name</td><td>Required</td><td>Description</td><td>Example</td></tr>
<tr><td>metric_timestamp</td><td>Yes</td><td>The timestamp format is epoch time (elapsed time since 1/1/1970), in milliseconds.</td><td>1504907933.000</td></tr>
<tr><td>metric_name</td><td>Yes</td><td>The metric name using dotted-string notation.</td><td>os.cpu.percent</td></tr>
<tr><td>_value</td><td>Yes</td><td>The numerical value associated with the metric_name.</td><td>42.12345</td></tr>
<tr><td>dimensions</td><td>No</td><td>All other fields are treated as dimensions.</td><td>ip</td></tr>
</table>

Here is an example of a CSV file that is formatted for single-measurement metric data points. The first three columns of the table are the fields that are required for single-measurement metric data points. All additional columns are dimensions. This CSV file has dc and host as dimensions.

다음은 단일 측정 메트릭 데이터 포인트 용으로 형식화 된 CSV 파일의 예입니다. 테이블의 처음 세 열은 단일 측정 메트릭 데이터 포인트에 필요한 필드입니다. 모든 추가 열은 치수입니다. 이 CSV 파일에는 DC 및 호스트가 차원으로 있습니다.

```csv
"metric_timestamp","metric_name","_value","dc","host"
"1562020701","cpu.usr",11.12,"east","east.splunk.com"
"1562020701","cpu.sys",12.23,"east","east.splunk.com"
"1562020701","cpu.idle",13.34,"east","east.splunk.com"
"1562020702","cpu.usr",21.12,"west","west.splunk.com"
"1562020702","cpu.sys",22.33,"west","west.splunk.com"
"1562020702","cpu.idle",23.34,"west","west.splunk.com"
```

If you compare this example to the example for multiple-measurement metric data points, you can see how the single-metric format would take up more space on disk. This table contains the same information as the multiple-measurement table. However, this table uses six data points where the multiple-measurement table only uses two.

이 예제를 다중 측정 메트릭 데이터 포인트의 예제와 비교하면 단일 메트릭 형식이 디스크에서 더 많은 공간을 차지하는 방식을 확인할 수 있습니다. 이 테이블에는 다중 측정 테이블과 동일한 정보가 포함됩니다. 그러나이 테이블은 다중 측정 테이블이 2 개만 사용하는 6 개의 데이터 포인트를 사용합니다.

### Get metrics in from clients over TCP/UDP

You can add metrics data from a client that is not natively supported to a metrics index by manually configuring a source type for your data, then defining regular expressions to specify how the Splunk software should extract the required metrics fields. See Metrics data format.

데이터의 소스 유형을 수동으로 구성한 다음 정규 표현식을 정의하여 Splunk 소프트웨어가 필요한 메트릭 필드를 추출하는 방법을 지정하여 기본적으로 지원되지 않는 클라이언트의 메트릭 데이터를 메트릭 인덱스에 추가 할 수 있습니다. 지표 데이터 형식을 참조하십시오.

For example, let's say you are using Graphite. The Graphite plaintext protocol format is:
예를 들어, Graphite를 사용한다고 가정 해 봅시다. Graphite 일반 텍스트 프로토콜 형식은 다음과 같습니다.

`<metric path> <metric value> <metric timestamp>`

A sample metric might be:

510fcbb8f755.sda2.diskio.read_time 250 1487747370

To index these metrics, edit Splunk configuration files to manually specify how to extract fields.
이러한 메트릭을 인덱싱하려면 Splunk 구성 파일을 편집하여 필드 추출 방법을 수동으로 지정하십시오.

#### Configure field extraction by editing configuration files

1. Define a custom source type for your metrics data.
In a text editor, open the props.conf configuration file from the local directory for the location you want to use, such as the Search & Reporting app ($SPLUNK_HOME/etc/apps/search/local/) or the system ($SPLUNK_HOME/etc/system/local). If a props.conf file does not exist in this location, create a text file and save it to that location.
Append a stanza to the props.conf file as follows:

1. 메트릭 데이터에 대한 사용자 정의 소스 유형을 정의하십시오.
텍스트 편집기에서 검색 및보고 앱 ($ SPLUNK_HOME / etc / apps / search / local /) 또는 시스템 ($ SPLUNK_HOME)과 같이 사용하려는 위치의 로컬 디렉토리에서 props.conf 구성 파일을여십시오. / etc / system / local). props.conf 파일이이 위치에 없으면 텍스트 파일을 만들어 해당 위치에 저장하십시오.
다음과 같이 props.conf 파일에 스탠자를 추가하십시오.

```properties
# props.conf
[<metrics_sourcetype_name>]
TIME_PREFIX = <regular expression>
TIME_FORMAT = <strptime-style format>
TRANSFORMS-<class> = <transform_stanza_name>
NO_BINARY_CHECK = true
SHOULD_LINEMERGE = false
pulldown_type = 1
category = Metrics
```

- metrics_sourcetype_name Name of your custom metrics source type.
- TIME_PREFIX = regular expression: A regular expression that indicates where the timestamp is located.
- TIME_FORMAT = strptime-style format: A strptime format string used to extract the date. For more about strptime, see Configure timestamp recognition in the Getting Data In manual.
- TRANSFORMS-<class> = <transform_stanza_name>: class is a unique literal string that identifies the namespace of the field to extract. transform_stanza_name is the name of the name of your stanza in transforms.conf that indicates how to extract the field.

-metrics_sourcetype_name 맞춤 측정 항목 소스 유형의 이름입니다.
-TIME_PREFIX = 정규식 : 타임 스탬프의 위치를 나타내는 정규식입니다.
-TIME_FORMAT = strptime 스타일 형식 : 날짜를 추출하는 데 사용되는 strptime 형식 문자열. strptime에 대한 자세한 내용은 데이터 가져 오기 매뉴얼의 타임 스탬프 인식 구성을 참조하십시오.
-TRANSFORMS- <class> = <transform_stanza_name> : class는 추출 할 필드의 네임 스페이스를 식별하는 고유 한 리터럴 문자열입니다. transform_stanza_name은 필드 추출 방법을 나타내는 transforms.conf의 스탠자 이름 이름입니다.

2. Define a regular expression for each metrics field to extract.
In a text editor, open the transforms.conf configuration file from the local directory for the location you want to use, such as the Search & Reporting app ($SPLUNK_HOME/etc/apps/search/local/) or the system ($SPLUNK_HOME/etc/system/local). If a transforms.conf file does not exist in this location, create a text file and save it to that location.
Append a stanza for each regular expression as follows:

2. 추출 할 각 메트릭 필드에 대한 정규식을 정의하십시오.
텍스트 편집기에서 검색 및보고 앱 ($ SPLUNK_HOME / etc / apps / search / local /) 또는 시스템 ($ SPLUNK_HOME)과 같이 사용하려는 위치의 로컬 디렉토리에서 transforms.conf 구성 파일을여십시오. / etc / system / local). 이 위치에 transforms.conf 파일이 없으면 텍스트 파일을 만들어 해당 위치에 저장하십시오.
다음과 같이 각 정규식에 대한 스탠자를 추가하십시오.

```properties
# transforms.conf

[<transform_stanza_name>]
REGEX = <regular expression>
FORMAT = <string>
WRITE_META = true
```

- transform_stanza_name: A unique name for this stanza.
- REGEX = <regular expression>: A regular expression that defines how to match and extract metrics fields from this metrics data.
- FORMAT = <string>: A string that specifies the format of the metrics event.

- transform_stanza_name :이 스탠자의 고유 이름입니다.
- REGEX = <regular expression> :이 메트릭 데이터에서 메트릭 필드를 일치시키고 추출하는 방법을 정의하는 정규식입니다.
- FORMAT = <string> : 메트릭 이벤트의 형식을 지정하는 문자열입니다.

3. Create a data input for this source type as described in Set up a data input for StatsD data, and select your custom source type.

3. StatsD 데이터에 대한 데이터 입력 설정에 설명 된대로이 소스 유형에 대한 데이터 입력을 작성하고 사용자 정의 소스 유형을 선택하십시오.

For more about editing these configuration files, see About configuration files, props.conf, and transforms.conf in the Admin Manual.

이러한 구성 파일 편집에 대한 자세한 내용은 관리자 매뉴얼의 구성 파일, props.conf 및 transforms.conf를 참조하십시오.

#### Example of configuring field extraction

This example shows how to create a custom source type and regular expressions to extract fields from Graphite metrics data.

이 예는 Graphite 지표 데이터에서 필드를 추출하기 위해 사용자 정의 소스 유형 및 정규식을 작성하는 방법을 보여줍니다.

```propeties
# props.conf.example
[graphite_plaintext]
TIME_PREFIX = \s(\d{0,10})$
TIME_FORMAT =  %s
NO_BINARY_CHECK = true
SHOULD_LINEMERGE = false
pulldown_type = 1
TRANSFORMS-graphite-host = graphite_host
TRANSFORMS-graphite-metricname = graphite_metric_name
TRANSFORMS-graphite-metricvalue = graphite_metric_value
category = Metrics
```

```properties
# transforms.conf.example
[graphite_host]
REGEX = ^(\S[^\.]+)
FORMAT = host::$1
DEST_KEY = MetaData:Host

[graphite_metric_name]
REGEX = \.(\S+)
FORMAT = metric_name::graphite.$1
WRITE_META = true

[graphite_metric_value]
REGEX = \w+\s+(\d+.?\d+)\s+
FORMAT = _value::$1
WRITE_META = true
```

### Get metrics in from clients over HTTP or HTTPS

If you want to send metrics data in JSON format from a client that is not natively supported to a metrics index over HTTP or HTTPS, use the HTTP Event Collector (HEC) and the /collector REST API endpoint.

HTTP 또는 HTTPS를 통해 메트릭 인덱스에 기본적으로 지원되지 않는 클라이언트에서 JSON 형식의 메트릭 데이터를 보내려면 HTTP 이벤트 콜렉터 (HEC) 및 / collector REST API 엔드 포인트를 사용하십시오.

#### Create a data input and token for HEC

1. In Splunk Web, click Settings > Data Inputs.
2. Under Local Inputs, click HTTP Event Collector.
3. Verify that HEC is enabled.
Click Global Settings.
For All Tokens, click Enabled if this button is not already selected.
Click Save.
4. Configure an HEC token for sending data by clicking New Token.
5. On the Select Source page, for Name, enter a token name, for example "Metrics token".
6. Leave the other options blank or unselected.
7. Click Next.
8. On the Input Settings page, for Source type, click New.
9. In Source Type, type a name for your new source type.
10. For Source Type Category, select Metrics.
11. Optionally, in Source Type Description type a description.
12. Next to Default Index, select your metrics index, or click Create a new index to create one.
If you choose to create an index, in the New Index dialog box:
Enter an Index Name.
For Index Data Type, click Metrics.
Configure additional index properties as needed.
Click Save.
13. Click Review, and then click Submit.
14. Copy the Token Value that is displayed. This HEC token is required for sending data.

See Getting data in with HTTP Event Collector on the Splunk Developer Portal.

1. Splunk Web에서 설정> 데이터 입력을 클릭하십시오.
2. 로컬 입력에서 HTTP 이벤트 콜렉터를 클릭하십시오.
3. HEC가 사용 가능한지 확인하십시오.
전역 설정을 클릭하십시오.
모든 토큰의 경우이 단추가 아직 선택되지 않은 경우 사용을 클릭하십시오.
저장을 클릭하십시오.
4. 새 토큰을 클릭하여 데이터를 보낼 HEC 토큰을 구성하십시오.
5. 소스 선택 페이지에서 이름에 토큰 이름 (예 : "Metrics token")을 입력하십시오.
6. 다른 옵션은 비워 두거나 선택하지 마십시오.
7. 다음을 클릭하십시오.
8. 입력 설정 페이지에서 소스 유형에 대해 새로 작성을 클릭하십시오.
9. 소스 유형에서 새 소스 유형의 이름을 입력하십시오.
10. 소스 유형 범주에서 메트릭을 선택하십시오.
11. 선택적으로 소스 유형 설명에 설명을 입력하십시오.
12. 기본 색인 옆에서 메트릭 색인을 선택하거나 새 색인 작성을 클릭하여 작성하십시오.
색인 작성을 선택한 경우 새 색인 대화 상자에서 다음을 수행하십시오.
색인 이름을 입력하십시오.
인덱스 데이터 유형의 경우 메트릭을 클릭하십시오.
필요에 따라 추가 인덱스 속성을 구성하십시오.
저장을 클릭하십시오.
13. 검토를 클릭 한 후 제출을 클릭하십시오.
14. 표시된 토큰 값을 복사하십시오. 이 HEC 토큰은 데이터 전송에 필요합니다.

Splunk 개발자 포털에서 HTTP 이벤트 콜렉터로 데이터 가져 오기를 참조하십시오.

#### Send data to a metrics index over HTTP

Use the /collector REST API endpoint and your HEC token to send data directly to a metrics index as follows:

```properties
http://<Splunk_host>:<HTTP_port>/services/collector \
-H "Authorization: Splunk <HEC_token>"                                                \
-d "<metrics_data>"
```

You need to provide the following values:

- Splunk host machine (IP address, host name, or load balancer name)
- HTTP port number
- HEC token value
- Metrics data, which requires an "event" field set to "metric".

For more about HEC, see Getting data in with HTTP Event Collector and Event formatting on the Splunk Developer Portal.
HEC에 대한 자세한 내용은 Splunk 개발자 포털에서 HTTP 이벤트 콜렉터 및 이벤트 형식을 사용하여 데이터 가져 오기를 참조하십시오.

For more about the /collector endpoint, see /collector in the REST API Reference Manual.
/ collector 엔드 포인트에 대한 자세한 내용은 REST API Reference Manual의 / collector를 참조하십시오.

#### Example of sending metrics using HEC

The following example shows a command that sends a metric data point to a metrics index, with the following values:
다음 예는 다음 값을 사용하여 지표 데이터 포인트를 지표 색인으로 보내는 명령을 보여줍니다.

- Splunk host machine: "localhost"
- HTTP port number: "8088"
- HEC token value: "b0221cd8-c4b4-465a-9a3c-273e3a75aa29"

```properties
curl -k https://localhost:8088/services/collector                     \
-H "Authorization: Splunk b0221cd8-c4b4-465a-9a3c-273e3a75aa29"       \
-d '{"time": 1486683865.000,"event":"metric","source":"disk","host":"host_1.splunk.com","fields":{"region":"us-west-1","datacenter":"dc1","rack":"63","os":"Ubuntu16.10","arch":"x64","team":"LON","service":"6","service_version":"0","service_environment":"test","path":"/dev/sda1","fstype":"ext3","metric_name:cpu.usr": 11.12,"metric_name:cpu.sys": 12.23, "metric_name:cpu.idle": 13.34}}'
```

The measurements for this metric data point appear at the end of the JSON blob. They follow a multiple-metric format that uses the "metric_name:<metric_name>":<numeric_value> syntax.
이 지표 데이터 포인트의 측정 값은 JSON Blob 끝에 나타납니다. "metric_name : <metric_name>": <numeric_value> 구문을 사용하는 다중 메트릭 형식을 따릅니다.

#### The multiple-metric JSON format

Versions of the Splunk platform previous to 8.0.0 used a JSON format that only supported one metric measurement per JSON object. This resulted in metric data points that could only contain one measurement at a time.
8.0.0 이전의 Splunk 플랫폼 버전은 JSON 객체 당 하나의 메트릭 측정 만 지원하는 JSON 형식을 사용했습니다. 결과적으로 한 번에 하나의 측정 만 포함 할 수있는 메트릭 데이터 포인트가 생성되었습니다.

Version 8.0.0 of the Splunk platform supports a JSON format which allows each JSON object to contain measurements for multiple metrics. These JSON objects generate multiple-measurement metric data points. Multiple-measurement metric data points take up less space on disk and can improve search performance.
Splunk 플랫폼 버전 8.0.0은 각 JSON 객체에 여러 메트릭에 대한 측정 값을 포함 할 수있는 JSON 형식을 지원합니다. 이러한 JSON 객체는 다중 측정 지표 데이터 포인트를 생성합니다. 다중 측정 메트릭 데이터 포인트는 디스크의 공간을 덜 차지하고 검색 성능을 향상시킬 수 있습니다.

Here is an example of a JSON object in the multiple-metric format.
다음은 다중 메트릭 형식의 JSON 객체 예입니다.

```json
{
  "time": 1486683865,
  "source": "metrics",
  "sourcetype": "perflog",
  "host": "host_1.splunk.com",
  "fields": {
    "region": "us-west-1",
    "datacenter": "dc2",
    "rack": "63",
    "os": "Ubuntu16.10",
    "arch": "x64",
    "team": "LON",
    "service": "6",
    "service_version": "0",
    "service_environment": "test",
    "path": "/dev/sda1",
    "fstype": "ext3"
    "metric_name:cpu.usr": 11.12,
    "metric_name:cpu.sys": 12.23,
    "metric_name:cpu.idle": 13.34
  }
}
```
