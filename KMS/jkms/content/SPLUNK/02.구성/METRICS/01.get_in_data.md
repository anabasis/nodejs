# Get metrics data in

## Get metrics in from StatsD

StatsD is a network daemon that runs on the Node.js platform, sending metrics over UDP or TCP. For an overview of StatsD, see Measure Anything, Measure Everything on the Code as Craft website.

StatsD has several implementations, some of which encode dimensions in different ways. The Splunk platform supports the following formats natively:

- Basic StatsD data line metric protocol, which includes metric_name, _value and metric_type.
- Expanded StatsD data line metric protocol, which adds sample rate and dimensions.
Splunk supports two metric_type values for StatsD metric data points: g, for gauge metrics, and c, for counter metrics.

### Basic StatsD metric protocol

The basic StatsD data line metric protocol just has three fields: the metric_name, the metric _value, and the metric_type.

Syntax

<metric_name>:<_value>|<metric_type>
Example metric

performance.os.disk:1099511627776|g

### Expanded StatsD metric protocol

The expanded StatsD data line metric protocol supports dimensions and a sample rate. Sample rates only apply to counter metrics, meaning they have a metric_type of c. For

For more about formats for metric names and dimensions, see Best practices for metrics.

#### Syntax

<metric_name>:<_value>|<metric_type>|@<sample_rate>|#dim1:valueX,dim2:valueY

##### Example gauge metric

A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. For example, you can use a gauge to represent the number of currently running search jobs, or the temperature in your server room.

performance.os.disk:1099511627776|g|#region:us-west-1,datacenter:us-west-1a,rack:63,os:Ubuntu16.10,arch:x64,team:LON,service:6,service_version:0,service_environment:test, path:/dev/sdal,fstype:ext3

#### Example counter metric, after processing by the Splunk platform

A counter metric counts occurrences of an event. Its value can only increase or be reset to zero. For example, you can use a counter to represent a number of requests served, tasks completed, or errors. For more information about counter metrics, see Investigate counter metrics.

Here is an example of an counter metric that has been processed by the Splunk platform.

event.login:6|c|@0.5|#region:west,dc:west-1,ip:10.1.1.1,host:valis1.buttercupgames.com,app:zoolu
Note that this counter metric has a sample rate of 0.5. This means that this counter metric is sampled only 50% of the time by the StatsD client. The Splunk platform adjusts for this by multiplying the metric value by 1/0.5, or 2. This means that the original metric sent from the StatsD client looked like this:

event.login:3|c|@0.5|#region:west,dc:west-1,ip:10.1.1.1,host:valis1.buttercupgames.com,app:zoolu
Note that the original metric event had a numeric value of 3.

#### About the sample rate

When large numbers of data points are being produced for a particular counter metric, it can be expensive for the Splunk platform to aggregate them. The StatsD client manages this by implementing a sample rate to reduce the network traffic that it sends to the Splunk platform.

The StatsD client puts the sample_rate value in the counter metric data point to indicate to the Splunk platform the actual downsampling percentage that it employed. The Splunk platform responds to this by multiplying the value of a downsampled counter metric by 1/<sample_rate>.

For example, say you have a counter metric named event.login with a sample_rate of 0.1. This means that only 10% of the event.login data points are passed from the StatsD client to your Splunk platform implementation. The Splunk platform multiplies the event.login values by 1/0.1, or 10, to adjust for the missed data points. So if your Splunk platform implementation receives a event.login data point with a value of 2, it will change that value to 20.

The Splunk platform passes an warning message for sample_rate values that are not within 0 and 1. The default setting for sample_rate is 1.

### Using other StatsD formats

If you use a StatsD implementation that uses a different format for dimensions from the ones that the Splunk platform supports natively, for example, one that embeds dimensions within the metric name, you can still use metrics in the Splunk platform. However, you'll need to customize Splunk configuration files to specify how to extract dimensions from your format.

Another option is to use StatsD to gather metrics, but use collectd to send the data to the Splunk platform over HTTP. The benefit of this method is that collectd normalizes the dimension format in the metrics data. For more, see Get metrics in from collectd.

### Set up a data input for StatsD data

After you configure your data source to send data in the StatsD protocol, create a UDP or TCP data input in the Splunk platform to listen for StatsD data on an open port.

1. In Splunk Web, go to Settings > Data inputs.
2. Under Local inputs, click Add new next to UDP or TCP, depending on the type of input you want to create.
When using UDP ports to ingest metric data, you cannot use parallel ingestion or the multiple pipeline sets feature.

3. For Port, enter the number of the port you are using for StatsD.
4. Click Next.
5. Click Select Source Type, then select Metrics > statsd.
6. For Index, select an existing metrics index. Or, click Create a new index to create one.
If you choose to create an index, in the New Index dialog box:
Enter an Index Name. User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. Index names cannot begin with an underscore or hyphen.
For Index Data Type, click Metrics.
Configure additional index properties as needed.
Click Save.
7. Click Review, then click Submit.

## Extract dimensions for unsupported StatsD formats

Many StatsD clients embed dimension names in the metric name. For example, let's say your StatsD client uses the following line metric protocol format, which is not supported natively by the Splunk platform:

`<dimension>.<metric_name>:<value>|<metric_type>`

Here's an example of a metric returned using this unsupported format:

`10.1.1.198.cpu.percent:75|g`

The extracted measurement should be:

`metric_name:cpu.percent=75`

The extracted dimension should be:

`ip=10.1.1.198`

To create the correct results, you must edit Splunk configuration files or use the REST API to create a custom source type that specifies how to extract dimensions from this metrics data.

### Configure dimension extraction by editing configuration files

1. Define a custom source type for your StatsD metrics data.
In a text editor, open the props.conf configuration file from the local directory for the location you want to use, such as the Search & Reporting app ($SPLUNK_HOME/etc/apps/search/local/) or from the system ($SPLUNK_HOME/etc/system/local). If a props.conf file does not exist in this location, create a text file and save it to that location.
Append a stanza to the props.conf file as follows:

```properties
# props.conf
[<metrics_sourcetype_name>]
METRICS_PROTOCOL = statsd
STATSD-DIM-TRANSFORMS = <statsd_dim_stanza_name1>,<statsd_dim_stanza_name2>...
```

metrics_sourcetype_name: The name of your custom metrics source type.
statsd_dim_stanza_name: A comma-separated list of transforms stanza names that specify how to extract dimensions. If only one stanza is used for the source type, and if the transforms stanza name is same as the metrics_sourcetype_name, this STATSD-DIM-TRANSFORMS setting can be omitted.

2. Define one or more regular expressions to extract the dimensions from metric_name.
In a text editor, open the transforms.conf configuration file from the local directory for the location you want to use, such as the Search & Reporting app ($SPLUNK_HOME/etc/apps/search/local/) or from the system ($SPLUNK_HOME/etc/system/local). If a transforms.conf file does not exist in this location, create a text file and save it to that location.
Append a stanza for each regular expression as follows:

```properties
# transforms.conf
[statsd-dims:<unique_transforms_stanza_name>]
REGEX = <regular expression>
REMOVE_DIMS_FROM_METRIC_NAME = <Boolean>
```

unique_transforms_stanza_name: A unique name for this stanza.
REGEX = <regular expression>: A regular expression that defines how to match and extract dimensions from StatsD metrics data. The Splunk platform supports a named capturing-group extraction format (?<dim1>group)(?<dim2>group)... to provide dimension names for the corresponding values that are extracted.
REMOVE_DIMS_FROM_METRIC_NAME = <Boolean>: Specifies whether unmatched segments of the StatsD dotted name segment are used as the metric_name.
When true, dimension values are removed from the measurement and the unmatched portion becomes the metric_name. The default value is true.

When false, extracted dimension values are included in the metric_name.

For example, a metric measurement name is "x.y.z". The regular expression matches "y" and "z". When REMOVE_DIMS_FROM_METRIC_NAME is true, metric_name is "x". When false, metric_name is "x.y.z".

3. Create a data input for this source type as described in Set up a data input for StatsD data, and select your custom source type.

For more about editing these configuration files, see About configuration files, props.conf, and transforms.conf in the Admin Manual.

### Examples of configuring dimension extraction

Let's say you have StatsD metrics data such as:

data=mem.percent.used.10.2.3.4.windows:33|g
You need to extract the "ipv4" and "os" dimensions.

If you defined two regular expressions, one for "ipv4" and one for "os", you would append the following stanzas to your configuration files:

```properties
# props.conf.example
[my_custom_metrics_sourcetype]
METRICS_PROTOCOL = statsd
STATSD-DIM-TRANSFORMS = regex_stanza1, regex_stanza2
```

```properties
# transforms.conf.example
[statsd-dims:regex_stanza1]
REGEX = (?<ipv4>\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3})
REMOVE_DIMS_FROM_METRIC_NAME = true
[statsd-dims:regex_stanza2]
REGEX = \S+\.(?<os>\w+):
REMOVE_DIMS_FROM_METRIC_NAME = true
```

Now let's say you can accomplish this same extraction using a single regular expression. In this case, you would append the following stanzas to your configuration files:

```properties
# props.conf.example
[my_custom_metrics_sourcetype]
METRICS_PROTOCOL = statsd
```

```properties
# transforms.conf.example
[statsd-dims:my_custom_metrics_sourcetype]
REGEX = (?<ipv4>\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3})\.(?<os>\w+):
REMOVE_DIMS_FROM_METRIC_NAME = true
```

Notice that the STATSD-DIM-TRANSFORMS setting in the props.conf configuration file is not needed when only a single regular expression is used for a source type.

### Configure dimension extraction for StatsD by using the REST API

1. Define a custom source type for your StatsD metrics data by using the /services/saved/sourcetypes REST endpoint:

```bash
https://<host>:<mPort>/services/saved/sourcetypes   \
-d "name=<metrics_sourcetype_name>&METRICS_PROTOCOL=statsd&STATSD-DIM-TRANSFORMS=<statsd_dim_stanza_name>&SHOULD_LINEMERGE=false&ANNOTATE_PUNCT=false&ADD_EXTRA_TIME_FIELDS=false&DATETIME_CONFIG=CURRENT&pulldown_type=true&category=Metrics"
```

- metrics_sourcetype_name: The name of your custom metrics source type.
- statsd_dim_stanza_name: A list of transforms stanza names that specify how to extract dimensions. If only one stanza is used for the source type, and if the transforms stanza name is same as the metrics_sourcetype_name, this STATSD-DIM-TRANSFORMS setting can be omitted.

For example, enter the following command:

```bash
curl -k -u admin:changeme https://localhost:8089/services/saved/sourcetypes   \
-d "name=statsd_custom&METRICS_PROTOCOL=statsd&STATSD-DIM-TRANSFORMS=statsd-ex&SHOULD_LINEMERGE=false&ANNOTATE_PUNCT=false&ADD_EXTRA_TIME_FIELDS=false&DATETIME_CONFIG=CURRENT&pulldown_type=true&category=Metrics"
```

2. Create one or more regular expressions to extract the dimensions from metric_name by using the /data/transforms/statsdextractions REST endpoint:

```properties
https://<host>:<mPort>/services/data/transforms/statsdextractions \
-d "name=<unique_transforms_stanza_name>&REGEX=<regular expression>&REMOVE_DIMS_FROM_METRIC_NAME=<Boolean>"
```

- unique_tran-sforms_stanza_name: A unique name for this stanza.
- REGEX = <regular expression>: A regular expression that defines how to match and extract dimensions from StatsD metrics data. The Splunk platform supports a named capturing-group extraction format (?<dim1>group)(?<dim2>group)... to provide dimension names for the corresponding values that are extracted.
- REMOVE_DIMS_FROM_METRIC_NAME = <Boolean>: Specifies whether unmatched segments of the StatsD dotted name segment are used as the metric_name.
When true, dimension values are be removed from the measurement and the unmatched portion becomes the metric_name. The default value is true.

When false, extracted dimension values are included in the metric_name.

For example, a metric measurement name is "x.y.z". The regular expression matches "y" and "z". When REMOVE_DIMS_FROM_METRIC_NAME is true, metric_name is "x". When false, metric_name is "x.y.z".

For example, enter the following command:

```properties
curl -k -u admin:changeme https://localhost:8089/services/data/transforms/statsdextractions \
-d "name=statsd-ex&REGEX=\.(?<hostname>\S%2B?)\.(?<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})&REMOVE_DIMS_FROM_METRIC_NAME=true"
```

3. Reload the metrics processor to load the configuration changes by using the /admin/metrics-reload/_reload REST endpoint:

```properties
https://<host>:<mPort>/services/admin/metrics-reload/_reload
```

For example, enter the following command:

```properties
curl -k -u admin:changeme \
https://localhost:8089/services/admin/metrics-reload/_reload
```

4. Create a data input for this source type as described in Set up a data input for StatsD data, and select your custom source type.

For more about using the Splunk REST API, see Using the REST API reference, /data/transforms/statsdextractions, and /admin/metrics-reload/_reload in the REST API Reference Manual.

## Get metrics in from collectd

Collectd is an open source daemon that collects performance metrics from a variety of sources. Using the collectd write_http plugin, collectd sends metrics data to a data input in the Splunk platform using the HTTP Event Collector (HEC).

To send metrics using collectd, do the following:

1. Configure the HTTP Event Collector (HEC) data input.
2. Install collectd.
3. Configure collectd.
4. Start collectd.

### Configure the HTTP Event Collector (HEC) data input

The HTTP Event Collector (HEC) is an endpoint that lets you send application events to your deployment of the Splunk platform using the HTTP or Secure HTTP (HTTPS) protocols. Configure this data input before setting up collectd because you'll need to use data input details for the collectd configuration.

1. In Splunk Web, click Settings > Data Inputs.
2. Under Local Inputs, click HTTP Event Collector.
3. Verify that HEC is enabled.
a Click Global Settings.
b For All Tokens, click Enabled if this button is not already selected.
c Note the value for HTTP Port Number, which you'll need to configure collectd.
d Click Save.
4. Configure an HEC token for sending data by clicking New Token.
5. On the Select Source page, for Name, enter a token name, for example "collectd token".
6. Leave the other options blank or unselected.
7. Click Next.
8. On the Input Settings page, for Source type, click Select.
9. Click Select Source Type, then select Metrics > collectd_http.
10. Next to Default Index, select your metrics index, or click Create a new index to create one.
If you choose to create an index, in the New Index dialog box:
Enter an Index Name. User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. Index names cannot begin with an underscore or hyphen.
For Index Data Type, click Metrics.
Configure additional index properties as needed.
Click Save.
11. Click Review, and then click Submit.
12. Copy the Token Value that is displayed, which you'll need to configure collectd.

### Add collectd events directly to a metrics index

To test your data input, you can send collectd events directly to your metrics index using the /collector/raw REST API endpoint, which accepts data in the collectd JSON format. Your metrics index is assigned to an HEC data input that has its unique HEC token, and "collectd_http" as its source type.

The following example shows a curl command that sends a collectd event to the index associated with your HEC token:

```properties
curl -k https://localhost:8088/services/collector/raw?sourcetype=collectd_http   \
-H "Authorization: Splunk <HEC_token>"                                      \
-d '[{"values":[164.9196798931339196],"dstypes":["derive"],"dsnames":["value"],"time":1505356687.894,"interval":10.000,"host":"collectd","plugin":"protocols","plugin_instance":"IpExt","type":"protocol_counter","type_instance":"InOctets"}]'
```

You can verify the HEC data input is working by running a search using mcatalog to list all metric names, with the time range set to "All Time", for example:

```spl
| mcatalog values(metric_name) WHERE index=<your_metrics_index> AND metric_name=protocols.protocol_counter.InOctets.value
```

Or, use the Metrics Catalog REST endpoint to list metric names:

```properties
curl -k -u <admin:passwd> "https://localhost:8089/services/catalog/metricstore/metrics?earliest=0"
```

For more information about using HEC, see the following topics in Getting Data In:

Set up and use HTTP Event Collector in Splunk Web
Format events for the HTTP Event Collector
Send metrics to a metrics index
See mstats and mcatalog in the Search Reference manual.

See the following topics in the REST API Reference Manual:

- Metrics Catalog endpoint descriptions
- /collector
- /collector/raw

### Install collectd

Install the collectd agent on the computers in your system from which you want to collect metrics.

1. Go to the First steps page on the collectd website.
2. Install collectd version 5.6 or higher, following the installation instructions for your operating system.

### Configure collectd

The collectd server is an optional daemon that can be used to aggregate metrics from different inputs and one-to-many collectd clients.

Configure the collectd client to collect data by configuring plugins in the collectd.conf configuration file. The location of the collectd.conf file depends on your operating system. For details, see "Configuration" on the First steps page on the collectd website.

#### The write_http plugin

The write_http plugin requires the following fields from your HEC data input:

<table>
<tr><td>Field name</td><td>Description</td><td>Syntax</td><td>Example</td></tr>
<tr><td>URL</td><td>URL to which the values are submitted. This URL includes your Splunk host machine (IP address, host name, or load balancer name), and the HTTP port number.</td><td>URL "https://<Splunk_host>:<HTTP_port>/services/collector/raw"</td><td>URL "https://10.66.104.127:8088/services/collector/raw"</td></tr>
<tr><td>Header</td><td>An HTTP header to add to the request.</td><td>Header "Authorization: Splunk <HEC_token>"</td><td>Header "Authorization: Splunk b0221cd8-c4b4-465a-9a3c-273e3a75aa29"</td></tr>
<tr><td>Format</td><td>The format of the data.</td><td>Format "JSON"</td><td>Format "JSON"</td></tr>
</table>

### Enable and configure plugins

Enable each plugin below by uncommenting the plugin's LoadPlugin statement, then configure the plugin as described. Most of these plugins are for gathering basic OS-level metrics. The logfile plugin is needed for debugging purposes. You can configure additional plugins according to your requirements.

>> You might need to install some plugins separately, depending on your installation method and operating system. For details, see the collectd website.

#### cpu

```xml
LoadPlugin cpu
<Plugin cpu>
  ReportByCpu true
</Plugin>
```

#### interface

```xml
LoadPlugin interface
Use the default configuration.
```

#### load

```xml
LoadPlugin load
<Plugin load>
    ReportRelative true
</Plugin>
```

#### logfile

```xml
LoadPlugin logfile
<Plugin logfile>
    LogLevel info
    File STDOUT
    Timestamp true
    PrintSeverity false
</Plugin>
```

#### memory

```xml
LoadPlugin memory
<Plugin memory>
    ValuesAbsolute true
    ValuesPercentage true
</Plugin>
```

#### network

```xml
LoadPlugin network
```

Enable this plugin only if the collectd client is not on the same machine as the connectd server, then use the default configuration.

#### syslog

```xml
LoadPlugin syslog
```

Use the default configuration.

#### write_http
You need the values from your HEC data input to configure this plugin.

```xml
LoadPlugin write_http
<Plugin write_http>
    <Node "node1">
        URL "https://<Splunk_host>:<HTTP_port>/services/collector/raw"
        Header "Authorization: Splunk <HEC_token>"
        Format "JSON"
        VerifyPeer false
        VerifyHost false
        Metrics true
        StoreRates true
    </Node>
</Plugin>
```

### Start collectd

To start collectd, follow the instructions under "Starting the daemon" on the First steps page on the collectd website.

Modules for all of the enabled plugins in your collectd.conf file must be installed. Errors are displayed for any modules that are missing. For more about the available collectd plugins, see Table of Plugins on the collectd Wiki website.

Install modules according to your operating system. For example, on Linux you must install collectd-write_http.x86_64 to use the write_http plugin.

Tips:

- For troubleshooting, refer to the collectd log file enabled by the logfile plugin for details.
- Use the File setting in the logfile plugin to write to a specified file rather than to standard output. For example:

```xml
<Plugin logfile>
    LogLevel info
    File "/var/log/collectd.log"
    Timestamp true
    PrintSeverity false
</Plugin>
```

- If you are installing collectd on Linux, you can use yum to list available modules. For example, use this CLI command:

```bash
yum list | grep collectd
```

- In the collectd.conf file, set the FQDNLookup setting to false to render a friendly name for the domain name.

## Get metrics in from other sources

If you are gathering metrics from a source that is not natively supported, you can still add this metrics data to a metrics index.

### Get metrics in from files in CSV format

There are two accepted formats for CSV files when you use them as inputs for metrics data. The format you use depends on how you want the Splunk software to index the information in the CSV file. Should it index so that each data point has multiple measurements, or so that each data point has only one measurement?

It is more efficient to use metric data points that can contain multiple measurements. When you index metrics data this way you reduce your data storage costs and can benefit from improved search performance.

#### Set metrics CSV source types and data inputs

If your metrics data is in CSV format, use the metrics_csv pre-trained source type. It can handle both CSV metrics formats.

Create a data input to add your CSV data to a metrics index. The input uses the pretrained metrics_csv source type. The data input should have:

- Source type: Metrics > metrics_csv
- Index: a metrics index

After you set up your metrics_csv input, you should have the following inputs.conf configuration on your universal forwarder. It monitors the CSV data and sends it to the metrics indexer.

```properties
#inputs.conf
[monitor:///opt/metrics_data]
index = metrics
sourcetype = metrics_csv
```

You should also have the following indexes.conf configuration on the metrics indexer:

```properties
#indexes.conf
[metrics]
homePath = $SPLUNK_DB/metrics/db
coldPath = $SPLUNK_DB/metrics/colddb
thawedPath = $SPLUNK_DB/metrics/thaweddb
datatype = metric
maxTotalDataSizeMB = 512000
```

See Monitor files and directories in the Getting Data In manual, and Create metrics indexes in the Managing Indexers and Clusters of Indexers manual.

#### Format a CSV file for multiple-measurement metric data points

When you format a CSV file for multiple-measurement metric data points, the first column header is _time, the metric timestamp. It is a required field.

This is followed by one or more column headers for each metric measurement. Each measurement column header follows this syntax: metric_name:<metric_name>.

The Splunk software considers additional columns that are not a timestamp or a measurement to be dimensions.

Each row of the CSV table is a separate metric data point.

<table>
<tr><td>Field name</td><td>Required</td><td>Description</td><td>Example</td></tr>
<tr><td>_time</td><td>Yes</td><td>The metric timestamp. The format is epoch time (elapsed time since 1/1/1970), in milliseconds.</td><td>1504907933.000</td></tr>
<tr><td>metric_name:&lt;metric_name&gt;</td><td>Yes</td><td>A measurement for a specific metric, as specified by <metric_name>, such as metric_name:os.cpu.idle or metric_name:max.size.kpbs. Their values are always numeric.</td><td>13.34</td></tr>
<tr><td>dimensions</td><td>No</td><td>All other fields are treated as dimensions.</td><td>For a dimension named ip, a value of 192.0.2.1.</td></tr>
</table>

Here is an example of a CSV file that is formatted for multiple-measurement metric data points. The first column is _time, the metric timestamp. The middle three columns are measurements. The last two columns are dimensions.

```csv
"_time","metric_name:cpu.usr","metric_name:cpu.sys","metric_name:cpu.idle","dc","host"
"1562020701",11.12,12.23,13.34,"east","east.splunk.com"
"1562020702",21.12,22.33,23.34,"west","west.splunk.com"
```

This CSV file example contains the same information as the example CSV file for single-measurement metric data points in the following section. However, because it uses two data points for this information instead of six, it will take up less space on disk when it is indexed.

#### Format a CSV file for single-measurement metric data points

When you format a CSV file for single-measurement metric data points, the first three columns are fields that are required for single-measurement metric data points:

- metric_timestamp
- metric_name
- _value.

All additional columns are considered to be dimensions.

During the ingestion and indexing process the metric_name and _value measurements will merged into the metric_name:<metric_name>=<numeric_value> format.

<table>
<tr><td>Field name</td><td>Required</td><td>Description</td><td>Example</td></tr>
<tr><td>metric_timestamp</td><td>Yes</td><td>The timestamp format is epoch time (elapsed time since 1/1/1970), in milliseconds.</td><td>1504907933.000</td></tr>
<tr><td>metric_name</td><td>Yes</td><td>The metric name using dotted-string notation.</td><td>os.cpu.percent</td></tr>
<tr><td>_value</td><td>Yes</td><td>The numerical value associated with the metric_name.</td><td>42.12345</td></tr>
<tr><td>dimensions</td><td>No</td><td>All other fields are treated as dimensions.</td><td>ip</td></tr>
</table>

Here is an example of a CSV file that is formatted for single-measurement metric data points. The first three columns of the table are the fields that are required for single-measurement metric data points. All additional columns are dimensions. This CSV file has dc and host as dimensions.

```csv
"metric_timestamp","metric_name","_value","dc","host"
"1562020701","cpu.usr",11.12,"east","east.splunk.com"
"1562020701","cpu.sys",12.23,"east","east.splunk.com"
"1562020701","cpu.idle",13.34,"east","east.splunk.com"
"1562020702","cpu.usr",21.12,"west","west.splunk.com"
"1562020702","cpu.sys",22.33,"west","west.splunk.com"
"1562020702","cpu.idle",23.34,"west","west.splunk.com"
```

If you compare this example to the example for multiple-measurement metric data points, you can see how the single-metric format would take up more space on disk. This table contains the same information as the multiple-measurement table. However, this table uses six data points where the multiple-measurement table only uses two.

### Get metrics in from clients over TCP/UDP

You can add metrics data from a client that is not natively supported to a metrics index by manually configuring a source type for your data, then defining regular expressions to specify how the Splunk software should extract the required metrics fields. See Metrics data format.

For example, let's say you are using Graphite. The Graphite plaintext protocol format is:

<metric path> <metric value> <metric timestamp>
A sample metric might be:

510fcbb8f755.sda2.diskio.read_time 250 1487747370
To index these metrics, edit Splunk configuration files to manually specify how to extract fields.

#### Configure field extraction by editing configuration files

1. Define a custom source type for your metrics data.
In a text editor, open the props.conf configuration file from the local directory for the location you want to use, such as the Search & Reporting app ($SPLUNK_HOME/etc/apps/search/local/) or the system ($SPLUNK_HOME/etc/system/local). If a props.conf file does not exist in this location, create a text file and save it to that location.
Append a stanza to the props.conf file as follows:

```properties
# props.conf
[<metrics_sourcetype_name>]
TIME_PREFIX = <regular expression>
TIME_FORMAT = <strptime-style format>
TRANSFORMS-<class> = <transform_stanza_name>
NO_BINARY_CHECK = true
SHOULD_LINEMERGE = false
pulldown_type = 1
category = Metrics
```

- metrics_sourcetype_name Name of your custom metrics source type.
- TIME_PREFIX = regular expression: A regular expression that indicates where the timestamp is located.
- TIME_FORMAT = strptime-style format: A strptime format string used to extract the date. For more about strptime, see Configure timestamp recognition in the Getting Data In manual.
- TRANSFORMS-<class> = <transform_stanza_name>: class is a unique literal string that identifies the namespace of the field to extract. transform_stanza_name is the name of the name of your stanza in transforms.conf that indicates how to extract the field.

2. Define a regular expression for each metrics field to extract.
In a text editor, open the transforms.conf configuration file from the local directory for the location you want to use, such as the Search & Reporting app ($SPLUNK_HOME/etc/apps/search/local/) or the system ($SPLUNK_HOME/etc/system/local). If a transforms.conf file does not exist in this location, create a text file and save it to that location.
Append a stanza for each regular expression as follows:

```properties
# transforms.conf

[<transform_stanza_name>]
REGEX = <regular expression>
FORMAT = <string>
WRITE_META = true
```

- transform_stanza_name: A unique name for this stanza.
- REGEX = <regular expression>: A regular expression that defines how to match and extract metrics fields from this metrics data.
- FORMAT = <string>: A string that specifies the format of the metrics event.

3. Create a data input for this source type as described in Set up a data input for StatsD data, and select your custom source type.

For more about editing these configuration files, see About configuration files, props.conf, and transforms.conf in the Admin Manual.

#### Example of configuring field extraction

This example shows how to create a custom source type and regular expressions to extract fields from Graphite metrics data.

```propeties
# props.conf.example
[graphite_plaintext]
TIME_PREFIX = \s(\d{0,10})$
TIME_FORMAT =  %s
NO_BINARY_CHECK = true
SHOULD_LINEMERGE = false
pulldown_type = 1
TRANSFORMS-graphite-host = graphite_host
TRANSFORMS-graphite-metricname = graphite_metric_name
TRANSFORMS-graphite-metricvalue = graphite_metric_value
category = Metrics
```

```properties
# transforms.conf.example
[graphite_host]
REGEX = ^(\S[^\.]+)
FORMAT = host::$1
DEST_KEY = MetaData:Host

[graphite_metric_name]
REGEX = \.(\S+)
FORMAT = metric_name::graphite.$1
WRITE_META = true

[graphite_metric_value]
REGEX = \w+\s+(\d+.?\d+)\s+
FORMAT = _value::$1
WRITE_META = true
```

### Get metrics in from clients over HTTP or HTTPS

If you want to send metrics data in JSON format from a client that is not natively supported to a metrics index over HTTP or HTTPS, use the HTTP Event Collector (HEC) and the /collector REST API endpoint.

#### Create a data input and token for HEC

1. In Splunk Web, click Settings > Data Inputs.
2. Under Local Inputs, click HTTP Event Collector.
3. Verify that HEC is enabled.
Click Global Settings.
For All Tokens, click Enabled if this button is not already selected.
Click Save.
4. Configure an HEC token for sending data by clicking New Token.
5. On the Select Source page, for Name, enter a token name, for example "Metrics token".
6. Leave the other options blank or unselected.
7. Click Next.
8. On the Input Settings page, for Source type, click New.
9. In Source Type, type a name for your new source type.
10. For Source Type Category, select Metrics.
11. Optionally, in Source Type Description type a description.
12. Next to Default Index, select your metrics index, or click Create a new index to create one.
If you choose to create an index, in the New Index dialog box:
Enter an Index Name.
For Index Data Type, click Metrics.
Configure additional index properties as needed.
Click Save.
13. Click Review, and then click Submit.
14. Copy the Token Value that is displayed. This HEC token is required for sending data.

See Getting data in with HTTP Event Collector on the Splunk Developer Portal.

#### Send data to a metrics index over HTTP

Use the /collector REST API endpoint and your HEC token to send data directly to a metrics index as follows:

```properties
http://<Splunk_host>:<HTTP_port>/services/collector \
-H "Authorization: Splunk <HEC_token>"                                                \
-d "<metrics_data>"
```

You need to provide the following values:

- Splunk host machine (IP address, host name, or load balancer name)
- HTTP port number
- HEC token value
- Metrics data, which requires an "event" field set to "metric".

For more about HEC, see Getting data in with HTTP Event Collector and Event formatting on the Splunk Developer Portal.

For more about the /collector endpoint, see /collector in the REST API Reference Manual.

#### Example of sending metrics using HEC

The following example shows a command that sends a metric data point to a metrics index, with the following values:

- Splunk host machine: "localhost"
- HTTP port number: "8088"
- HEC token value: "b0221cd8-c4b4-465a-9a3c-273e3a75aa29"

```properties
curl -k https://localhost:8088/services/collector                     \
-H "Authorization: Splunk b0221cd8-c4b4-465a-9a3c-273e3a75aa29"       \
-d '{"time": 1486683865.000,"event":"metric","source":"disk","host":"host_1.splunk.com","fields":{"region":"us-west-1","datacenter":"dc1","rack":"63","os":"Ubuntu16.10","arch":"x64","team":"LON","service":"6","service_version":"0","service_environment":"test","path":"/dev/sda1","fstype":"ext3","metric_name:cpu.usr": 11.12,"metric_name:cpu.sys": 12.23, "metric_name:cpu.idle": 13.34}}'
```

The measurements for this metric data point appear at the end of the JSON blob. They follow a multiple-metric format that uses the "metric_name:<metric_name>":<numeric_value> syntax.

#### The multiple-metric JSON format

Versions of the Splunk platform previous to 8.0.0 used a JSON format that only supported one metric measurement per JSON object. This resulted in metric data points that could only contain one measurement at a time.

Version 8.0.0 of the Splunk platform supports a JSON format which allows each JSON object to contain measurements for multiple metrics. These JSON objects generate multiple-measurement metric data points. Multiple-measurement metric data points take up less space on disk and can improve search performance.

Here is an example of a JSON object in the multiple-metric format.

```json
{
  "time": 1486683865,
  "source": "metrics",
  "sourcetype": "perflog",
  "host": "host_1.splunk.com",
  "fields": {
    "region": "us-west-1",
    "datacenter": "dc2",
    "rack": "63",
    "os": "Ubuntu16.10",
    "arch": "x64",
    "team": "LON",
    "service": "6",
    "service_version": "0",
    "service_environment": "test",
    "path": "/dev/sda1",
    "fstype": "ext3"
    "metric_name:cpu.usr": 11.12,
    "metric_name:cpu.sys": 12.23,
    "metric_name:cpu.idle": 13.34
  }
}
```
