
About proactive Splunk component monitoring

Proactive Splunk component monitoring lets you view the health of Splunk Enterprise features from the output of a REST API endpoint. Individual features report their health status through a tree structure that provides a continuous, real-time view of the health of your deployment. 

You can access feature health information using the splunkd health report in Splunk Web. See View the splunkd health report. 

You can also access feature health information programmatically from the server/health/splunkd endpoint. See Query the server/health/splunkd endpoint. 

For more information on the splunkd process, see Splunk Enterprise Processes. 


How it works

Proactive Splunk component monitoring records the health status of splunkd in a tree structure, where leaf nodes represent particular Splunk Enterprise features, and intermediary nodes categorize the various features. Feature health status is color-coded in three states: 

• Green: The feature is functioning properly. 


• Yellow: The feature is experiencing a problem. 


• Red: The feature has a severe issue and is negatively impacting the functionality of your deployment. 


The screen image shows the splunkd status tree. The tree displays the health status color (green, yellow, or red) for each Splunk Enterprise feature. 

The health status tree structure

The splunkd health status tree has the following nodes: 


Health status tree node 

Description 

splunkd  The top level node of the status tree shows the overall health status (color) of splunkd. The status of splunkd shows the least healthy state present in the tree. The REST endpoint retrieves the instance health from the splunkd node.  
Feature categories  Feature categories represent the second level in the health status tree. Feature categories are logical groupings of features. For example, "BatchReader" and "TailReader" are features that form a logical grouping with the name "File Monitor Input". Feature categories act as buckets for groups of features, and do not have their own health status.  
Features  The next level in the status tree is feature nodes. Each node contains information on the health status of a particular feature. Each feature contains one or more indicators that determine the status of the feature. The overall health status of a feature is based on the least healthy color of any of its indicators.  
Indicators  Indicators are the fundamental elements of the splunkd health report. These are the lowest levels of functionality that are tracked by each feature, and change colors as functionality changes. Indicator values are measured against red or yellow threshold values to determine the status of the feature. See What determines the status of a feature?  

For a list of supported Splunk Enterprise features, see Supported features. 

What determines the status of a feature?

The current status of a feature in the status tree depends on the value of its associated indicators. Indicators have configurable thresholds for yellow and red. When an indicator's value meets threshold conditions, the feature's status changes. 

For information on how to configure indicator thresholds, see Set feature indicator thresholds. 

For information on how to troubleshoot the root cause of feature status changes, see Investigate feature health status changes. 

Default health status alerts

By default, each feature in the splunkd health report generates an alert when a status change occurs, for example from green to yellow, or yellow to red. You can enable/disable alerts for any feature and set up alert notifications via email, PagerDuty, or web hook in health.conf or via REST enpoint. For more information, see Configure health report alerts. 


Health status viewpoint

The splunkd status tree shows the health of your Splunk Enterprise deployment from the viewpoint of the instance type that you are monitoring. For example, in an indexer cluster environment, the cluster master and peer nodes each show a different set of features contributing to the overall health of splunkd. 



Requirements

Proactive Splunk component monitoring has the following requirements and limitations. 


Splunk Enterprise version requirements

Proactive Splunk component monitoring requires Splunk Enterprise version 7.1 or later. 


Operating system requirements

Proactive Splunk component monitoring is available on all operating systems supported for Splunk Enterprise. For a list of supported operating systems, see System requirements in the Installation Manual. 


REST endpoint access requirements

Proactive Splunk component monitoring displays information from the server/health/splunkd endpoint. You can view this information in the splunkd health report in Splunk Web. 

To query the server/health/splunkd endpoint directly requires access to the splunkd management port (default port 8089) over http. For more information, see Connecting to splunkd in the REST API User Manual. 

For endpoint details, see server/health/splunkd in the REST API Reference Manual. 



Configure the splunkd health report

The splunkd health report displays the status of a pre-defined set of Splunk Enterprise features. You can modify some health report settings, including feature thresholds, using the health report manager page in Splunk Web, or by editing health.conf. 

For more information on health report configuration settings in health.conf, see health.conf.spec in the Admin Manual. 


Supported features

The splunkd health report lets you monitor these Splunk Enterprise features: 


Feature Category 

Features 

Data Forwarding / Splunk-2-Splunk Forwarding  TCPOutAutoLB  
File Monitor Input  BatchReader, TailReader  
Index Processor  Buckets, Disk Space, Index Optimization  
Indexer Clustering  Cluster Bundles, Data Durability, Data Searchable, Indexers, Indexing Ready, Master Connectivity, Replication Failures, Slave State, Slave Version, Search Head Connectivity  
Search Head Clustering  Member to Captain Connection, Captain Common Baseline, Captain Election Overview, Members Overview, Snapshot Creation  

For more information on the above features, see $SPLUNK_HOME/etc/system/default/health.conf. 


Set feature indicator thresholds

Each feature in the health status tree has one or more indicators. Each indicator reports a value against a pre-set threshold, which determines the status of the feature. When the indicator value meets the threshold condition, the health status of the feature changes, for example, from green to yellow, or yellow to red. 

There are two valid thresholds for each indicator: yellow and red. You can modify threshold values for any feature using Splunk Web or health.conf. 

Set thresholds using Splunk Web

To set feature threshold values in Splunk Web: 

1. Log in to Splunk Web on the instance you are monitoring. 


2. Click Setttings > Health report manager. 


3. For the feature you want to modify, click Edit Thresholds.

Health report manager.png




4. Set new indicator threshold values. For example, to modify thresholds for the Batch Reader feature, set new Red or Yellow threshold values for the data_out_rate indicator:

Edit threshold modal.png




5. Click Save. 


To view and edit threshold settings on the health report manager page, your role must be assigned list_health and edit_health capabilities. For more information, see Set access controls for the splunkd health report.

Set thresholds using health.conf

To set feature threshold values in health.conf: 

1.Log in to the instance you are monitoring. 


2.Edit $SPLUNK_HOME/etc/system/local/health.conf 


3.In the feature stanza, set new indicator threshold values. For example, to modify indicator threshold values for the batchreader feature, set new values for data_out_rate:yellow and data_out_rate:red thresholds in the following stanza: 
[feature:batchreader]
indicator:data_out_rate:red = 10
indicator:data_out_rate:yellow = 5


Indicator thresholds are pre-set to values that apply to most use cases. When you modify threshold values, make changes in small increments. Setting threshold values too high can mask serious problems or failures.


For detailed descriptions of each feature indicator, see $SPLUNK_HOME/etc/system/default/health.conf. 


Disable a feature

You can disable any feature in health.conf. Disabling a feature removes that feature from the splunkd health status tree. This is useful, for example, if you want to exclude a feature's status from the health report, while you troubleshoot a problem with that feature. All supported features are enabled by default in health.conf. 

There are three ways to disable a feature: 

• Disable the feature in Splunk Web.


• Edit the feature stanza in health.conf.


• Use the /server/health-config endpoint.


Disable a feature in Splunk Web

1. Log in to Splunk Web on the instance you are monitoring. 


2. Click Settings > Health report manager. 


3. Set the switch to disable for the particular feature.
 The feature is disabled and no longer impacts the overall health status of splunkd. 


Disable a feature in health.conf

1. Log in to the instance you are monitoring. 


2. Edit $SPLUNK_HOME/etc/system/local/health.conf. 


3. In the feature stanza, add disabled = 1. For example, to disable the Data Durability feature: 
[feature:data_durability]
indicator:cluster_replication_factor:red = 1
indicator:cluster_search_factor:red = 1
disabled = 1


To enable a feature, set disabled = 0


4. Reload health.conf: 
curl -k -u admin:pass https://<host>:<mPort>/services/configs/conf-health.conf/_reload



Disable a feature using REST endpoint

1.Log in to the instance your are monitoring. 


2. Run the following command against the server/health-config/{feature_name} endpoint. For example, to disable the batchreader feature: 
curl -k -u admin:pass \ 
https://<host>:<mPort>/services/server/health-config/feature:batchreader -d disabled=1



3. Validate the feature no longer appears in the splunkd status report in Splunk Web. 


For endpoint details, see server/health-config/{feature_name} in the REST API Reference Manual. 

To access server/health-config/ endpoints, your role must have the edit_health capability.


Suppress health status updates

Features in the health status tree update their status at predetermined intervals. A feature whose health status changes frequently can cause excessive undesirable changes to the overall status of the splunkd health report. To prevent this, use the suppress_status_update_ms attribute in health.conf to reduce the frequency with which a particular feature can update its health status. 

Use the suppress_health_status_update_ms attribute to: 

• Limit excessive changes to the internal state by individual features. 


• Reduce the number of log entries that arise from rapid feature status changes.


• Help quiet "noisy" features.


For example, an indexer clustering feature, such as data_durability, can experience frequent status changes during operations that impact its indicators: cluster_replication_factor and cluster_search_factor. To avoid frequent changes to the overall splunkd health report, you might set suppress_status_update_ms = 60000 to reduce health status updates to once every minute. 

To suppress health status updates: 

1. Log in to the instance you are monitoring. 


2. Edit $SPLUNK_HOME/etc/system/local/health.conf 


3. In the appropriate feature stanza, add the suppress_status_update_ms attribute. For example: 
[feature:data_durability]
indicator:cluster_replication_factor:red = 1
indicator:cluster_search_factor:red = 1
suppress_status_update_ms = 60000


By default, the minimum amount of time that must elapse between status updates is 300ms.


For more information, see health.conf.spec in the Admin Manual. 


Configure health status logs

Each feature in the splunkd health status tree generates log entries in health.log. These log entries record information about feature indicator status changes over time. health.log is located in SPLUNK_HOME/var/log/splunk/. 

There are two types of health.log log entries: 

HealthChangeReporter: This log entry records specific health status changes for a feature indicator. Each entry includes a timestamp, feature name, indicator name, previous color, new color, and a possible reason for the status change. This log entry appears only if a feature's status changes, for example, from green to red: 

02-28-2018 20:26:52.775 +0000 INFO  HealthChangeReporter - feature="Data Durability" indicator="cluster_replication_factor" previous_color=green color=red reason="Replication Factor is not met"


PeriodicHealthReporter: This log entry keeps an ongoing record of the status of each feature in the health status tree. Each entry includes a timestamp, the feature name, and current color. Log entries are made at a user-configurable interval. For example: 

02-28-2018 20:27:06.826 +0000 INFO  PeriodicHealthReporter - feature="Data Durability" color=red


Set health.log entry intervals

You can set the interval at which PeriodicHealthReporter log entries are added to health.log. This is useful if you want to increase or decrease the overall number of log entries that appear in health.log. 

To adjust the frequency of PeriodicHealthReporter log entries in health.log: 

1. Log in to the instance you are monitoring. 


2. Edit $SPLUNK_HOME/etc/system/local/health.conf 


3. In the [health_reporter] stanza, set the full_health_log_interval attribute to an appropriate value in seconds. For example: 
[health_reporter]
full_health_log_interval = 60


By default, each feature generates a PeriodicHealthReporter log entry every 30 seconds.



Set up alerts for the splunkd health report

The splunkd health report generates alerts for all features in the health status tree. When a feature indicator meets the threshold condition, the feature's health status changes, for example from green to red, and an alert fires. Use health status alerts to maintain visibility into the health of your deployment, whether or not you are logged into Splunk Web. 

You can configure health report alerts as follows: 

• Enable/disable alerts on the global, feature, or indicator level. 


• Send alert notifications via email or PagerDuty.


• Set the health status color (yellow or red) that triggers an alert. 


• Set a minimum duration that must elapse between alerts.


You can configure health report alerts by directly editing health.conf or querying the server/health-config endpoint. 


Disable health report alerts

Alerts are enabled by default for all features in the splunkd health report. You can disable alerts at the global, feature, or indicator level. Disabling alerts at the global level overrides enabled alerts at the feature level. Likewise, disabling alerts at the feature level overrides enabled alerts at the indicator level. 

Disabling alerts is useful for reducing noise from non-critical features and minimizing false positives when performing maintenance tasks. 

Disable alerts using health.conf

To disable alerts for all features in the splunkd health report: 

1. Edit $SPLUNK_HOME/etc/system/local/health.conf.


2. In the health_reporter stanza, set alert.disabled = 1. For example: 
[health_reporter]
full_health_log_interveral = 30
suppress_status_update = 600
alert.disabled = 1


To enable alerts for all features in the splunkd health report, set alert.disabled = 0.


To disable alerts for a single feature: 

1. Edit $SPLUNK_HOME/etc/system/local/health.conf.


2. In the stanza for the particular feature, set alert.disabled = 1. For example: 
[feature:indexers]
...
indicator:missing_peers:yellow = 1
indicator:missing_peers:red = 1
alert.disabled = 1


To enable alerts for a feature, set alert.disabled = 0.


To disable alerts for a single feature indicator: 

1. Edit $SPLUNK_HOME/etc/system/local/health.conf.


2. In the stanza for a particular feature, set alert:<indicator_name>.disabled = 1. For example, in the following stanza, alerting for the indicator s2s_connections is disabled: 
[feature:s2s_autolb]
...
indicator:s2s_connections:yellow = 20
indicator:s2s_connections:red = 70
alert:s2s_connections.disabled = 1


To enable alerts for an indicator, set alert:.disabled = 0.


Disable alerts using REST endpoint

To disable alerts for features and indicators, send a POST request to server/health-config/{feature_name}. For example, to disable alerts for the batchreader feature on the instance you are monitoring run the following command: 

curl -k -u admin:pass https://<host>:<mPort>/services/server/health-config/feature:batchreader -d alert.disabled=1


For endpoint details, see server/health-config/{feature_name} in the REST API Reference Manual. 

To access server/health-config endpoints, a role must have the edit_health capability.


Set up health report alert actions

You can set up alert actions that run when an alert fires, such as sending alert notifications via email or PagerDuty. 

Alert actions apply on the global level only. Multiple alert actions for the same action type are not supported. For example, you cannot have multiple email actions and multiple PagerDuty actions. 

Before you can send health email alert notifications, you must configure email notification settings in Splunk Web. For instructions, see Email notification action in the ''Alerting Manual''.

Set up email notifications in health.conf

To set up email alert notifications: 

1. Edit SPLUNK_HOME/etc/system/local/health.conf 


2. Add the [alert_action:email] stanza and specify the recipients. For example: 
[alert_action:email] 
disabled = 0
action.to =  <recipient@example.com>
action.cc = <recipient_2@example.com>
action.bcc = <other_recipients@example.com>



Set up PagerDuty notifications in health.conf

Before you can send alert notifications to PagerDuty, you must install the  PagerDuty App from Splunkbase. You must also add a new service to your PagerDuty integration, and copy the integration key. For more information, see PagerDuty documentation.

To set up PagerDuty alert notifications: 

1.Edit $SPLUNK_HOME/etc/system/local/health.conf. 


2. Add the [alert_action:pagerduty] stanza and specify the integration key. For example: 
[alert_action:pagerduty]
disabled = 0
action.integration_url_override = <integration key>



For more information, see health.conf.example. 

Set up alert notifications using REST

To set up alert notifications, send a POST request to server/health-config/{alert_action}. For example, to set up an email alert notification: 

curl -k -u admin:pass https://localhost:8089/services/server/health-config/alert_action:email -d action.to=admin@example.com -d action.cc=admin2@example.com


For endpoint details, see server/health-config/{alert_action} in the REST API Reference Manual. 


Set the alert threshold color

You can set the threshold color that triggers an alert. Possible alert threshold values are yellow or red. If the threshold value is yellow, an alert fires for both yellow and red. If the value is red, an alert fires for red only. The default alert threshold value is red. 

Set the alert threshold color in health.conf

To set the alert threshold color on the global or feature level: 

1. Edit $SPLUNK_HOME/etc/system/local/health.conf. 


2. Add the alert.threshold_color setting to the [health_reporter] or [feature:<feature_name>] stanza. For example: 
[feature:replication_failures]
...
alert.threshold_color = yellow
indicator:replication_failures:red = 10
indicator:replication_failures:yellow = 5



To set the alert threshold color on the indicator level: 

1. Edit $SPLUNK_HOME/etc/system/local/health.conf. 


2. Add the <code>alert:<indicator name>.threshold_color setting to the feature stanza. For example: 
[feature:replication_failures]
...
indicator:replication_failures:red = 10
indicator:replication_failures:yellow = 5
alert:replication_failures.threshold_color = yellow



Alert threshold color settings at the indicator level override alert threshold color settings at the feature level.

Set the alert threshold color using REST

To set the alert threshold color for a feature or indicator, send a POST request to server/health-config{feature_name}. For example, to set the alert threshold color for the Replication Failures feature: 

curl -k -u admin:pass https://localhost:8089/services/server/health-config/feature:replication_failures -d alert.threshold_color=yellow


For endpoint details, see server/health-config/{feature_name} in the REST API Reference Manual. 


Set minimum duration between alerts

You can set the amount of time an unhealthy health status persists before an alert fires using the alert.min_duration_sec setting. You can use this setting to help reduce noise from feature health status changes that might be rapidly flipping between states, for example, between green and yellow or yellow and red. 

Set minimum duration between alerts in health.conf

To set the minimum duration between alerts on the global or feature level: 

1. Edit $SPLUNK_HOME/etc/system/local/health.conf. 


2. Add the alert.min_duration_sec setting to the [health_reporter] or [feature:<feature_name>] stanza. For example: 
[feature:replication_failures]
...
alert.min_duration_sec = 600
indicator:replication_failures:red = 10
indicator:replication_failures:yellow = 5



To set the minimum duration between alerts on the indicator level: 

1. Edit $SPLUNK_HOME/etc/system/local/health.conf. 


2. Add the <code>alert:<indicator name>.min_duration_sec setting to the [feature:<feature_name>] stanza. For example: 
[feature:replication_failures]
...
indicator:replication_failures:red = 10
indicator:replication_failures:yellow = 5
alert:replication_factor.min_duration_sec = 600



Minimum duration between alerts settings on the feature level override settings on the indicator level.

Set minimum duration between alerts using REST

To set the minimum duration between alerts for a feature or indicator, send a POST request to server/health-config/{feature_name}. For example, to set the minimum duration between alerts for the Replication Failures feature: 

curl -k -u admin:pass https://localhost:8089/services/server/health-config/feature:replication_failures -d alert.min_duration_sec=600


For endpoint details, see server/health-config/{feature_name} in the REST API Reference Manual. 



Set access controls for the splunkd health report

To monitor, edit, and enable splunkd health report features, the user's role must be assigned the appropriate capabilities. The following health report capabilities are enabled by default for role_admin: 


Capability 

Permissions granted to role 

list_health  View the splunkd health report and health manager page in Splunk Web.  
edit_health  Edit and enable health report features in Splunk Web or through REST endpoint.  

You can add the above capabilities to any role in Splunk Web or in authorize.conf. 

To add a capability to a role in Splunk Web, see Add and edit roles with Splunk Web. 

To add a capability to a role in authorize.conf, see Add and edit roles with authorize.conf. 



Investigate feature health status changes

There are two ways to access feature health status information from the /server/health/splunkd endpoint: 

• View the splunkd health report in Splunk Web.


• Query the server/health/splunkd endpoint.


You can also monitor feature health status changes in $SPLUNK_HOME/var/log/health.log. For more information on health.log file entries, see Configure health status logs. 


View the splunkd health report

The splunkd health report lets you view the current status of features in the splunkd health status tree. You can use the report to identify features whose status indicates a problem, and investigate the cause of those problems. 

1. On the instance you want to monitor, log in to Splunk Web. 


2. In the main menu, check the color of the health report icon. The color of this icon indicates the overall status of splunkd. 



3. Click the health report icon to open the health report.
The image shows the splunkd health report. The health report includes the splunkd status tree, and provides detailed diagnostic information about changes in feature health status. 


4.In the health status tree, click on any feature to view information about the feature's status. 


5. For features in the red or yellow state, review Root Cause and Last 50 related messages for information that can help you identify the cause of a feature's status change. 



Example

This example shows how you can use the splunkd health report to investigate feature health status changes on a cluster master instance. 

1. Review the splunkd health report. 


2. Investigate root cause and related messages. 


3. Confirm the cause of feature status change. 


1. Review the splunkd health report

1. On the cluster master instance, log in to Splunk Web. 


2. Check the color of the health report icon.
The image shows a red health report icon in the Splunk Web main menu
 The red icon indicates that one or more features in the splunkd health status tree has a severe issue. 


3.Click the health report icon to open the health report. 
Data durability red.png
 The health report shows the data_durability feature has a severe issue. 


2. Investigate root cause and related messages

In the health status tree, click the data_durability feature. The following diagnostic information appears: 

Root Cause: "Replication Factor is not met."
 If the number of peers in the cluster is less than the replication factor, the replication factor cannot be met. Therefore a possible cause of the feature's red status is an offline peer.

Last 50 Related Messages: Searching the related messages, you see log entries that contain streaming errors. For example: 

09-04-2018 18:33:43.724 +0000 INFO CMMaster - streaming error src=CFEAE46E-A7F5-485E-99B3-9DE293772D88 tgt=88A0C501-5A3B-4BBD-9B32-BBD20C7158C5 failing=target bid=_internal~2~CFEAE46E-A7F5-485E-99B3-9DE293772D88 bytes=2442356


The streaming error suggests that bucket replication is failing because a source peer cannot communicate with a target peer. This type of error can be caused by a network interruption or an offline peer. 

3. Confirm the cause of feature status change

After you use the splunkd health report to investigate the cause of the feature's status change, which suggests a peer is offline, you can use the Monitoring Console to check the status of cluster peers and confirm if the suspected cause is correct. 

1. In Splunk Web, click Settings > Monitoring Console. 


2. Click Indexing > Indexer Clustering > Indexer Clustering: Status.
 The dashboard shows that the peer node idx3 is stopped.
Monitoring console idx3.png



3. Click on the idx3 peer to see more information. The idx3 peer GUID matches the GUID of the target peer in the streaming error message. This confirms that the cause of the data_durability feature's red status is an offline peer.
Idx3 GUID 7.2.png
 You can now take steps to restart the peer and have it rejoin the cluster, which returns the data_durability feature to the green state. 



Query the server/health/splunkd endpoint

You can integrate proactive Splunk component monitoring with existing third-party monitoring tools and other applications. 

To see the overall health status of splunkd, query the server/health/splunkd endpoint. For example: 

curl -k -u admin:pass https://<host>:8089/services/server/health/splunkd


For endpoint details, see server/health/splunkd in the REST API Reference Manual. 

To see the overall health of splunkd, as well as the health of each feature reporting to the status tree, and information on feature health status changes, query server/health/splunkd/details. For example: 

curl -k -u admin:pass https://<host>:8089/services/server/health/splunkd/details


For endpoint details, see server/health/splunkd/details in the REST API Reference Manual. 

For more information on how to use Splunk REST API endpoints, see Connecting to splunkd in the REST API User Manual. 

For information on Splunk SDKs that support the Splunk REST API, see Overview of Splunk SDKs. 







