
Multi-instance deployment monitoring console setup steps

This topic is an overview of the steps to set up your monitoring console for your entire Splunk Enterprise deployment. To configure the monitoring console on a standalone instance, see Single-instance Monitoring Console setup steps in this manual. 

To set up a monitoring console for a distributed deployment, perform the following steps: 


Step number 

Description of step 

How to proceed with this step 

1  Decide which instance will host the monitoring console for your deployment.  See Which instance should host the console?  
2  Ensure your deployment meets the prerequisites.  See Monitoring console setup prerequisites.  
3  Set search head cluster and indexer cluster labels.  See Set cluster labels.  
4  Add all instances as search peers.  See Add instances as search peers to the monitoring console.  
5  Set up the monitoring console in distributed mode.  See Configure the monitoring console in distributed mode.  
6 (optional)  Use the monitoring console forwarder dashboards.  See Configure forwarder monitoring for the Monitoring Console.  
7 (optional)  Enable platform alerts.  See Enable and configure platform alerts.  
8 (optional)  Modify or add Health Check items.  See Customize Health Check.  
9 (optional)  Customize color mappings for the Overview page.  Navigate to Monitoring Console > Settings > Overview Preferences.  

Get started 

To set up your monitoring console for a distributed deployment, first decide where to host it in your deployment. Begin with Which instance should host the console? 



Single-instance Monitoring Console setup steps

This topic is an overview of the steps to set up your monitoring console for a standalone Splunk Enterprise instance. To configure the monitoring console for your entire Splunk Enterprise deployment, see "Multi-instance deployment setup steps." 

To set up a monitoring console for a standalone deployment, perform the following steps: 


Step number 

Description of step 

How to proceed with this step 

1  Ensure your deployment meets the prerequisites.  See Monitoring console setup prerequisites.  
2  Set up the monitoring console in standalone mode.  See Configure Monitoring Console in standalone mode.  
3 (optional)  Use the monitoring console forwarder dashboards.  See Configure forwarder monitoring for the Monitoring Console.  
4 (optional)  Enable platform alerts.  See Enable and configure platform alerts.  
5 (optional)  Modify or add Health Check items.  See Customize Health Check.  
6 (optional)  Customize color mappings for the Overview page.  Navigate to Monitoring Console > Settings > Overview Preferences.  

Get started 

To configure your monitoring console for standalone deployment, first verify the prerequisites on your instance. Begin with Monitoring Console setup prerequisites. 



Which instance should host the console?

This topic is a step in the process of setting up the monitoring console for a distributed Splunk Enterprise deployment. 

To start, determine which instance will best host the monitoring console. You have several options for where to host the monitoring console, depending on the nature of your deployment: 

• The instance you choose must meet or exceed the search head reference hardware requirements. See Reference hardware in the Capacity Planning Manual. 


• For security and performance reasons, only Splunk Enterprise administrators should have access to this instance.


• The instance hosting the monitoring console must not run any searches unrelated to its function as monitoring console. The exception to this rule is if you are using the console to monitor a standalone single-instance deployment.


This table outlines the recommended locations for the monitoring console, based on deployment type. 


Distributed mode? 

Indexer clustering? 

Search head clustering? 

Recommended locations 

No  N/A  N/A  The standalone instance.  
Yes  No  No  The license master or a deployment server servicing a small number (<50) of clients. Otherwise, run the monitoring console on a search head that is dedicated to running monitoring console searches.  
Yes  Single indexer cluster  Not relevant  The master node, if the load on the master node is below the limits specified in Additional roles for the master node in the Managing Indexers and Clusters of Indexers manual. Otherwise, run the monitoring console on a search head node that is dedicated to running monitoring console searches.  
Yes  Multiple indexer clusters  Not relevant  A search head that is configured as a search head node across all the clusters. This search head must be dedicated to monitoring console use.  
Yes  No  Yes  The search head cluster deployer, a license master, or a standalone search head that is dedicated to running monitoring console searches. Do not run the monitoring console on a search head cluster member.  

For a general discussion of management component colocation, see Components that help to manage your deployment in the Distributed Deployment Manual. 

See the sections that follow for detailed information for certain deployment types. 


In a non-clustered deployment

You can locate the monitoring console on any of these instances: 

• A license master


• A deployment server that is servicing a small number (<50) of clients 


• A dedicated search head



In a deployment with a single indexer cluster

In a single indexer cluster, you can host the monitoring console on the instance running the master node if the load on the master node is below the limits specified in Additional roles for the master node in the Managing Indexers and Clusters of Indexers manual. 

You can also host the monitoring console on a search head node in the cluster, but you must dedicate the node to monitoring console searches. You cannot use the search head to run any other searches. 


In a deployment with multiple indexer clusters

If your deployment has multiple indexer clusters, host the monitoring console on a search head configured as a search head node on each of the clusters. Do not use this search head to run any non-monitoring console searches. 

To do this: 

1. Configure a search head to serve as a node on each of the indexer clusters. See Search across multiple indexer clusters in the Managing Indexers and Clusters of Indexers manual. This is your monitoring console instance. 

2. Configure each master node and all search head nodes in the clusters as search peers of the monitoring console instance. See Add instances as search peers in this manual. 

Do not configure the cluster peer nodes (indexers) as search peers to the monitoring console node. As nodes in the indexer clusters, they are already known to all search head nodes in their cluster, including the monitoring console node.


In a deployment with a search head cluster but without an indexer cluster

You can locate the monitoring console on any of these instances: 

• A search head cluster deployer


• A license master


• A standalone, dedicated search head


Do not run the monitoring console on a search head cluster member.

The Monitoring Console is not supported for search head pooling deployments. 


Why not to host the console on a production search head

Do not configure the monitoring console on a production search head for the following reasons: 

• Non-monitoring console searches that run on this search head might have incomplete results. The monitoring console distributed search groups modify default search behavior to ensure that the searches for the monitoring console dashboards are narrowly scoped to the list of search peers that they target. When you set up the monitoring console in distributed mode, it creates one search group for each server role, identified cluster, or custom group. Unless you use a "splunk_server_group" or the "splunk_server" option, only search peers that are members of the indexer group are searched by default. Because all searches that run on the monitoring console instance follow this behavior, non-monitoring console searches might have incomplete results.


• All production search heads should be monitored for performance, and the monitoring console affects the performance of the search head that hosts it. It can be difficult to disentangle monitoring console resource usage from production resource usage on the same instance.



The monitoring console and deployment server

In most cases, you cannot host the distributed monitoring console on a deployment server. The exception is if the deployment server handles only a small number of deployment clients, no more than 50. The monitoring console and deployment server functionalities can interfere with each other at larger client counts. See Deployment server provisioning in the Updating Splunk Enterprise Instances manual. 




Next step 

To continue setting up the monitoring console in distributed mode, make sure your deployment meets the prerequisites. See Monitoring Console setup prerequisites. 




Monitoring Console setup prerequisites

This topic is a step in the process of setting up the monitoring console for either a distributed Splunk Enterprise deployment or a standalone Splunk Enterprise instance. 

To proceed with your monitoring console deployment, verify that you meet the following setup prerequisites: 

• Have a functional Splunk Enterprise deployment.


• Ensure that each instance in the deployment has a unique server.conf serverName value and inputs.conf host value.


• Enable platform instrumentation for every Splunk Enterprise instance that you intend to monitor, except forwarders. See About Splunk Enterprise platform instrumentation. 


• Forward internal logs (both $SPLUNK_HOME/var/log/splunk and $SPLUNK_HOME/var/log/introspection) to indexers from all other components. Without this step, many dashboards will lack data. See Best practice: Forward search head data in the Distributed Search Manual. 


• The user setting up the monitoring console needs the admin_all_objects capability.



Dashboard version dependencies

The dashboards in the monitoring console rely on data collected from Splunk Enterprise internal log files and endpoints. Much of the data comes from platform instrumentation, which was introduced in Splunk Enterprise version 6.1 and enhanced in subsequent releases. The following table summarizes the Splunk Enterprise version requirements for particular platform instrumentation capabilities. If the instances that you monitor in the console do not meet these version requirements, the related dashboard panels are empty. 


Feature 

Panel 

System requirement 

All dashboards  Most panels  Splunk Enterprise 6.1  
KV store dashboards  All panels  Splunk Enterprise 6.2.0 (which introduced the KV store)  
Search head clustering dashboards  All panels  Splunk Enterprise 6.2.0  
Distributed search dashboards  Panels about bundle replication  Splunk Enterprise 6.3.0  
HTTP Event Collector (HEC) dashboards  All panels  Splunk Enterprise 6.3.0 (which introduced HEC)  
Scheduler dashboards  Most panels  Splunk Enterprise 6.3.0  
Resource usage: Machine, Resource usage: Deployment  I/O panels  Splunk Enterprise 6.4.0  
Health check  N/A  Splunk Enterprise 6.5.0 on instance hosting monitoring console  


Next step 

To continue setting up your monitoring console in a distributed deployment, see Set cluster labels. 

To continue setting up your monitoring console on a standalone instance, skip to Configure Monitoring Console in standalone mode. 




Set cluster labels

This topic is a step in the process of setting up the monitoring console for a Splunk Enterprise deployment. 

Set labels on your indexer clusters and search head clusters, so that the monitoring console can identify the instances associated with them. The labels allow the monitoring console to populate the indexer clustering and search head clustering dashboards. 


During cluster deployment

Set the label during initial deployment of the cluster. If necessary, you can set or change the label later. 

To set the indexer cluster label during deployment, see Enable the indexer cluster master node in the Managing Indexers and Clusters of Indexers manual. 

To set the search head cluster label during deployment, see Deploy a search head cluster in the Distributed Search manual. 


After cluster deployment

If you are deploying a new cluster, set cluster labels during the deployment process. If your cluster is already deployed, set cluster labels at the beginning of the monitoring console setup according to the following tables. 


Splunk Enterprise version and cluster type 

Where? 

How? 

6.3.0+ indexer cluster  On the cluster master  See Configure the master with the dashboard in the Managing Indexers and Clusters of Indexers manual. Alternatively, run the following CLI command from the cluster master: 
splunk edit cluster-config -cluster_label <CLUSTER LABEL> 
 
Pre-6.3.0 indexer cluster  On the monitoring console setup page  Set label for all instances in cluster  





Splunk Enterprise version and cluster type 

Where? 

How? 

6.3.0+ search head cluster members  On any search head cluster member  See Configure the search head cluster in the Distributed Search manual. Alternatively, run the following CLI command from any cluster member: 
splunk edit shcluster-config -shcluster_label <CLUSTER LABEL> 
 
Pre-6.3.0 search head cluster members  On the monitoring console setup page  Set the label for all instances in the cluster  
Search head cluster deployer  On the monitoring console setup page  Set the label for the deployer instance  

Whichever method you use to edit cluster labels, go to the monitoring console setup page and click Apply Changes to update the monitoring console asset table. 

About the monitoring console setup page

The monitoring console setup page in Splunk Web is at Monitoring Console > Settings > General Setup. 

After editing cluster labels, click Apply Changes. 


Next step 

To continue setting up your monitoring console in a distributed deployment, see Add instances as search peers to the Monitoring Console. 



Add instances as search peers to the Monitoring Console

This topic is a step in the process of setting up the monitoring console for a distributed Splunk Enterprise deployment. 

To add instances as search peers to the monitoring console, perform the following steps: 

1.Log into the instance on which you want to configure the monitoring console.


2.In Splunk Web, click Settings > Distributed search > Search peers.


3.Click New.


4.Fill in the requested fields, and then click Save.


5.Repeat these steps for each search head, deployment server, license master, and nonclustered indexer. Do not add clustered indexers, but be sure to add clustered search heads. If you are monitoring an indexer cluster and you are hosting the monitoring console on an instance other than the cluster master, you must add the cluster master as a search peer.



Next step 

To continue setting up the monitoring console on a distributed deployment, see Configure Monitoring Console in distributed mode. 



Configure Monitoring Console in standalone mode

This topic is a step in the process of setting up the monitoring console for a standalone Splunk Enterprise instance. 

To configure, perform the following steps: 

1.In Splunk Web, navigate to Monitoring Console > Settings > General Setup.


2.Check that search head, license master, and indexer are listed under Server Roles, and nothing else. If not, click Edit to correct.


3.Click Apply Changes.



Next step 

To continue setting up the monitoring console for a standalone deployment, skip to Configure forwarder monitoring for the Monitoring Console. 



Configure the Monitoring Console in distributed mode

This topic is a step in the process of setting up the monitoring console for a distributed Splunk Enterprise deployment. 

To configure, perform the following steps: 

1.Log into the instance on which you want to configure the monitoring console. The instance by default is in standalone mode, unconfigured.


2.In Splunk Web, select Monitoring Console > Settings > General Setup.


3.Turn on Distributed mode.


4.Confirm the following: 

•The columns labeled instance and machine are populated correctly and populated with values that are unique within a column. 



•The server roles are correct,. For example, a search head that is also a license master should have both primary and major roles marked. If not, click Edit to correct.



•A cluster master is identified if you are using indexer clustering. If not, click Edit to correct.


Make sure anything marked as an indexer is really an indexer.


5.(Optional) Set custom groups. Custom groups are tags that map directly to distributed search groups. You might find groups useful, for example, if you have multisite indexer clustering in which each group can consist of the indexers in one location, or if you have an indexer cluster plus standalone peers. Custom groups are allowed to overlap. For example, one indexer can belong to multiple groups. See Create distributed search groups in the Distributed Search manual.


6.Click Apply Changes.


If you add another node to your deployment later, click Settings > General Setup and check that these items are accurate. 


Reset server roles after restart

After you configure the monitoring console in distributed mode, a restart of the instance hosting the monitoring console can cause any changes to server role settings on the instances you are monitoring to be lost. 

To properly reset server roles after restart of the distributed monitoring console: 

1.Click Settings > General Setup > Reset All Settings > Refresh. 
 This restores the monitoring console to its original default configuration. 


2. Click Distributed. 


3. For the remote instances whose server roles you want to change, click Edit. 


4. Select or remove the specific server roles. Click Save. 


5. Click Apply Changes. 



Next step

To configure the monitoring console for forwarders, see Configure forwarder monitoring for the Monitoring Console. 



Configure forwarder monitoring for the Monitoring Console

This topic is a step in the process of setting up the monitoring console for either a distributed or a standalone Splunk Enterprise deployment. 


Prerequisites

For several dashboard monitoring panels to work, your forwarders need unique and persistent GUIDs. One way to accomplish this is to clone your forwarder before starting it. The forwarder GUID is in instance.cfg, which populates when you start the forwarder. 


Setup

In Splunk Web, click Monitoring Console > Settings > Forwarder setup and follow the setup steps. 


About time settings

On the forwarder monitoring setup page, you can enable or disable forwarder monitoring and set the data collection interval. Enabling forwarder monitoring runs a scheduled search that populates dmc_forwarder_assets.csv, a lookup file that resides on the monitoring console node in $SPLUNK_HOME/etc/apps/splunk_monitoring_console/lookups. The monitoring console uses this forwarder asset table to know which forwarders to display information about in the forwarder monitoring dashboards. 

In Splunk Web click Settings > Searches and reports > DMC Forwarder - Build Asset Table to review the scheduled search. 

Click Monitoring Console > Settings > Forwarder Monitoring Setup and choose from several values for data collection interval. This interval determines how often that scheduled search runs. The default value is 15 minutes. 

When the scheduled search runs to rebuild the forwarder asset table it always looks back 15 minutes. This lookback time is not configurable, and it is different from the data collection interval. For example, if you set the data collection interval to 24 hours, the scheduled search will run once every 24 hours, but check only the 15 minutes before it starts running. 

Scheduled search can be expensive if you have many forwarders. You might want to run the search less often than the default value. 


Rebuild the forwarder asset table

The data in the forwarder asset table is cumulative. If a forwarder connects to an indexer, its record exists in the table. If you later remove the forwarder from your deployment, the forwarder's record is not removed from the asset table. It is instead marked "missing" in the asset table, and it still appears in the DMC forwarder dashboards. 

To remove a forwarder entirely from the monitoring console dashboards, click rebuild forwarder assets in Monitoring Console > Settings > Forwarder Monitoring Setup. You can choose a lookback time when you perform this action. The lookback selection during this action does not change the 15-minute lookback time for the scheduled search or the data collection interval discussed elsewhere in this topic. 




Next step 

To set up platform alerts, see Enable and configure platform alerts. This step is optional. 




Enable and configure platform alerts

Platform alerts are saved searches included in the monitoring console. Platform alerts notify Splunk Enterprise administrators of conditions that might compromise their deployment environment. When an alert is triggered, the Monitoring Console Overview page displays a notification. You can also view the alert and its results by going to Overview > Alerts > Managed triggered alerts. 

The included platform alerts get their data from REST endpoints. Platform alerts are disabled by default. 


Enable platform alerts

Prerequisite 

Configure your Monitoring Console. See Single instance Monitoring Console setup steps or Multi-instance Monitoring Console setup steps depending on your deployment type. 

1. From the monitoring console Overview, click Triggered Alerts > Enable or Disable.


2. Click the Enabled check box next to the alerts that you want to enable.


You can also set an alert action, such as an email notification. 


Configure platform alerts and set alert actions

You can view and configure the default settings and parameters for platform alerts, including the following: 

• Alert thresholds, if applicable


• Alert schedule


• Suppression time


• Alert actions (such as sending an email or starting a custom script)


To change an alert threshold, perform the following steps: 

1. From the Monitoring Console, click Overview > Alerts > Enable or Disable.


2. Find the alert you want to configure and click Edit.


3. Edit the threshold field to your desired value.


4. Click Save.


To view and edit advanced settings like alert schedule, trigger conditions, and alert actions, perform the following steps: 

1. From the Monitoring Console, click Overview > Alerts > Enable or Disable.


2. Find the alert you want to configure and click Advanced edit.


3. Modify the settings, if desired.


4. Click Save.


If you enable email notifications, make sure that you have defined a valid mail host in Settings > Server settings > Email settings. 

For guidance on alert actions, see Set up alert actions in the Alerting Manual. 

You can also view the complete list of default parameters for platform alerts in $SPLUNK_HOME/etc/apps/splunk_management_console/default/savedsearches.conf. If you choose to edit configuration files directly, put the new configurations in a local directory instead of the default. 

Never edit configuration files in the default directory.


Default platform alerts in the monitoring console

The following platform alerts are available by default in the monitoring console. To monitor your deployment with platform alerts, enable the individual alerts that you want. 


Alert name 

Description 

For more information 

Abnormal State of Indexer Processor  Fires when one or more of your indexers reports an abnormal state. This abnormal state can be either throttled or stopped.  For details on which indexer is in which abnormal state, and to begin investigating causes, see the monitoring console Indexing Performance: Deployment dashboard's Indexing Performance by Instance panel. For information about the dashboard, see Indexing performance dashboards in this manual.  
Critical System Physical Memory Usage  Fires when one or more instances exceeds 90% memory usage (by any process, Splunk software or otherwise). On most Linux distributions, this alert can trigger if the OS is engaged in buffers and filesystem caching activities. The OS releases this memory if other processes need it, so it does not always indicate a serious problem.  For details on instance memory usage, navigate to the monitoring console Resource Usage: Deployment dashboard. For information about the dashboard, see Resource usage dashboards in this manual.  
Expired and Soon To Expire Licenses  Fires when you have licenses that have expired or will expire within two weeks.  For information about your licenses and license usage, click Licensing in the monitoring console.  
Missing forwarders  Fires when one or more forwarders are missing.  See the forwarders dashboards in the monitoring console.  
Near Critical Disk Usage  Fires when you have used 80% of your disk capacity.  For more information about your disk usage, navigate to the three monitoring console Resource Usage dashboards and read Resource usage dashboards in this manual.  
Saturated Event-Processing Queues  Fires when one or more of your indexer queues reports a fill percentage, averaged over the last 15 minutes, of 90% or more. This alert can inform you of potential indexing latency.  For more details about your indexer queues, navigate to the two monitoring console Indexing Performance dashboards and read Indexing performance dashboards in this manual.  
Search Peer Not Responding  Fires when any of your search peers (indexers) is unreachable.  For the status of all your instances, see the monitoring console Instances view.  
Total License Usage Near Daily Quota  Fires when you have used 90% of your total daily license quota.  For more information about your license usage, click Licensing in the monitoring console.  


Longevity of platform alert search artifacts

In savedsearches.conf, the dispatch.ttl setting dictates that the searches from platform alerts keep search artifacts for four hours. But if an alert is triggered, its search artifact stays for seven days. This means that the link sent in an email to inspect the search results of a triggered alert expires in seven days (by default). 




Next step 

To set up health checks, see Access and customize health check. This step is optional. 




Access and customize health check

The monitoring console comes with preconfigured health checks. You can modify existing health checks or create new ones. 


Use the health check

Find the health check at Monitoring Console > Health Check. Start the health check by clicking Start. 

Each health check item runs a separate search. The searches run sequentially. When one search finishes, the next one starts. After all searches have completed, the results are sorted by severity: Error, Warning, Info, Success, or N/A. 

Click a severity level at the top of the results to see only results with that severity level. Click a row to see more information, including suggested actions. 

To run only some of the checks, filter by tag or category before clicking Start. From the monitoring console, you can run health checks that have been created in any app installed on your monitoring console node. Use the app drop-down list to filter health checks by app context. 


Exclude a check

You can disable a specific check to prevent it from running when you click Start: 

1. Click Monitoring Console > Settings > Health Check Items.


2. Locate the check you wish to disable in the list.


3. Click Disable.


4. Reload Monitoring Console > Health Check.


You can also filter the checks by group, app, tag, and category at the top of the page before clicking Start. 


Modify an existing check

You can modify an existing check. For example, to modify the warning threshold for the Excessive physical memory usage check from 90% to 80%: 

1. Click Monitoring Console > Settings > Health Check Items.


2. In the Excessive physical memory usage row, click Edit.


3. Edit the Search and Description fields.


4. (Optional) Rename the health check item to reflect your modification.


5. Click Save.


The modifications are saved to your filesystem in $SPLUNK_HOME/etc/apps/splunk_monitoring_console/local/checklist.conf 


Create a new check

You can add a new health check item: 

1. Click Monitoring Console > Settings > Health Check Items.


2. Click New Health Check Item.


3. Fill in the title and ID fields.


4. (Optional) Choose an app context for this check. The default is monitoring console.


5. Continue filling in the fields. Be sure to include a severity level in your search (| eval severity_level). Without this, the search returns results as N/A. See About searches for guidance filling in the Search field.


6. (Optional) For Environments to exclude, select either Standalone or Distributed. Any other value in this field is ignored. See What can the monitoring console do? for information about standalone and distributed modes.


7. Click Save.


The modifications are saved to your filesystem in $SPLUNK_HOME/etc/apps/<app_name>/local/checklist.conf on *nix or %SPLUNK_HOME%\etc\apps\<app_name>\local\checklist.conf on Windows. If you do not specify an app context, the modifications are saved in the splunk_monitoring_console app directory. 

Search results format

In standalone mode, the search string generates the final result. In distributed mode, this search generates one row per instance in the result table. 

The search results must be in the following format. 


instance 

metric 

severity_level 

<instance name>  <metric number or string>  <level number>  

Severity level names correspond to values as follows. 


Severity level name 

Severity level value 

Error  3  
Warning  2  
Info  1  
Success  0  
N/A  -1  

Add a drilldown to a search or dashboard

You can also include a drilldown to another search or to a dashboard, for example a monitoring console dashboard, in your health check results. 

To include a monitoring console dashboard drilldown: 

1. Choose an existing dashboard in the monitoring console that is relevant to the data you want to run a health check on. Choose a dashboard that has a drop-down list to choose an instance or machine.


2. Inspect the URL using the drop-down list to see which parts of the URL are needed to specify the instance you want. Look for &form.splunk_server=$instance$ toward the end of the URL.


3. Trim the URL to a URI that starts with /app/ and has a $ delimited variable name that is a column in the search results for your health check item. For example, /app/splunk_monitoring_console/distributed_search_instance?form.splunk_server=$search_head$


To include a search drilldown, find or create a search with a $ delimited variable in it. The variable must exist as a column name in the health check search results. For example, a drilldown of index=_internal $instance$ will work, as long as "instance" is a column name in the health check search. 
Most likely, you want a drilldown search of the search you just ran. In that case, replace $rest_scope$ or $hist_scope$ with $instance$, where instance is a column name in the health check search. For example: 

`dmc_set_index_internal` host=$instance$ earliest=-60m source=*splunkd.log* (component=AggregatorMiningProcessor OR component= LineBreakingProcessor OR component=DateParserVerbose) (log_level=WARN OR log_level=ERROR)


Proactively alert on health check conditions

Many health check items already have a corresponding platform alert. You can also turn an additional health check into an alert. 

This table lists the health check items with corresponding platform alerts: 


Health check 

Corresponding platform alert 

Condition 

Indexing status  Abnormal State of Indexer Processor  Tests the current status of the indexer processor on indexer instances.  
Excessive physical memory usage  Critical System Physical Memory Usage  Assesses system-wide physical memory usage and raises a warning for those servers where it is >90%.  
Expiring or expired licenses  Expired and Soon To Expire Licenses  Checks for licenses that are expired or will expire within 2 weeks.  
Missing forwarders  Missing forwarders  Checks for forwarders that have not connected to indexers for >15 minutes in the recent past.  
Near-critical disk usage  Near Critical Disk Usage  Checks for 80% of the disk usage of partitions that Splunk Enterprise reads or writes to.  
Saturation of event-processing queues  Saturated Event-Processing Queues  One or more of your indexer queues is reporting a fill percentage, averaged over the last 15 minutes, of 90% or more.  
Distributed search health assessment  Search Peer Not Responding  Checks the status of the search peers (indexers) of each search head.  

To create a new alert from a health check when a counterpart does not already exist: 

1. Run the health check.


2. Click Open in search.


3. Modify the search with a where clause.


4. Save it as a new scheduled search with an alert action. For example, email the admin.



Export health check results

You can export the results from a health check item to your local machine to share with others. 

To export results from a health check item: 

1. Run the health check.


2. Click the row with the results you want to export.


3. In the results table on the right, click Export.


4. Choose the format of the results (XML, CSV, or JSON). You can also choose a file name and the number of results.


5. Click Export.










