# Installation & Configuration

## Install the Splunk Add-on for AWS

1. Get the Splunk Add-on for AWS by downloading it from <https://splunkbase.splunk.com/app/1876> or browsing to it using the app browser within Splunk Web.
2. Determine where and how to install this add-on in your deployment, using the tables on this page.
3. Perform any prerequisite steps before installing, if required and specified in the tables below.
4. Complete your installation.

If you need step-by-step instructions on how to install an add-on in your specific deployment environment, see the [installation walkthroughs](http://docs.splunk.com/Documentation/AddOns/released/AWS/Distributeddeployment#Installation_walkthroughs) section at the bottom of this page for links to installation instructions specific to a single-instance deployment, distributed deployment, Splunk Cloud, or Splunk Light.

### Distributed deployments

Use the tables below to determine where and how to install this add-on in a distributed deployment of Splunk Enterprise or any deployment for which you are using forwarders to get your data in. Depending on your environment, your preferences, and the requirements of the add-on, you may need to install the add-on in multiple places.

#### Where to install this add-on

Unless otherwise noted, all supported add-ons can be safely installed to all tiers of a distributed Splunk platform deployment. See [Where to install Splunk add-ons](http://docs.splunk.com/Documentation/AddOns/latest/Overview/Wheretoinstall) in Splunk Add-ons for more information.

This table provides a reference for installing this specific add-on to a distributed deployment of Splunk Enterprise.

<table>
<tr><td>Splunk platform component</td><td>Supported</td><td>Required</td><td>Comments</td></tr>
<tr><td>Search Heads</td><td>Yes</td><td>Yes</td><td>Install this add-on to all search heads where AWS knowledge management is required.</td></tr>
<tr><td>Indexers</td><td>Yes</td><td>No</td><td>Not required, as the parsing operations occur on the heavy forwarders.</td></tr>
<tr><td>Heavy Forwarders</td><td>Yes</td><td>Yes</td><td>This add-on requires heavy forwarders to perform data collection via modular inputs and to perform the setup and authentication with AWS in Splunk Web.</td></tr>
<tr><td>Universal Forwarders</td><td>No</td><td>No</td><td>This add-on requires heavy forwarders.</td></tr>
<tr><td>Light Forwarders</td><td>No</td><td>No</td><td>This add-on requires heavy forwarders.</td></tr>
</table>

#### Distributed deployment compatibility

This table provides a quick reference for the compatibility of this add-on with Splunk distributed deployment features.

<table>
<tr><td>Distributed deployment feature</td><td>Supported</td><td>Comments</td></tr>
<tr><td>Search Head Clusters</td><td>Yes</td><td>You can install this add-on on a search head cluster for all search-time functionality, but configure inputs on forwarders to avoid duplicate data collection.
Before installing this add-on to a cluster, make the following changes to the add-on package:

1. Remove the `eventgen.conf` files and all files in the `samples` folder
2. Remove the `inputs.conf` file.

</td></tr>
<tr><td>Indexer Clusters</td><td>Yes</td><td>Before installing this add-on to a cluster, make the following changes to the add-on package:

1. Remove the `eventgen.conf` files and all files in the `samples` folder
2. Remove the `inputs.conf` file.

</td></tr>
<tr><td>Deployment Server</td><td>No</td><td>Supported for deploying unconfigured add-ons only.
Using a deployment server to deploy the configured add-on to multiple forwarders acting as data collectors causes duplication of data.
The add-on uses the credential vault to secure your credentials, and this credential management solution is incompatible with the deployment server.</td></tr>
</table>

### Installation walkthroughs

The Splunk Add-Ons manual includes an [Installing add-ons](http://docs.splunk.com/Documentation/AddOns/latest/Overview/Installingadd-ons) guide that helps you successfully install any Splunk-supported add-on to your Splunk platform.

For a walkthrough of the installation procedure, follow the link that matches your deployment scenario:

- Single-instance Splunk Enterprise
- Distributed Splunk Enterprise
- Splunk Cloud
- Splunk Light

## Manage accounts for the Splunk Add-on for AWS

Manage your accounts, proxy connections, and log levels for the Splunk Add-on for AWS on your data collection node, usually a heavy forwarder, using Splunk Web. Managing these items using the configuration files is not supported.

The Splunk Add-on for AWS supports two methods for connecting to AWS to collect data: EC2 IAM roles and AWS user accounts.

### Discover an EC2 IAM role

If you are running your data collection node of your Splunk platform in your own managed AWS environment using commercial regions, you can set up an IAM role for the EC2 and use that role to configure data collection jobs. The Splunk Add-on for AWS automatically discovers this role once it is set up.

> Collecting data using an auto-discovered EC2 IAM role is not supported in China or GovCloud regions.

1. Follow the AWS documentation to set up an IAM role for your EC2: <http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html>.
2. Ensure that this role has all of the required permissions specified in [Configure AWS permissions for the Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWSpermissions). If you do not want to give the role all of the permissions required for all inputs, you also need to configure AWS accounts that you can use for the other inputs not covered by the permissions in this role.
3. Click Splunk Add-on for AWS in your left navigation bar on Splunk Web's home page.
4. Click Configuration in the app navigation bar. By default, the add-on displays the Account tab.
5. Look for the EC2 IAM role in the Autodiscovered IAM Role column. If you are in your own managed AWS environment and you have an EC2 IAM role configured, it appears in this account list automatically.

No further configuration is required. You can also configure AWS accounts if you want to use both EC2 IAM roles and user accounts to ingest your AWS data.

> You cannot edit or delete EC2 IAM roles from the add-on.

### Manage AWS accounts

To add an AWS account:

1. On Splunk Web's home page, click Splunk Add-on for AWS in your left navigation bar.
2. Click Configuration in the app navigation bar. By default, the add-on displays the Account tab.
3. Click Add.
4. Enter a friendly name for the AWS account. You cannot change the friendly name once you have configured the account.
5. Enter the credentials (Key ID and Secret Key) for an AWS account that the Splunk platform should use to access your AWS data. The accounts you configure here must have adequate permissions to access the AWS data that you want to collect. See [Configure AWS permissions for the Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWSpermissions) for more information.
6. Select the Region Category for the account. The most common is Global.
7. Click Add.

You can edit existing accounts by clicking Edit in the Actions column.

You can delete an existing account by clicking Delete in the Actions column. The add-on prevents you from deleting accounts that are in use by one or more inputs, even if those inputs are disabled. Delete the inputs or edit them to use a different account, then you can delete the account.

> If you want to use custom commands and alert actions, you must set up at least one AWS account on your Splunk platform deployment's search head or search head cluster.

### Manage IAM roles

Under the **Configuration** menu, you manage AWS IAM roles that can be assumed by IAM accounts for the Splunk Add-on for AWS to access the following AWS resources: Generic S3, Incremental S3, SQS-Based S3, Billing, Description, CloudWatch, Kinesis.

> Note: Configuring roles by directly editing `splunk_ta_aws_iam_roles.conf` is not supported. Use the add-on's Configuration menu in Splunk Web to manage IAM roles. You cannot edit or delete EC2 IAM roles from the add-on. To add an IAM Role:

1. On Splunk Web's home page, click **Splunk Add-on for AWS** in your left navigation bar.
2. Click **Configuration** in the app navigation bar, and then click the **IAM Role** tab.
3. Click **Add**.
4. In the Name field, enter a unique friendly name for the role to be assumed by authorized AWS accounts managed on the Splunk platform. You cannot change the friendly name once you have configured the role.
5. In the **ARN** field, enter the role's Amazon Resource Name in the valid format: `arn:aws:iam::<aws_resource_id>:role/<role_name>`, for example, `arn:aws:iam::123456789012:role/s3admin`.
    The IAM role you configure here must have adequate permissions to access the AWS data that you want to collect. See [Configure AWS permissions for the Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureAWSpermissions) for more information.
6. Click Add.

You can edit existing IAM roles by clicking **Edit** in the Actions column.

You can delete an existing role by clicking **Delete** in the Actions column. The add-on prevents you from deleting roles that are being assumed and in use by one or more inputs, even if those inputs are disabled. Delete the inputs or edit them to use a different assumed role, then you can delete the role.

### Configure a proxy connection

1. Click **Splunk Add-on for AWS** in your left navigation bar on Splunk Web's home page.
2. Click **Configuration** in the app navigation bar.
3. Click the **Proxy** tab.
4. Select the **Enable** box to enable the proxy connection and fill in the fields required for your proxy.
5. Click **Save**.

If, at any time, you want to disable your proxy but save your configuration, clear the **Enable** box. The add-on stores your proxy configuration so you can easily enable it again later.

To delete your proxy configuration, delete the values in the fields.

## Configure inputs for the Splunk Add-on for AWS

### Input configuration overview

You can use the Splunk Add-on for AWS to collect many types of useful data from AWS to gain valuable insight of your cloud infrastructure. For each supported data type, one or more input types are provided for data collection.

Follow these steps to plan and perform your AWS input configuration:

1. Read the [Supported data types and corresponding AWS input types](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureInputs#Supported_data_types_and_corresponding_AWS_input_types) section to view a list of supported data types and use the matrix to determine which input type to configure for your data collection.
2. View the details of the input type you plan to configure under [AWS input types](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureInputs#AWS_input_types). From there, click the input type link to go to the input configuration details.
3. Follow the steps described in the input configuration details to complete the configuration.

### Supported data types and corresponding AWS input types

The following matrix lists all the data types that can be collected using the Splunk Add-on for AWS and the corresponding input types that you can configure to collect this data.

For some data types, the Splunk Add-on for AWS provides you with the flexibility to choose from multiple input types based on specific requirements (for example, collect historical logs as opposed to only collect newly created logs). Whenever applicable, SQS-based S3 is the recommended input type to use for all of its collectible data types.

> **S \= Supported, R \= Recommended \(when there are multiple supported input types\)**

<table>
<tr><td>Data Type</td><td>Source type</td><td>SQS-based S3</td><td>Generic S3</td><td>Incremental S3</td><td>AWS Config</td><td>Config Rules</td><td>Inspector</td><td>Cloud-
Trail</td><td>Cloud-
Watch</td><td>Cloud-
Watch Logs</td><td>Description</td><td>Billing</td><td>Kinesis</td><td>SQS</td></tr>
<tr><td>Billing</td><td>aws:
billing</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td></tr>
<tr><td>Cloud-
Watch</td><td>aws:
cloudwatch</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>CloudFront Access Logs</td><td>aws:
cloudfront:
accesslogs</td><td>R</td><td>S</td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Config</td><td>aws:
config, aws:
config:
notification</td><td>R</td><td></td><td></td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Config Rules</td><td>aws:
config:
rule</td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Description</td><td>aws:
description</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td></td></tr>
<tr><td>ELB Access Logs</td><td>aws:elb:
accesslogs</td><td>R</td><td>S</td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Inspector</td><td>aws:
inspector</td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Cloud-
Trail</td><td>aws:
cloudtrail</td><td>R</td><td>S</td><td>S</td><td></td><td></td><td></td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>S3 Access Logs</td><td>aws:s3:
accesslogs</td><td>R</td><td>S</td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>VPC Flow Logs</td><td>aws:
cloudwatchlogs:
vpcflow</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td>R</td><td></td></tr>
<tr><td>SQS</td><td>aws:sqs</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td></tr>
<tr><td>Others</td><td>custom source types</td><td>S</td><td>S</td><td></td><td></td><td></td><td></td><td></td><td></td><td>S</td><td></td><td></td><td>S</td><td>S</td></tr>
</table>

### AWS input types

The Splunk Add-on for AWS provides two categories of input types to gather useful data from your AWS environment:

- Dedicated (single-purpose) input types: designed to ingest one specific data type
- Multi-purpose input types: collect multiple data types from the S3 bucket

Some data types can be ingested using either a dedicated input type or a multi-purpose input type. For example, CloudTrail logs can be collected using any of the following input types: CloudTrail, S3, or SQS-based S3. The SQS-based S3 input type is the recommended option because it is more scalable and provides higher ingestion performance.

#### Dedicated input types

To ingest a specific type of logs, configure the corresponding dedicated input designed to collect the log type. Click the input type name in the table below for instructions on how to configure it.

<table>
<tr><td>Input</td><td>Description</td></tr>
<tr><td>AWS Config</td><td>Configuration snapshots, historical configuration data, and change notifications from the AWS Config service.</td></tr>
<tr><td>Config Rules</td><td>Compliance details, compliance summary, and evaluation status of your AWS Config Rules.</td></tr>
<tr><td>Inspector</td><td>Assessment Runs and Findings data from the Amazon Inspector service.</td></tr>
<tr><td>CloudTrail</td><td>AWS API call history from the AWS CloudTrail service.</td></tr>
<tr><td>CloudWatch Logs</td><td>Logs from the CloudWatch Logs service, including VPC Flow Logs. VPC Flow Logs allow you to capture IP traffic flow data for the network interfaces in your resources.</td></tr>
<tr><td>CloudWatch</td><td>Performance and billing metrics from the AWS CloudWatch service.</td></tr>
<tr><td>Description</td><td>Metadata about your AWS environment.</td></tr>
<tr><td>Billing</td><td>Billing data from the billing reports that you collect in the Billing & Cost Management console.</td></tr>
<tr><td>Kinesis</td><td>Data from your Kinesis streams.

> **Note** : It is a best practice to collect VPC flow logs and CloudWatch logs through Kinesis streams. However, the AWS Kinesis input currently has the following limitations:

- Multiple inputs collecting data from a single stream cause duplicate events in Splunk.
- Does not support monitoring of dynamic shards repartition, which means when there is a shard split or merge, the add-on cannot automatically discover and collect data in the new shards until it is restarted. After you repartition shards, you must restart your data collection node to collect data from the partitions.

You can also collect data from Kinesis streams using the Splunk Add-on for Amazon Kinesis Firehose. The Splunk Add-on for Amazon Kinesis Firehose simplifies some of the configuration steps, but the same limitations about collecting data from streams apply there as to the Kinesis input in this add-on. For more information, see About the Splunk Add-on for Amazon Kinesis Firehose.</td></tr>
<tr><td>SQS</td><td>Data from your AWS SQS.</td></tr>
</table>

#### Multi-purpose input types

Configure multi-purpose inputs to ingest supported log types.

Splunk recommends that you use the SQS-based input type to collect its supported log types. If you are already collecting logs using generic S3 inputs, you can still create SQS-based inputs and migrate your existing generic S3 inputs to the new inputs. For detailed migration steps, see [Migrate from the S3 input to the SQS-based input](http://docs.splunk.com/Documentation/AddOns/released/AWS/SQS-basedS3#Migrate_from_the_S3_input_to_the_SQS-based_S3_input) in this manual.

If the log types you want to collect are not supported by the SQS-based input type, use the generic S3 input type instead.

Read the [multi-purpose input types comparison table](http://docs.splunk.com/Documentation/AddOns/released/AWS/ConfigureInputs#Multi-purpose_input_types_comparison_table) to view the differences between the multi-purpose S3 collection input types.

Click the input type name in the table below for instructions on how to configure it.

<table>
<tr><td>Input</td><td>Description</td></tr>
<tr><td>SQS-based S3 (recommended)</td><td>A more scalable and higher-performing alternative to the generic and incremental S3 inputs, the SQS-based S3 input polls messages from SQS that subscribes to SNS notification events from AWS services and collects the corresponding log files - generic log data, CloudTrail API call history, Config logs, and access logs - from your S3 buckets in real time.

Unlike the other S3 input types, the SQS-based S3 input type takes advantage of the SQS visibility timeout setting and enables you to configure multiple inputs to scale out data collection from the same folder in an S3 bucket without ingesting duplicate data. Also, the SQS-based S3 input automatically switches to multipart, in-parallel transfers when a file is over a specific size threshold, thus preventing timeout errors caused by large file size.</td></tr>
<tr><td>Generic S3</td><td>General-purpose input type that can collect any log type from S3 buckets: CloudTrail API call history, access logs, and even custom non-AWS logs.
The generic S3 input lists all the objects in the bucket and examines each file's modified date every time it runs to pull uncollected data from an S3 bucket. When the number of objects in a bucket is large, this can be a very time-consuming process with low throughput.</td></tr>
<tr><td>Incremental S3</td><td>The incremental S3 input type collects four AWS service log types.
There are four types of logs you can collect using the Incremental S3 input:

- CloudTrail Logs: This add-on will search for the cloudtrail logs under `<bucket_name>/<log_file_prefix>/AWSLogs/<Account ID>/CloudTrail/<Region ID>/<YYYY/MM/DD>/<file_name>.json.gz`.
- ELB Access Logs: This add-on will search the elb access logs under `<bucket_name>/<log_file_prefix>/AWSLogs/<Account ID>/elasticloadbalancing/<Region ID>/<YYYY/MM/DD>/<file_name>.log.gz`.
- S3 Access Logs: This add-on will search the S3 access logs under `<bucket_name>/<log_file_prefix><YYYY-mm-DD-HH-MM-SS><UniqueString>`.
- CloudFront Access Logs: This add-on will search the cloudfront access logs under `<bucket_name>/<log_file_prefix><distributionID><YYYY/MM/DD>.<UniqueID>.gz`

The incremental S3 input only lists and retrieves objects that have not been ingested from a bucket by comparing datetime information included in filenames against checkpoint record, which significantly improves ingestion performance.</td></tr>
</table>

#### Multi-purpose input types comparison table

<table>
<tr><td></td><td>Generic S3</td><td>Incremental S3</td><td>SQS-based S3 (recommended)</td></tr>
<tr><td>Supported log types</td><td>Any log type, including non-AWS custom logs</td><td>4 AWS services log types: CloudTrail logs, S3 access logs, CloudFront access logs, ELB access logs</td><td>5 AWS services log types (Config logs, CloudTrail logs, S3 access logs, CloudFront access logs, ELB access logs), as well as non-AWS custom logs</td></tr>
<tr><td>Data collection method</td><td>Lists all objects in the bucket and compares modified date against the checkpoint</td><td>Directly retrieves AWS log files whose filenames are distinguished by datetime</td><td>Decodes SQS messages and ingests corresponding logs from the S3 bucket</td></tr>
<tr><td>Ingestion performance</td><td>Low</td><td>High</td><td>High</td></tr>
<tr><td>Can ingest historical logs (logs generated in the past)?</td><td>Yes</td><td>Yes</td><td>No</td></tr>
<tr><td>Scalable?</td><td>No</td><td>No</td><td>Yes

You can scale out data collection by configuring multiple inputs to ingest logs from the same S3 bucket without creating duplicate data</td></tr>
<tr><td>Fault-tolerant?</td><td>No

Each generic S3 input is a single point of failure.</td><td>No

Each incremental S3 input is a single point of failure.</td><td>Yes

Takes advantage of the SQS visibility timeout setting. Any SQS message not successfully processed in time by the SQS-based S3 input will reappear in the queue and will be retrieved and processed again.

In addition, data collection can be horizontally scaled out so that if one SQS-based S3 input fails, other inputs can still continue to pick up messages from the SQS queue and ingest corresponding data from the S3 bucket.</td></tr>
</table>