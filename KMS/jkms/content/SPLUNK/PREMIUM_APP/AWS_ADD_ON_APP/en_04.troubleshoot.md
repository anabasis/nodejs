# Troubleshooting

## Troubleshoot the Splunk Add-on for AWS

### General troubleshooting

For helpful troubleshooting tips that you can apply to all add-ons, see [Troubleshoot add-ons](http://docs.splunk.com/Documentation/AddOns/released/Overview/Troubleshootadd-ons). You can also access these [support and resource links](http://docs.splunk.com/Documentation/AddOns/latest/Overview/Learnmoreandgethelp).

### Health Check Dashboards

You can choose the dashboards from the **Health Check** menu to troubleshoot data collection errors and performance issues.

The **Health Overview** dashboard gives you an at-a-glance view of data collection errors and performance metrics for all input types:

- Errors count by error category
- Error count over time by input type, host, data input, and error category
- Throughput over time by host, input type, and data input

The **S3 Health Details** dashboard focuses on the generic, incremental, and SQS-based S3 input types and provides indexing time lag and detailed error information of these multi-purpose inputs.

You can customize the health dashboard. Refer to the [About the Dashboard Editor](http://docs.splunk.com/Documentation/Splunk/7.1.1/Viz/CreateandeditdashboardsviatheUI) topic in the [Dashboards and Visualizations](http://docs.splunk.com/Documentation/Splunk/7.1.1/Viz/Aboutthismanual) manual.

### Internal logs

You can directly access internal log data for help with troubleshooting. Data collected with these source types are used in the Health Check dashboards.

<table>
<tr><td>Data source</td><td>Source type</td></tr>
<tr><td>Logs from splunk_ta_aws_cloudtrail_cloudtrail_{input_name}.log.</td><td>aws:cloudtrail:log</td></tr>
<tr><td>Logs from splunk_ta_aws_cloudwatch.log.</td><td>aws:cloudwatch:log</td></tr>
<tr><td>Logs from splunk_ta_aws_cloudwatch_logs.log.</td><td>aws:cloudwatchlogs:log</td></tr>
<tr><td>Logs from splunk_ta_aws_config_{input_name}.log.</td><td>aws:config:log</td></tr>
<tr><td>Logs from splunk_ta_aws_config_rule.log.</td><td>aws:configrule:log</td></tr>
<tr><td>Logs from splunk_ta_aws_inspector_main.log, splunk_ta_aws_inspector_app_env.log, splunk_ta_aws_inspector_proxy_conf.log, and splunk_ta_aws_inspector_util.log.</td><td>aws:inspector:log</td></tr>
<tr><td>Logs from splunk_ta_aws_description.log.</td><td>aws:description:log</td></tr>
<tr><td>Logs from splunk_ta_aws_billing_{input_name}.log.</td><td>aws:billing:log</td></tr>
<tr><td>Logs from splunk_ta_aws_generic_s3_{input_name}.</td><td>aws:s3:log</td></tr>
<tr><td>Logs from splunk_ta_aws_logs_{input_name}.log, each incremental S3 input has one log file with the input name in the log file.</td><td>aws:logs:log</td></tr>
<tr><td>Logs from splunk_ta_aws_kinesis.log.</td><td>aws:kinesis:log</td></tr>
<tr><td>Logs from splunk_ta_aws_ sqs_based_s3_{input_name} .</td><td>aws:sqsbaseds3:log</td></tr>
<tr><td>Logs from splunk_ta_aws_sns_alert_modular.log and splunk_ta_aws_sns_alert_search.log.</td><td>aws:sns:alert:log</td></tr>
<tr><td>Logs from splunk_ta_aws_rest.log, populated by REST API handlers called when setting up the add-on or data input.</td><td>aws:resthandler:log</td></tr>
<tr><td>Logs from splunk_ta_aws_proxy_conf.log, the proxy handler used in all AWS data inputs.</td><td>aws:proxy-conf:log</td></tr>
<tr><td>Logs from splunk_ta_aws_s3util.log, populated by the S3, CloudWatch, and SQS connectors.</td><td>aws:resthandler:log</td></tr>
<tr><td>Logs from splunk_ta_aws_util.log, a shared utilities library.</td><td>aws:util:log</td></tr>
</table>

### Configure log levels

1. Click **Splunk Add-on for AWS** in your left navigation bar on Splunk Web's home page.
2. Click **Configuration** in the app navigation bar.
3. Click the **Logging** tab.
4. Adjust the log levels for each of the AWS services as needed by changing the default of `INFO` to one of the other available options, `DEBUG` or `ERROR`.

> Note: These log level configurations apply only to runtime logs. Some REST endpoint logs from configuration activity log at DEBUG, and some validation logs log at ERROR. These levels cannot be configured.

### Problem saving during account or input configuration

If you experience errors or trouble saving while configuring your AWS accounts on the setup page, go to `$SPLUNK_HOME/etc/system/local/web.conf` and change your timeout settings as shown below.

```properties
  [settings]
  splunkdConnectionTimeout = 300
```

### Problems deploying with a deployment server

If you use a deployment server to deploy the Splunk Add-on for Amazon Web Services to multiple heavy forwarders, you must configure the Amazon Web Services accounts using the Splunk Web setup UI for each instance separately, because the deployment server does not support sharing hashed password storage across instances.

### S3 issues

#### S3 input performance issues

You can configure multiple S3 inputs for a single S3 bucket to improve performance. The Splunk platform dedicates one process for each data input, so provided that your system has sufficient processing power, performance will improve with multiple inputs. See [Performance reference for the S3 input in the Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/S3PerformanceReference).

> Note: Be sure that the S3 key names in multiple inputs against the same bucket do not overlap, to prevent indexing duplicate data.

#### S3 key name whitelist/blacklist filtering issues

Whitelist and blacklist matches the full key name, not just the last segment.

Whitelist .*abc/.* will match /a/b/abc/e.gz.

For more help with regex:

- watch the video in this blog post: <http://blogs.splunk.com/2008/10/22/all-my-regexs-live-in-texas/>
- read "[About Splunk regular expressions](http://docs.splunk.com/Documentation/Splunk/7.1.1/Knowledge/AboutSplunkregularexpressions)" in the Knowledge Manager Manual, part of the Splunk Enterprise documentation.

#### S3 event line breaking issues

If your indexed S3 data has incorrect line breaking, configure a custom source type in `props.conf` to control how the lines should be broken for your events.

If S3 events are too long and get truncated, set `TRUNCATE = 0` in `props.conf` to prevent line truncating.

More more information, see [Configure event line breaking](http://docs.splunk.com/Documentation/Splunk/latest/Data/Configureeventlinebreaking) in the Getting Data In manual, part of the Splunk Enterprise documentation.

### CloudWatch configuration issues

#### Throttling

If you have a high volume of CloudWatch data, search `index=_internal Throttling` to determine if you are experiencing an API throttling issue. If you are, contact AWS support to increase your CloudWatch API rate. You can also decrease the number of metrics you collect or increase the granularity in order to make fewer API calls.

#### Granularity

If the granularity of your indexed data does not match your expectations, check that your configured granularity falls within what AWS supports for the metric you have selected. Different AWS metrics support different minimum granularities, based on the sampling period that AWS allows for that metric. For example, CPUUtilization has a sampling period of 5 minutes, whereas Billing Estimated Charge has a sampling period of 4 hours.

If you configured a granularity that is less than the sampling period for the selected metric, the reported granularity in your indexed data reflects the actual sampling granularity but is labeled with your configured granularity. Clear the `local/inputs.conf` cloudwatch stanza with the problem, adjust the granularity configuration to match the supported sampling granularity so that newly indexed data is correct, and reindex the data.

### CloudTrail data indexing problems

If you are not seeing CloudTrail data in the Splunk platform, follow this troubleshooting process.

1. Review the internal logs by searching for: `index=_internal source=*cloudtrail*`
2. Check to see if the Splunk platform is connecting to SQS successfully by searching for the string "Connected to SQS".
3. Check to see if the Splunk platform is processing messages successfully. Look for strings that follow the pattern "X completed, Y failed while processing notification batch".
4. Check to see if the Splunk platform is discarding messages. Look for strings that follow the pattern "fetched X, wrote Y, discarded Z".
5. Review your Amazon Web Services configuration to verify that SQS messages are being placed into queue. If messages are being removed and the logs do not show that our input is removing them, then there may be another script or input consuming messages from the queue. Review your data inputs to ensure there is not another input configured to consume the same queue.
6. Go to the AWS console to view CloudWatch metrics with the detail set to 1 minute to view the trend. For more details, see <https://aws.amazon.com/blogs/aws/amazon-cloudwatch-search-and-browse-metrics-in-the-console/>. If you see messages consumed but no Splunk platform inputs are consuming them, check for remote services that might be accessing the same queue.

### Billing Report issues

#### Problems accessing billing reports from AWS

Ensure there are Billing Reports available on the S3 bucket you select when you configure the billing input and that the AWS account you specify has the permission to read the files inside that bucket.

#### Problems understanding the billing report data

Splunk recommends [accessing the saved searches](http://docs.splunk.com/Documentation/AddOns/released/AWS/AccessBillingReportdata#Access_saved_searches) included with the add-on to analyze billing report data.

#### Problems configuring the billing data interval

> Note: The default billing data ingestion collection intervals for billing report data is designed to minimize license usage. Review the default behavior and make adjustments with caution.

To configure the interval by which Splunk Enterprise pulls Monthly and Detailed Billing Reports:

1. In Splunk Web, go to the Splunk Add-on for AWS inputs screen.
2. Create a new Billing input or click to edit your existing one.
3. Click the Settings tab.
4. Customize the value in the Interval field.

### SNS alert issues

Because the modular input module is inactive, it cannot check whether the AWS is correct or existing in the AWS SNS. If you cannot send the message to AWS SNS account, you can perform the following procedures.

- Ensure the SNS topic name exists in AWS and the region ID is correctly configured.
- Ensure the AWS account is correctly configured in Splunk add-on for AWS.

If you still have the issue, you can perform the following search to check the log for AWS SNS

`index=_internal sourcetype=aws:sns:alert:log"`

### Proxy settings for VPC endpoints

When using proxy with VPC endpoints, check the proxy setting defined in `$SPLUNK_HOME/etc/splunk-launch.conf`, for example:

`no_proxy = 169.254.169.254,127.0.0.1,s3.amazonaws.com,[s3-ap-southeast-2.amazonaws.com|http://s3-ap-southeast-2.amazonaws.com/]`

You must add each S3 region endpoint to the no_proxy setting, and use the correct hostname in your region: s3-\<your_aws_region\>.amazonaws.com. The no_proxy setting does not allow for any spaces between the IP addresses.

## Access billing data for the Splunk Add-on for AWS

The Splunk Add-on for Amazon Web Services supports extracting generic data from your S3 buckets. One example of data that you might want to extract from your S3 buckets is your AWS billing report data. Use the billing input to collect your AWS billing reports, then extract useful information from them using pre-built reports included with this add-on. The add-on's pre-built reports are based on the AWS report formats to make it easier for you to access and work with this data. You can use these reports as examples of how to use the Splunk platform to explore your other S3 data.

> Note: The Billing input does not collect billing reports for your AWS Marketplace charges.

### Billing report collection behavior

Amazon Web Services offers four distinct billing reports, which you can read more about in the AWS documentation: <http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html#d0e2817>. You can collect any of the four types of billing report using the Splunk Add-on for AWS.

The Amazon Billing service updates all four of these reports continuously over the month until they are finalized a few days after the last day of the month. Until the reports are finalized, the daily updates add new billing information for the most recent day's activity and update previously reported line items. For example, the billing service may recalculate pricing discounts for events that already passed.

Because any portion of the data in the report may be updated, the add-on pulls the entire billing report every time it pulls any of the four report types. The Monthly report and Monthly cost allocation report are quite small, so the add-on retrieves these daily by default, resulting in many copies of the same (small) report. Detailed reports, however, are very large, so the add-on collects these reports only for months prior to the current month.

By default, the add-on collects all available reports that match the collection criteria for all available months. For example, assume that you install the Splunk Add-on for AWS on February 15, 2016 and configure your inputs to collect both Monthly cost allocation reports and Detailed billing reports with resources and tags. On the first day, the add-on collects the Monthly report for February 2016 current up to February 16, 2015 and the monthly reports for all previous months available for your account. The add-on also collects the Detailed report for January 2016, as well as the Detailed report for all previous months available for your account. On every subsequent day, the add-on downloads a new version of the Monthly report, current up to the latest day. It also checks the etag of the most recently completed month's (in this case, January's) Detailed report against the latest January Detailed report available from the Amazon Billing service. If they are the same, it does not download a Detailed report again until the month ends. On March 1, the add-on begins downloading Detailed reports for the month of February every day until the etag for that report is unchanged.

See the guidelines for [configuring billing inputs](http://docs.splunk.com/Documentation/AddOns/released/AWS/Billing) for details of how to use cron schedules and regex to control the number of reports that you collect.

When you want to access any of the report data in the Splunk platform, use the reports included with the add-on to view the latest snapshots. Adjusting the reports is not supported.

### Billing report types

#### Monthly report

The **Monthly report** lists AWS usage for each product dimension used by an account and its IAM users in monthly line items. You can download this report from the Bills page of the Billing and Cost Management console.

File Name Format: {AWS account number}-aws-billing-csv-yyyy-mm.csv

This report is small in size, so the add-on pulls the entire report once daily to get the latest snapshot.

#### Monthly cost allocation report

The **Monthly cost allocation report** contains the same data as the monthly report, but it also includes any cost allocation tags that you have created. You must obtain this report from the Amazon S3 bucket that you specify. Standard AWS storage rates apply.

File Name Format: {AWS account number}-aws-cost-allocation-yyyy-mm.csv

This report is small in size, so the add-on pulls the entire report once daily to get the latest snapshot.

#### Detailed billing report

The **Detailed billing report** lists AWS usage for each product dimension used by an account and its IAM users in hourly line items. You must obtain this report from the Amazon S3 bucket that you specify. Standard AWS storage rates apply.

File Name Format: {AWS account number}-aws-billing-detailed-line-items-yyyy-mm.csv.zip

This report can grow very large, so the add-on collects the report only after the month has ended. The add-on continues to collect the report once per day until it is finalized by Amazon billing services.

#### Detailed billing report with resources and tags

The **Detailed billing report with resources and tags** contains the same data as the detailed billing report, but also includes any cost allocation tags you have created and ResourceIDs for the AWS resources used by your account. You must obtain this report from the Amazon S3 bucket that you specify. Standard AWS storage rates apply.

File Name Format: {AWS account number}-aws-billing-detailed-line-items-with-resources-and-tags-yyyy-mm.csv.zip

This report can grow very large, so the add-on collects the report only after the month has ended. The add-on continues to collect the report once per day until it is finalized by Amazon billing services.

### Event types for billing data

The Splunk platform indexes two types of billing data using the sourcetype `aws:billing`:

- monthly reports, which have the event type `aws_billing_monthly_report`
- detailed reports, which have the event type `aws_billing_detail_report`

### Access the add-on's preconfigured reports

The Splunk Add-on for Amazon Web Services includes several reports based on the indexed billing report data. You can find these saved reports in Splunk Web by clicking **Home > Reports** and looking for items with prefix `AWS Bill -` . Some of the saved searches return a table. Others return just a single value, such as `AWS Bill - Total Cost till Now`.

The Splunk platform typically indexes multiple monthly report snapshots, because AWS places multiple snapshots of the current month's billing report into the S3 bucket. To obtain the most recent monthly report snapshot click **Home > Reports** and open the saved report called `AWS Bill - Monthly Latest Snapshot`. Or, search for it using the search string: `| savedsearch "AWS Bill - Monthly Latest Snapshot"`

You can obtain the most recent detailed report by clicking **Home > Reports** and opening the saved report called `AWS Bill - Daily Cost`. Or, search for it using the search string `| savedsearch "AWS Bill - Daily Cost"`. Searching against detailed reports can be slow due to the volume of data in the report. Splunk recommends accelerating the searches against detailed reports.

#### Report sources

These saved reports are based on AWS Billing Reports instead of the billing metric data in CloudWatch. By default, Total or Monthly reports are based on data indexed from the AWS Monthly Reports (`*-aws-billing-csv-yyyy-mm.csv` or `*-aws-cost-allocation-yyyy-mm.csv`) on the S3 bucket, while Daily reports are based on AWS Detail Reports (`*-aws-billing-detailed-line-items-yyyy-mm.csv.zip` or `*-aws-billing-detailed-line-items-with-resources-and-tags-yyyy-mm.csv.zip`).

#### Default index behavior

By default, reports are looking for data in the default index, main. If you changed the default index when you configured the data input, the reports will not work unless you include the index in the default search indexes list or change the two reports so they filter to the custom index.

To include a custom index in the default search indexes list:

1. Click **Settings > Users and authentication > Access controls > Roles > [Role that uses the saved searches] > Indexes searched by default**.
2. Include the custom index in default search indexes list.
3. Repeat for each role that uses the saved searches.

To change the saved searches to filter to a custom index:

1. Open the saved search `AWS Bill - Monthly Latest Snapshot`.
2. Add a filter to specify the index you configured. For example, `index=new_index`.
3. Save your changes to the saved search.
4. Repeat these steps for the other saved search, `AWS Bill - Detailed Cost`.

## Lookups for the Splunk Add-on for AWS

The Splunk Add-on for AWS includes eight lookups. The lookup files map fields from AWS services to CIM-compliant or human-readable values in the Splunk platform. The lookup files are located in `$SPLUNK_HOME/etc/apps/Splunk_TA_aws/lookups`.

<table>
<tr><td>Filename</td><td>Purpose</td></tr>
<tr><td>aws_config_action_lookup.csv</td><td>Maps the status field to a CIM-compliant value for the action field.</td></tr>
<tr><td>aws_config_object_category_lookup.csv</td><td>Sorts the various AWS Config object categories into a CIM-compliant values for the object_category field.</td></tr>
<tr><td>aws-cloudtrail-action-status.csv</td><td>Maps the eventName and errorCode fields to CIM-compliant values for action and status.</td></tr>
<tr><td>aws-cloudtrail-changetype.csv</td><td>Maps the eventSource to a CIM-compliant value for the change_type field.</td></tr>
<tr><td>aws-health-error-type.csv</td><td>Maps ErrorCode to ErrorDetail, ErrorCode, ErrorDetail</td></tr>
<tr><td>aws-log-sourcetype-modinput.csv</td><td>Maps sourcetype to modinput</td></tr>
<tr><td>cloudfront_edge_location_lookup</td><td>Maps the x_edge_location value to a human-readable edge_location_name.</td></tr>
<tr><td>vendor-product-aws-cloudtrail.csv</td><td>Defines CIM-compliant values for the vendor, product, and appfields based on the source type.</td></tr>
<tr><td>vpcflow_action_lookup.csv</td><td>Maps the numerical protocol code to a CIM-compliant protocol field and a human-readable protocol_full_name.</td></tr>
<tr><td>vpcflow_protocol_code_lookup.csv</td><td>Maps the vpcflow_action field to a CIM-compliant action field.</td></tr>
<tr><td>VmSizeToResources.csv</td><td>Map the instance_type field to CIM-compliant cpu_cores, mem_capacity field.</td></tr>
</table>

## Performance reference for the Splunk Add-on for AWS data inputs

Many factors impact throughput performance. The rate at which the Splunk Add-on for AWS ingests input data varies depending on a number of variables: deployment topology, number of keys in a bucket, file size, file compression format, number of events in a file, event size, and of course, hardware and networking conditions.

This section provides measured throughput data achieved under certain operating conditions and draws from the performance testing results some rough conclusions and guidelines on tuning AWS add-on throughput performance. Use the information here as a basis for estimating and optimizing the AWS add-on throughput performance in your own production environment. As performance may vary based on user characteristics, application usage, server configurations, and other factors, specific performance results cannot be guaranteed. Consult Splunk Support for accurate performance tuning and sizing.

### Reference hardware and software environment

The throughput data and conclusions provided here are based on performance testing using Splunk instances (dedicated heavy forwarders and indexers) running on the following environment.

<table>
<tr><td>Instance type</td><td>M4 Double Extra Large (m4.4xlarge)</td></tr>
<tr><td>Memory</td><td>64 GB</td></tr>
<tr><td>Compute Units (ECU)</td><td>53.5</td></tr>
<tr><td>vCPU</td><td>16</td></tr>
<tr><td>Storage (GB)</td><td>0 (EBS only)</td></tr>
<tr><td>Arch</td><td>64-bit</td></tr>
<tr><td>EBS Optimized (Max Bandwidth)</td><td>2000 Mbps</td></tr>
<tr><td>Network performance</td><td>High</td></tr>
</table>

The following settings are configured in `outputs.conf` on the heavy forwarder:

`useACK = true`
`maxQueueSize = 15MB`

### Measured performance data

The throughput data provided here is the maximum performance for each single input achieved in performance testing under specific operating conditions and is subject to change when any of the hardware and software variables changes. Use this data for very rough reference only.

#### Single-input max throughput

<table>
<tr><td>Data Input</td><td>Sourcetype</td><td>Max Throughput (KB/s)</td><td>Max EPS (event/s)</td><td>Max Throughput (GB/day)</td></tr>
<tr><td>Generic S3</td><td>aws:elb:accesslogs<br/>
(plain text, syslog, event size 250B, S3 key size 2MB)</td><td>17,000</td><td>86,000</td><td>1,470</td></tr>
<tr><td>Generic S3</td><td>aws:cloudtrail<br/>
(gz, json, event size 720B, S3 key size 2MB)</td><td>11,000</td><td>35,000</td><td>950</td></tr>
<tr><td>Incremental S3</td><td>aws:elb:accesslogs<br/>
(plain text, syslog, event size 250B, S3 key size 2MB)</td><td>11,000</td><td>43,000</td><td>950</td></tr>
<tr><td>Incremental S3</td><td>aws:cloudtrail<br/>
(gz, json, event size 720B, S3 key size 2MB)</td><td>7,000</td><td>10,000</td><td>600</td></tr>
<tr><td>SQS-based S3</td><td>aws:elb:accesslogs<br/>
(plain text, syslog, event size 250B, S3 key size 2MB)</td><td>12,000</td><td>50,000</td><td>1,000</td></tr>
<tr><td>SQS-based S3</td><td>aws:elb:accesslogs<br/>
(gz, syslog, event size 250B, S3 key size 2MB)</td><td>24,000</td><td>100,000</td><td>2,000</td></tr>
<tr><td>SQS-based S3</td><td>aws:cloudtrail<br/>
(gz, json, event size 720B, S3 key size 2MB)</td><td>13,000</td><td>19,000</td><td>1,100</td></tr>
<tr><td>CloudWatch logs [1]</td><td>aws:cloudwatchlog:vpcflow</td><td>1,000</td><td>6,700</td><td>100</td></tr>
<tr><td>CloudWatch<br/>
(ListMetric, 10,000 metrics)</td><td>aws:cloudwatch</td><td>240 (Metrics/s)</td><td>NA</td><td>NA</td></tr>
<tr><td>CloudTrail</td><td>aws:cloudtrail<br/>
(gz, json, sqs=1000, 9K events/key)</td><td>5,000</td><td>7,000</td><td>400</td></tr>
<tr><td>Kinesis</td><td>aws:cloudwatchlog:vpcflow<br/>
(json, 10 shards)</td><td>15,000</td><td>125,000</td><td>1,200</td></tr>
<tr><td>SQS</td><td>aws:sqs<br/>
(json, event size 2.8K)</td><td>N/A</td><td>160</td><td>N/A</td></tr>
</table>

[1] API throttling error occurs if input streams > 1k

#### Multi-inputs max throughput

The following throughput data was measured with multiple inputs configured on a heavy forwarder in an indexer cluster distributed environment.

> Note: Configuring more AWS accounts increases CPU usage and lowers throughput performance due to increased API calls. It is recommended that you consolidate AWS accounts when configuring the Splunk Add-on for AWS.

<table>
<tr><td>Data Input</td><td>Sourcetype</td><td>Max Throughput (KB/s)</td><td>Max EPS (events/s)</td><td>Max Throughput (GB/day)</td></tr>
<tr><td>Generic S3</td><td>aws:elb:accesslogs<br/>
(plain text, syslog, event size 250B, S3 key size 2MB)</td><td>23,000</td><td>108,000</td><td>1,980</td></tr>
<tr><td>Generic S3</td><td>aws:cloudtrail<br/>
(gz, json, event size 720B, S3 key size 2MB)</td><td>45,000</td><td>130,000</td><td>3,880</td></tr>
<tr><td>Incremental S3</td><td>aws:elb:accesslogs<br/>
(plain text, syslog, event size 250B, S3 key size 2MB)</td><td>34,000</td><td>140,000</td><td>2,930</td></tr>
<tr><td>Incremental S3</td><td>aws:cloudtrail<br/>
(gz, json, event size 720B, S3 key size 2MB)</td><td>45,000</td><td>65,000</td><td>3,880</td></tr>
<tr><td>SQS-based S3 [1]</td><td>aws:elb:accesslogs<br/>
(plain text, syslog, event size 250B, S3 key size 2MB)</td><td>35,000</td><td>144,000</td><td>3,000</td></tr>
<tr><td>SQS-based S3 [1]</td><td>aws:elb:accesslogs<br/>
(gz, syslog, event size 250B, S3 key size 2MB)</td><td>42,000</td><td>190,000</td><td>3,600</td></tr>
<tr><td>SQS-based S3 [1]</td><td>aws:cloudtrail<br/>
(gz, json, event size 720B, S3 key size 2MB)</td><td>45,000</td><td>68,000</td><td>3,900</td></tr>
<tr><td>CloudWatch logs</td><td>aws:cloudwatchlog:vpcflow</td><td>1,000</td><td>6,700</td><td>100</td></tr>
<tr><td>CloudWatch (ListMetric)</td><td>aws:cloudwatch<br/>
(10,000 metrics)</td><td>240 (metrics/s)</td><td>NA</td><td>NA</td></tr>
<tr><td>CloudTrail</td><td>aws:cloudtrail<br/>
(gz, json, sqs=100, 9K events/key)</td><td>20,000</td><td>15,000</td><td>1,700</td></tr>
<tr><td>Kinesis</td><td>aws:cloudwatchlog:vpcflow<br/>
(json, 10 shards)</td><td>18,000</td><td>154,000</td><td>1,500</td></tr>
<tr><td>SQS</td><td>aws:sqs<br/>
(json, event size 2.8K)</td><td>N/A</td><td>670</td><td>N/A</td></tr>
</table>

[1] Performance testing of the SQS-based S3 input indicates that optimal performance throughput is reached when running four inputs on a single heavy forwarder instance. To achieve higher throughput performance beyond this bottleneck, you can further scale out data collection by creating multiple heavy forwarder instances each configured with up to four SQS-based S3 inputs to concurrently ingest data by consuming messages from the same SQS queue.

#### Max inputs benchmark per heavy forwarder

The following input number ceiling was measured with multiple inputs configured on a heavy forwarder in an indexer cluster distributed environment, where CPU and memory resources were utilized to their fullest.

If you have a smaller event size, fewer keys per bucket, or more available CPU and memory resources in your environment, you may configure more inputs than the maximum input number indicated in the table.

<table>
<tr><td>Data Input</td><td>Sourcetype</td><td>Format</td><td>Number of Keys/Bucket</td><td>Event Size</td><td>Max Inputs</td></tr>
<tr><td>S3</td><td>aws:s3</td><td>zip, syslog</td><td>100K</td><td>100B</td><td>300</td></tr>
<tr><td>S3</td><td>aws:cloudtrail</td><td>gz, json</td><td>1,300K</td><td>1KB</td><td>30</td></tr>
<tr><td>Incremental S3</td><td>aws:cloudtrail</td><td>gz, json</td><td>1,300K</td><td>1KB</td><td>20</td></tr>
<tr><td>SQS-based S3</td><td>aws:cloudtrail, aws:config</td><td>gz, json</td><td>1,000K</td><td>1KB</td><td>50</td></tr>
</table>

#### Memory usage benchmark for generic S3 inputs

<table>
<tr><td>Event Size</td><td>Number of Events per Key</td><td>Total Number of Keys</td><td>Archive Type</td><td>Number of Inputs</td><td>Memory Used</td></tr>
<tr><td>1K</td><td>1,000</td><td>10,000</td><td>zip</td><td>20</td><td>20G</td></tr>
<tr><td>1K</td><td>1,000</td><td>1,000</td><td>zip</td><td>20</td><td>12G</td></tr>
<tr><td>1K</td><td>1,000</td><td>10,000</td><td>zip</td><td>10</td><td>18G</td></tr>
<tr><td>100B</td><td>1,000</td><td>10,000</td><td>zip</td><td>10</td><td>15G</td></tr>
</table>

### Performance tuning and sizing guidelines

If you do not achieve the expected AWS data ingestion throughput, follow these steps to tune the throughput performance:

1. Identify the bottleneck in your system that prevents it from achieving a higher level of throughput performance. The bottleneck in AWS data ingestion may lie in one of the following components:
    - The Splunk Add-on for AWS: its capacity to pull in AWS data through API calls
    - Heavy forwarder: its capacity to parse and forward data to the indexer tier, which involves the throughput of the parsing, merging, and typing pipelines
    - Indexer: the index pipeline throughput
    To troubleshoot the indexing performance on the heavy forwarder and indexer, refer to [Troubleshooting indexing performance](http://docs.splunk.com/Documentation/Splunk/7.1.1/Troubleshooting/Troubleshootindexingperformance) in the Capacity Planning Manual.
    A chain is as only as strong as its weakest link. The capacity of the bottleneck is the capacity of the entire system as a whole. Only by identifying and tuning the performance of the bottleneck component can you improve the overall system performance.
2. Tune the performance of the bottleneck component.
    If the bottleneck lies in heavy forwarders or indexers, refer to the [Summary of performance recommendations](http://docs.splunk.com/Documentation/Splunk/7.1.1/Capacity/Summaryofperformancerecommendations) in the Capacity Planning Manual.
    If the bottleneck lies in the Splunk Add-on for AWS, adjust the following key factors that usually impact the AWS data input throughput:
    - Parallelization settings
    To achieve optimal throughput performance, you can set the `parallelIngestionPipelines` value to 2 in `server.conf` if your resource capacity permits. For information about `parallelIngestionPipelines`, see [Parallelization settings](http://docs.splunk.com/Documentation/Splunk/7.1.1/Capacity/Parallelization) in the Splunk Enterprise Capacity Planning Manual.
    - AWS data inputs
    When there is no shortage of resources, adding more inputs in the add-on increases throughput but it also consumes more memory and CPU. Increase the number of inputs to improve throughput until memory or CPU is running short.
    If you are using SQS-based S3 inputs, you can horizontally scale out data collection by configuring more inputs on multiple heavy forwarders to consume messages from the same SQS queue.
    - Number of keys in a bucket
    For both the Generic S3 and Incremental S3 inputs, the number of keys (or objects) in a bucket is a factor that impacts initial data collection performance. The first time a Generic or Incremental S3 input collects data from a bucket, the more keys the bucket contains, the longer time it takes to complete the list operation, and the more memory is consumed. A large number of keys in a bucket require a huge amount of memory for S3 inputs in the initial data collection and limit the number of inputs you can configure in the add-on.
    If applicable, you can use log file prefix to subset keys in a bucket into smaller groups and configure different inputs to ingest them separately. For information about how to configure inputs to use log file prefix, see [Add an S3 input for Splunk Add-on for AWS](http://docs.splunk.com/Documentation/AddOns/released/AWS/S3).
    For SQS-based S3 inputs, the number of keys in a bucket is not a primary factor since data collection can be horizontally scaled out based on messages consumed from the same SQS queue.
    - File format
    Compressed files consume much more memory than plain text files.
3. When you have resolved the bottleneck, see if the improved performance meets your requirements. If not, continue the previous steps to identify the next bottleneck in the system and address it until the expected overall throughput performance is achieved.

## Performance reference for the Kinesis input in the Splunk Add-on for AWS

This page provides reference information about Splunk's performance testing of the Kinesis input in Splunk Add-on for AWS. The testing was performed on version 4.0.0, when the Kinesis input was first introduced. Use this information to enhance the performance of your own Kinesis data collection tasks.

> Note: Many factors impact performance results, including file size, file compression, event size, deployment architecture, and hardware. These results represent reference information and do not represent performance in all environments.

### Summary

While results in different environments will vary, Splunk's performance testing of the Kinesis input showed:

- Each Kinesis input can handle up to 6 MB/s of data, with a daily ingestion volume of 500 GB.
- More shards can slightly improve the performance. Three shards are recommended for large streams.

### Testing architecture

Splunk tested the performance of the Kinesis input using a single-instance Splunk Enterprise 6.4.0 on an m4.4xlarge AWS EC2 instance to ensure CPU, memory, storage, and network did not introduce any bottlenecks. Instance specs:

<table>
<tr><td>Instance type</td><td>M4 Quadruple Extra Large (m4.4xlarge)</td></tr>
<tr><td>Memory</td><td>64 GB</td></tr>
<tr><td>ECU</td><td>53.5</td></tr>
<tr><td>Cores</td><td>16</td></tr>
<tr><td>Storage</td><td>0 GB (EBS only)</td></tr>
<tr><td>Architecture</td><td>64-bit</td></tr>
<tr><td>Network performance</td><td>High</td></tr>
<tr><td>EBS Optimized: Max Bandwidth</td><td>250 MB/s</td></tr>
</table>

### Test scenario

Splunk tested the following parameters to target the use case of high-volume VPC flow logs ingested through a Kinesis stream.

- Shard numbers: 3, 5, and 10 shards
- Event size: 120 bytes per event
- Number of events: 20,000,000
- Compression: gzip
- Initial stream position: TRIM_HORIZON

AWS reports that each shard is limited to 5 read transactions per second, up to a maximum read rate of 2MB per second. Thus, with 10 shards, the theoretical upper limit is 20 MB per second.

### Test results

Splunk observed a data ingestion rate of 6 million events per minute at peak, which is 100,000 events per second. Because each event is 120 bytes, this indicates a maximum throughput of 10 MB/s.

Splunk observed an average throughput of 6 MB/s for a single Kinesis modular input, or a daily ingestion throughput of approximately 500 GB.

After reducing the shard number from 10 shards to 3 shards, Splunk observed a throughput downgrade of approximately 10%.

During testing, Splunk observed the following resource usage on the instance:

- normalized CPU usage of approximately 30%
- Python memory usage of approximately 700 MB

The indexer is largest consumer of CPU, and the modular input is largest consumer of memory.

> Note: AWS throws a ProvisionedThroughputExceededException if a call returns 10 MB of data and subsequent calls are made within the next 5 seconds. Splunk observed this error while testing with three shards only every one to five minutes.

## Use SNS Alert for the Splunk Add-on for AWS

> Note: To use custom search commands and custom alert actions, you must either be a Splunk administrator or a user with the appropriate capability:

- `list_storage_passwords` if you are using Splunk platform 6.5.0 or later
- `admin_all_objects` if you are using an earlier version of the Splunk platform

### Use the custom search command

The Splunk Add-on for AWS includes a custom search command to send alerts to AWS SNS.

```sql
...| eval message="My Message" | eval entity="My Entity" | eval correlation_id="1234567890" | awssnsalert account=real region="ap-southeast-1" topic_name="ta-aws-sns-ingestion" publish_all=1
```

<table>
<tr><td>Attribute</td><td>Description</td></tr>
<tr><td>account</td><td>Required. AWS account name configured in add-on</td></tr>
<tr><td>region</td><td>Required. AWS region name</td></tr>
<tr><td>topic_name</td><td>Required. The alert message is sent to this AWS SNS topic name.</td></tr>
<tr><td>message</td><td>Required. The message that the Splunk Add-on for AWS sent to AWS SNS.</td></tr>
<tr><td>publish_all</td><td>You can set publish_all to 0 or 1. If you set publish_all=1, it means that this add-on will send all the records in this search. If you set publish_all=0, it means that this add-on will only send the first result to this search. The default value of this field is 0.</td></tr>
</table>

### Use the alert action

The Splunk Add-on for AWS supports automatic incident and event creation and incident update from custom alert actions. Custom alert actions are available in Splunk platform version 6.3.0 and later.

To create a new incident or event from a custom alert action:

1. Write a search string that you want to use to trigger incident or event creation in AWS SNS.
2. Click **Save As > Alert**.
3. Fill out the Alert form. Give your alert a unique name and indicate whether the alert should be a real-time alert or a scheduled alert. See [Getting started with Alerts](http://docs.splunk.com/Documentation/Splunk/7.1.1/Alert/Aboutalerts) in the Alerting Manual, part of the Splunk Enterprise documentation, for more information.
4. Under Trigger Actions, click **Add Actions**.
5. From the list, select **AWS SNS Alert** if you want the alert to create an event in AWS SNS.
6. Enter values for all required fields, as shown.

<table>
<tr><td>Field</td><td>Example Value</td></tr>
<tr><td>Account</td><td>Required. The account name configured in Splunk Add-on for AWS.</td></tr>
<tr><td>Region</td><td>Required. The region of AWS SNS the events will be sent to. You have to make sure the region is consistent with AWS SNS.</td></tr>
<tr><td>Topic Name</td><td>Required. The name of the topic the events will be sent to. You have to make sure the topic name exists in AWS SNS.</td></tr>
<tr><td>Correlation ID</td><td>Optional. The ID that correlates this alert with the other events. If you leave this field empty, it will use $result.correlation_id$ by default.</td></tr>
<tr><td>Entity</td><td>Optional. Object related to the event or alert, such as host, database, EC2 instance. If you leave this field empty, Splunk will use $result.entity$ by default.</td></tr>
<tr><td>Source</td><td>Optional. The source of the event or alert. If you leave this field empty, the Splunk platform will use $result.source$ by default.</td></tr>
<tr><td>Timestamp</td><td>Optional. The time of the event occurs. If you leave this field empty, the Splunk platform will use $result._time$ by default.</td></tr>
<tr><td>Event</td><td>Optional. The details of the event. If you leave this field empty, the Splunk platform will use $result._raw$ by default.</td></tr>
<tr><td>Message</td><td>Required. The message that the Splunk Add-on for AWS sent to AWS SNS.</td></tr>
</table>