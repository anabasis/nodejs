# 강화학습 수학

확률실험(random experiment)
원소(element)
표본공간(sample space)
사건(event)

이산 표본공간(discrete)
연속 표본공간(countinuous)

확율

- 경험적 확률
- 통계적 확률
- 고전적 수학적 확률
- 공리에 바탕을 둔 수학적 확률

랜덤변수(random variable) = 확률변수, 함수

랜덤변수X에 대한 누적분포함수 : cdf(cumulative distribution function) , 단조증가함수(monotonically non-decreasing function)
랜덤변수X에 대한 확률밀도함수 : pdf(probability density function), 누적분포함수의 미분

이산랜덤변수X에 대한 확률질량함수 : pmf(probability mass function)

랜덤변수 X,Y에 대한 결합 누적분포함수
랜덤변수 X,Y에 대한 결합 확률밀도함수
결합 확률함수

X의 한계밀도함수(marginal density function) : 랜덤변수 X,Y에 대한 결합 확률밀도함수에서 X만의 확률밀도함수

조건부 확률함수(conditional probability function)

베이즈정리
전확률(total probability)

기대값(expectation)
분산(variance)
조건부 기대값
조건부 분산

랜덤벡터 (random vector)
공분산 행렬()
대칭행렬(symmetric matrix)
독립동일분포(iid)

가우시안 분포(gaussian distribution) = 정규 확률밀도함수

랜덤시퀀스(random sequence) - 이산시간(descrete-time) 랜덤 프로세스 - 확률 실험의 결과에 시간함수를 대응시키는 함수로 정의
샘플함수 = 확정적(deterministic function)이며 앙상블(ensemble)

앙상블(ensemble)
<https://teddylee777.github.io/machine-learning/ensemble%EA%B8%B0%EB%B2%95%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4%EC%99%80-%EC%A2%85%EB%A5%98-1>
<https://m.blog.naver.com/PostView.nhn?blogId=gksshdk8003&logNo=220895953955&proxyReferer=https:%2F%2Fwww.google.com%2F>

자기 공분산 함수(auto-covariance function)

이산간 화이트 노이즈(white noise,백색잡음)
크로넥터 델타(Kronecker delta)

마르코프 시퀀스(Markov Sequence) - 과거의 모든 확률 정보는 현재의 확률정보에 모두 녹아 있다.(강화학습, 추정이론, 신호처리)

## 엔트로피(entropy)

- 항상 일어나는 사건은 새로울 것이 없으므로 여기에서 얻을 수 있는 정보의 양은 매우 적다
- 잘일어나지 않는 사건에서는 빈번하게 일어나는 사건에서보다 얻을 수 있는 정보의 양이 더 많다.
- 따라서 사건의 발생 빈도에 영향을 받으므로 확률적인 속성을 가지고 있고 확률함수로 나타내는 것
- 엔트로피는 분산에만 영향을 받는다. 분산이 커질수록 증가함.
- 분산이 클수록 사건의 무작위성이 커지고 특정 사건의 발생 빈도수가 작아지기 때문에 정보량이 증가한다.
- 물리 시스템에서도 엔트로피가 증가할 수록 무작위성 또는 무질서의 정도가 증가

상대 엔트로피 = KL발산(Kullback-Leibler divergence)

정보량의 과학적 개념 : `내가 아는 어떤 것의 총량`이 아니라 `어떤 것의 가능한 대안의 수 또는 모든 경우의 수의 총량`

## 추정기(estimator)

베이즈 방법(Bayesian approach) vs 비베이즈 방법(non-Bayesian approach)
베이즈 방법(Bayesian approach) : 최대사후(MAP,maximum a posteriori), 최소평균제곱오차(MMSE,minimum mean-square error)
비베이즈 방법(non-Bayesian approach) : 최대빈도(ML,maximum likelihood), 최소제곱오차(LSE, least-square error)
강화학습 : MAP, ML

최대사후 추정기
최대빈도 추정기

촐레스키 분해(Cholesky decomposition)

### 경사하강법(gradient descent) OR 경사상승법(gradient ascent)

- 배치 경사하강법
- 확률적 경사하강법

### 경사하강법의 개선

- 모멤텀 경사하강법(gradient descent with momentum)
- RMSprop
- 아담(Adam)
- 학습속도가 빠름 : RMSprop, 아담(Adam)

모멤텀 : 파라미터가 업데이트된 방향으로 관성 효과를 주는 것

- 손실함수 그래디언트가 가리키는 방향의 반대 방향으로 파라미터를 바로 이동시키는 것이 아니라, 기존에 이동하던 방향으로의 움직임을 일정부분 유지하면서(관성효과) 그래디언트가 가리키는 방향의 반대 방향을 적당히 혼합해 새로운 방향으로 이동시키는 것

>> 대부분의 신경망 학습 알고리즘은 손실함수를 정하거나 최적화를 위한 목적함수를 만드는 것으로 시작



